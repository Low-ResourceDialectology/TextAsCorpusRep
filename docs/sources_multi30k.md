
# Sources

#### Flickr30k
- [b] Flickr30k [URL](https://paperswithcode.com/dataset/flickr30k)
- [i] The Flickr30k dataset contains 31,000 images collected from Flickr, together with 5 reference sentences provided by human annotators.
→ Introduced in
- [b] (Young et al., 2014) From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions [URL](https://paperswithcode.com/paper/from-image-descriptions-to-visual-denotations)
- [*] Flickr30k Captions Quintets on HuggingFace [URL](https://huggingface.co/datasets/embedding-data/flickr30k_captions_quintets)


#### Multi30k-uk 
- [b] (Saichyshyna et al., 2023) Extension Multi30K: Multimodal Dataset for Integrated Vision and Language Research in Ukrainian [URL](https://aclanthology.org/2023.unlp-1.7)
- [i] Initially this dataset extends the Flickr30K dataset by adding [[German]] translations. The descriptions were collected from a crowdsourcing platform, while the translations were collected from professionally contracted translators.
    We present a variation of this dataset manually translated for [[Ukrainian]] language.
- [*] HuggingFace [URL](https://huggingface.co/datasets/turuta/Multi30k-uk)


#### Flickr30k-CNA
- [b] (Xie et al., 2023) CCMB: A Large-scale Chinese Cross-modal Benchmark [URL](https://arxiv.org/abs/2205.03860)
- [i] We gather professional English and Chinese linguists to meticulously re-translate all data of Flickr30k and double-check each sentence. Beijing Magic Data Technology Co., Ltd. contributes for the translation of this dataset.
- [*] CCMB and R2D2: A Large-scale Chinese Cross-modal Benchmark and A Vision-Language Framework [github](https://github.com/yuxie11/R2D2)
- [*] Flickr30k-CNA → Flickr30k-CNA Google Drive Baidu Drive We provide the re-translated high-quality texts for Flickr30k. [URL](https://zero.so.com/download.html)
##### ZERO [URL](https://zero.so.com/index.html)
- [i] 250.000.000 images with 750.000.000 descriptions . . . . . . 


#### MSVD-Turkish Dataset [URL](https://hucvl.github.io/MSVD-Turkish/)
- [b] (Citamak et al., 2020) MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish [URL](https://arxiv.org/abs/2012.07098)
- [b] (Barrault et al., 2018) Findings of the Third Shared Task on Multimodal Machine Translation [URL](https://aclanthology.org/W18-6402/)


#### Multi30k 
- [b] (Elliot et al., 2017) Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description [URL](https://aclanthology.org/W17-4718/)
- [*] Multi30k Data Repository [github](https://github.com/multi30k/dataset)
	- English, German, French, 


#### Flickr30k
- [b] ACL 2016 FIRST CONFERENCE ON MACHINE TRANSLATION (WMT16) [URL](https://www.statmt.org/wmt16/multimodal-task.html)
- [i] The data to be used for both tasks is an extended version of the Flickr30K dataset [URL](http://shannon.cs.illinois.edu/DenotationGraph/). The original dataset contains 31,783 images from Flickr on various topics and five crowdsourced English descriptions per image, totalling 158,915 English descriptions.
Images + 5 Descriptions each [URL](http://shannon.cs.illinois.edu/DenotationGraph/data/flickr30k.html) 
- [?] If you use this corpus, please cite:  
	Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics 2 (2014), 67–78.



