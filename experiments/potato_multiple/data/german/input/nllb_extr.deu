Lillian Diana Gish (14. Oktober 1893 – 27. Februar 1993) war eine US-amerikanische Schauspielerin, Regisseurin und Drehbuchautorin.
Gish war von 1912 bis in die 1920er Jahre ein prominenter Filmstar und wurde insbesondere mit den Filmen des Regisseurs D. W. Griffith in Verbindung gebracht.
Von den frühen 1950er bis in die 1980er Jahre war sie auch fürs Fernsehen tätig und beendete ihre Karriere 1987 mit der Rolle an der Seite von Bette Davis im Film „Die Wale im August“.
Die ersten Generationen von Gishes waren Minister von Dunkard.
Ihre Mutter eröffnete die Majestic Candy Kitchen und die Mädchen halfen beim Verkauf von Popcorn und Süßigkeiten an die Gäste des alten Majestic Theatre, das sich nebenan befindet.
Die siebzehnjährige Lillian reiste nach Shawnee, Oklahoma, wo James‘ Bruder Alfred Grant Gish und seine Frau Maude lebten.
Ihr Vater starb 1912 in Norman, Oklahoma, aber sie war einige Monate zuvor nach Ohio zurückgekehrt.
Als Lillian und Dorothy alt genug waren, traten sie dem Theater bei und reisten oft getrennt in verschiedenen Produktionen.
Gish trat weiterhin auf der Bühne auf und brach 1913 während einer Aufführung von „A Good Little Devil“ aufgrund einer Anämie zusammen.
Ihre Leistung unter diesen eisigen Bedingungen verursachte bleibende Nervenschäden an mehreren Fingern.
Er nutzte ihre Ausdrucksfähigkeiten voll aus und entwickelte sie zu einer leidenden, aber starken Heldin.
Sie führte bei dem Film „Remodeling Her Husband“ (1920) mit ihrer Schwester Dorothy Regie, als D. W. Griffith mit seiner Einheit vor Ort war.
Sie lehnte das Geld ab und forderte ein bescheideneres Gehalt und einen Prozentsatz, damit das Studio die Mittel verwenden könne, um die Qualität ihrer Filme zu verbessern – indem sie die besten Schauspieler, Drehbuchautoren usw. engagierten.
Viele der Hauptdarstellerinnen der Stummfilmära, wie Gish und Pickford, waren gesund und unschuldig gewesen, aber in den frühen 1930er Jahren (nach der vollständigen Einführung des Tons und bevor der Filmproduktionscode durchgesetzt wurde) galten diese Rollen als veraltet.
Louis Mayer wollte einen Skandal inszenieren („sie vom Podest stoßen“), um die öffentliche Sympathie für Gish zu erregen, aber Lillian wollte nicht sowohl auf der Leinwand als auch außerhalb schauspielern und kehrte zu ihrer ersten Liebe, dem Theater, zurück.
Gish kehrte zum Film zurück und wurde 1946 für Duel in the Sun für den Oscar als beste Nebendarstellerin nominiert.
Sie wurde für verschiedene Rollen in „Vom Winde verweht“ in Betracht gezogen, von Ellen O’Hara, Scarletts Mutter (die an Barbara O’Neil ging) bis hin zur Prostituierten Belle Watling (die an Ona Munson ging).
Sie trat als Kaiserinwitwe Maria Fjodorowna im kurzlebigen Broadway-Musical Anya von 1965 auf.
Sie wurde in der Fernsehdokumentationsserie Hollywood: A Celebration of the American Silent Film (1980) interviewt.
Sie hat einen Stern auf dem Hollywood Walk of Fame in der Vine Street 1720.
Beim Cannes-Festival erhielt Gish zehnminütige Standing Ovations vom Publikum.
Die Folge „Marry for Murder“ wurde am 9. September 1943 ausgestrahlt.
1971 wurde ihr ein Oscar-Ehrenpreis verliehen, und 1984 erhielt sie einen AFI Life Achievement Award.
Am nächsten Tag verlieh die Universität Gish die Ehrendoktorwürde des Doktors der Darstellenden Künste.
Nach Gishs Tod im Jahr 1993 sammelte die Universität Geld, um ihre Galerie zu erweitern und Erinnerungsstücke aus Gishs Nachlass auszustellen.
Die Verbindung zwischen ihr und D. W. Griffith war so eng, dass einige eine romantische Verbindung vermuteten, ein Problem, das Gish nie zugab, obwohl mehrere ihrer Mitarbeiter sicher waren, dass sie zumindest kurzzeitig beteiligt waren.
In den 1920er Jahren wurde Gishs Verbindung mit Duell zu einer Art Boulevardskandal, als er sie verklagte und die Einzelheiten ihrer Beziehung öffentlich machte.
George Jean Nathan lobte Gishs schauspielerische Leistung und verglich sie mit Eleonora Duse.
Während der Zeit der politischen Unruhen in den USA, die vom Ausbruch des Zweiten Weltkriegs in Europa bis zum Angriff auf Pearl Harbor andauerte, vertrat sie eine ausgesprochen nicht-interventionistische Haltung.
Joseph Frank Keaton (4. Oktober 1895 – 1. Februar 1966), beruflich bekannt als Buster Keaton, war ein US-amerikanischer Schauspieler, Komiker, Filmregisseur, Produzent, Drehbuchautor und Stunt-Darsteller.
Seine Karriere ging zurück, als er bei Metro-Goldwyn-Mayer unterschrieb und seine künstlerische Unabhängigkeit verlor.
Viele von Keatons Filmen aus den 1920er Jahren genießen nach wie vor hohes Ansehen, beispielsweise Sherlock Jr. (1924), The General (1926) und The Cameraman (1928).
Sein Vater war Joseph Hallie „Joe“ Keaton, der mit Harry Houdini eine Wandershow namens Mohawk Indian Medicine Company oder Keaton Houdini Medicine Show Company leitete, die auf der Bühne auftrat und nebenbei Patentmedizin verkaufte.
In Keatons Nacherzählung war er sechs Monate alt, als sich der Vorfall ereignete, und Harry Houdini gab ihm den Spitznamen.
Der Akt war hauptsächlich eine Comedy-Skizze.
In Keatons Kleidung war ein Koffergriff eingenäht, um das ständige Hin- und Herwerfen zu erleichtern.
Allerdings konnte Buster den Behörden jederzeit nachweisen, dass er keine Prellungen oder Knochenbrüche hatte.
Mehrere Male wäre ich getötet worden, wenn es mir nicht gelungen wäre, wie eine Katze zu landen.
Als er bemerkte, dass das Publikum dadurch weniger lachte, nahm er bei seinem Auftritt seinen berühmten ausdruckslosen Gesichtsausdruck an.
Trotz Verstößen gegen das Gesetz und einer katastrophalen Tournee durch Konzertsäle im Vereinigten Königreich war Keaton ein aufstrebender Stern im Theater.
Im Februar 1917 traf er Roscoe „Fatty“ Arbuckle in den Talmadge Studios in New York City, wo Arbuckle bei Joseph M. Schenck unter Vertrag stand.
Buster war in seinem ersten Film „The Butcher Boy“ so ein Naturtalent, dass er sofort engagiert wurde.
Keaton behauptete später, er sei bald Arbuckles zweiter Regisseur und dessen gesamte Gag-Abteilung.
Es basierte auf dem erfolgreichen Theaterstück „The New Henrietta“, das bereits einmal unter dem Titel „The Lamb“ verfilmt worden war und in dem Douglas Fairbanks die Hauptrolle spielte.
Er drehte eine Reihe von Komödien mit zwei Rollen, darunter One Week (1920), The Playhouse (1921), Cops (1922) und The Electric House (1922).
Comedy-Regisseur Leo McCarey erinnerte sich an die unbeschwerte Zeit, in der er Slapstick-Komödien drehte: „Wir alle haben versucht, uns gegenseitig die Gagmen zu stehlen.“
Während der Szene mit dem Wassertank der Eisenbahn in „Sherlock Jr.“ brach sich Keaton das Genick, als ein Wasserstrom von einem Wasserturm auf ihn niederprasselte, aber er bemerkte es erst Jahre später.
Keatons Charakter blieb aufgrund eines einzigen offenen Fensters unversehrt.
Neben „Steamboat Bill, Jr.“ (1928) gehören zu Keatons beständigsten Langfilmen „Our Hospitality“ (1923), „The Navigator“ (1924), „Sherlock Jr.“ (1924), „Seven Chances“ (1925), „The Cameraman“ (1928), und Der General (1926).
Obwohl der Film als Keatons größte Errungenschaft galt, erhielt er damals gemischte Kritiken.
Sein Verleih, United Artists, bestand auf einem Produktionsleiter, der die Ausgaben überwachte und in bestimmte Elemente der Geschichte eingriff.
Die Schauspieler prägten sich phonetisch die fremdsprachigen Drehbücher ein paar Zeilen nach dem anderen ein und drehten unmittelbar danach.
Der Regisseur war normalerweise Jules White, dessen Schwerpunkt auf Slapstick und Farce die meisten dieser Filme an Whites berühmte Three Stooges-Kurzfilme erinnern ließ.
Das Beharren von Regisseur White auf unverblümten, gewalttätigen Gags führte jedoch dazu, dass die Columbia-Kurzfilme die am wenigsten einfallsreichen Komödien waren, die er gemacht hatte.
Seinen letzten Hauptdarsteller drehte er in El Moderno Barba Azul (1946) in Mexiko; Bei dem Film handelte es sich um eine Low-Budget-Produktion, und möglicherweise wurde er in den Vereinigten Staaten erst in den 1980er-Jahren gezeigt, als er unter dem Titel „Boom in the Moon“ auf VHS erschien.
In „In the Good Old Summertime“ führte Keaton persönlich Regie bei den Stars Judy Garland und Van Johnson in ihrer ersten gemeinsamen Szene, in der sie einander auf der Straße begegnen.
Die Reaktion war so stark, dass ein lokaler Sender in Los Angeles Keaton 1950 seine eigene Show anbot, die ebenfalls live übertragen wurde.
Buster Keatons Frau Eleanor war ebenfalls in der Serie zu sehen (insbesondere als Julia in Busters Romeo in einer kleinen Theatervignette).
Keatons regelmäßige Fernsehauftritte in den 1950er und 1960er Jahren trugen dazu bei, das Interesse an seinen Stummfilmen wiederzubeleben.
Bis weit in seine Fünfziger hinein reproduzierte Keaton erfolgreich seine alten Routinen, einschließlich eines Stunts, bei dem er einen Fuß auf einen Tisch stützte, dann den anderen Fuß daneben schwang und die unangenehme Position in der Luft einen Moment lang hielt, bevor er auf den Bühnenboden stürzte .
Keaton hatte Abzüge der Spielfilme Three Ages, Sherlock Jr., Steamboat Bill, Jr. und College (eine Rolle fehlte) sowie der Kurzfilme „The Boat“ und „My Wife’s Relations“, die Keaton und Rohauer dann auf Zelluloseacetat übertrugen Film vor sich verschlechternden Nitratfilmmaterialien.
In einer Reihe von Stummfilmwerbespots für Simon Pure Beer, die Jim Mohr 1962 in Buffalo, New York, drehte, griff Keaton einige der Gags aus seiner Stummfilmzeit noch einmal auf.
Im Dezember 1958 war Keaton Gaststar in der Folge „A Very Merry Christmas“ der Donna Reed Show auf ABC.
1960 kehrte er zum letzten Mal zu MGM zurück und spielte einen Löwenbändiger in einer Adaption von Mark Twains „Die Abenteuer des Huckleberry Finn“ aus dem Jahr 1960.
Er arbeitete mit dem Komiker Ernie Kovacs an einem Fernsehpiloten mit dem vorläufigen Titel „Medicine Man“ und drehte die Szenen dafür am 12. Januar 1962 – einen Tag bevor Kovacs bei einem Autounfall ums Leben kam. "
Er reiste mit einer motorisierten Draisine von einem Ende Kanadas zum anderen, trug seinen traditionellen Pork-Pie-Hut und führte Gags auf, die denen in Filmen ähnelten, die er 50 Jahre zuvor gedreht hatte.
Ebenfalls 1965 reiste er nach Italien, um eine Rolle in Two Marines and a General mit Franco Franchi und Ciccio Ingrassia zu spielen.
Eine seiner bissigsten Parodien ist The Frozen North (1922), eine satirische Interpretation von William S. Harts Western-Melodramen wie Hell's Hinges (1916) und The Narrow Trail (1917).
Das Publikum der 1920er Jahre erkannte die Parodie und fand den Film unglaublich witzig.
Der Kurzfilm enthielt auch den Eindruck eines auftretenden Affen, der wahrscheinlich von der Tat eines Mitrechners (genannt Peter der Große) abgeleitet war.
Hinweis: In der Quelle wird Keatons häufige Bezeichnung „Great Stoneface“ falsch geschrieben.
Keaton war ab den 1920er Jahren mit der Schauspielerin Dorothy Sebastian und Anfang der 1930er Jahre mit Kathleen Key zusammen.
Mit Tricks, die er von Harry Houdini gelernt hatte, entkam er der Zwangsjacke.
Sie reichte 1935 die Scheidung ein, nachdem sie Keaton mit Leah Clampitt Sewell, der Frau des Millionärs Barton Sewell, in einem Hotel in Santa Barbara gefunden hatte.
Er hörte fünf Jahre lang auf zu trinken.
Die Ehe hielt bis zu seinem Tod.
Keaton war in seinen letzten Tagen in einem Krankenhaus eingeliefert, war unruhig und ging endlos im Zimmer auf und ab, mit dem Wunsch, nach Hause zurückzukehren.
Das Drehbuch von Sidney Sheldon, der auch Regie führte, basierte lose auf Keatons Leben, enthielt jedoch viele sachliche Fehler und verschmolz seine drei Frauen zu einer Figur.
Mit dem Ziel, größere öffentliche Aufmerksamkeit auf Keatons Leben und Werk zu lenken, gehören zu den Mitgliedern viele Personen aus der Fernseh- und Filmbranche: Schauspieler, Produzenten, Autoren, Künstler, Graphic Novel-Autoren, Musiker und Designer sowie diejenigen, die einfach die Magie von Keaton bewundern Buster Keaton.
Hirschfeld sagte, dass moderne Filmstars schwieriger darzustellen seien und dass Stummfilmkomiker wie Laurel, Hardy und Keaton „wie ihre Karikaturen aussahen“.
Der Filmkritiker Roger Ebert erklärte: „Der größte stille Clown ist Buster Keaton, nicht nur aufgrund dessen, was er getan hat, sondern auch aufgrund der Art und Weise, wie er es getan hat.“
Der Filmemacher Mel Brooks nennt Buster Keaton einen großen Einfluss und sagt: „Ich habe (Buster) auf zwei Ebenen viel zu verdanken: Zum einen, weil er für mich selbst als Filmemacher ein so großartiger Lehrer war, und zum anderen, weil ich einfach nur ein Mensch bin, der das sieht.“ begabte Person, die diese erstaunlichen Dinge tut.
Der Schauspieler und Stunt-Darsteller Johnny Knoxville nennt Keaton als Inspiration, wenn er Ideen für Jackass-Projekte entwickelt.
Lewis war besonders berührt von der Tatsache, dass Eleanor sagte, seine Augen ähnelten denen von Keaton.
Im Jahr 1964 erzählte er einem Interviewer, dass er bei der Herstellung „dieser besonderen Schweinefleischpastete“ „mit einem guten Stetson angefangen und es dann reduziert und den Rand mit Zuckerwasser versteift“ habe.
Seine Urgroßeltern väterlicherseits waren Waliser.
Lloyd begann mit Roach zusammenzuarbeiten, der 1913 sein eigenes Studio gegründet hatte.
1919 verließ sie Lloyd, um ihren dramatischen Ambitionen nachzugehen.
Berichten zufolge mochte Lloyd sie umso mehr, je mehr Lloyd Davis beobachtete.
Harold Lloyd entfernte sich von tragikomischen Figuren und porträtierte einen Jedermann mit unerschütterlichem Selbstvertrauen und Optimismus.
Um seinen neuen Charakter zu erschaffen, setzte Lloyd eine linsenlose Hornbrille auf, trug aber normale Kleidung; Zuvor hatte er als Chaplinesker „Lonesome Luke“ einen falschen Schnurrbart und schlecht sitzende Kleidung getragen. "
Sie waren natürlich und die Romantik könnte glaubwürdig sein.
Als er am Sonntag, dem 24. August 1919, im Los Angeles Witzel Photography Studio für einige Werbefotos posierte, nahm er etwas, was er für eine Requisitenbombe hielt, und zündete sie mit einer Zigarette an.
Lloyd war gerade dabei, sich eine Zigarette an der Zündschnur der Bombe anzuzünden, als diese explodierte, wobei er sich außerdem schwere Verbrennungen im Gesicht und in der Brust zufügte und sein Auge verletzte.
Lloyd und Roach trennten sich 1924 und Lloyd wurde unabhängiger Produzent seiner eigenen Filme.
Alle diese Filme waren enorm erfolgreich und profitabel, und Lloyd wurde schließlich der bestbezahlte Filmdarsteller der 1920er Jahre.
Allerdings hatte seine umtriebige Filmfigur keinen Bezug zum Publikum der Filme über die Weltwirtschaftskrise der 1930er Jahre.
Am 23. März 1937 verkaufte Lloyd das Land seines Studios, der Harold Lloyd Motion Picture Company, an die Kirche Jesu Christi der Heiligen der Letzten Tage.
Er kehrte für eine weitere Hauptrolle in „The Sin of Harold Diddlebock“ zurück, einer unglückseligen Hommage an Lloyds Karriere unter der Regie von Preston Sturges und finanziert von Howard Hughes.
Lloyd und Sturges hatten unterschiedliche Vorstellungen vom Material und stritten sich während der Dreharbeiten häufig; Lloyd war besonders besorgt darüber, dass Sturges zwar drei bis vier Monate mit dem Drehbuch für das erste Drittel des Films verbracht hatte, „die letzten zwei Drittel jedoch in einer Woche oder weniger schrieb“.
Einige betrachteten das Old Gold Comedy Theatre als eine leichtere Version des Lux Radio Theatre und es traten einige der bekanntesten Film- und Radiopersönlichkeiten der Zeit auf, darunter Fred Allen, June Allyson, Lucille Ball, Ralph Bellamy, Linda Darnell, Susan Hayward, Herbert Marshall, Dick Powell, Edward G. Robinson, Jane Wyman und Alan Young.
Viele Jahre später wurden in Lloyds Haus Acetatplatten von 29 Sendungen entdeckt, die heute unter Sammlern alter Radiosender im Umlauf sind.
Er war ehemaliger Potentat des Al-Malaikah-Schreins in Los Angeles und wurde schließlich für das Jahr 1949–50 zum kaiserlichen Potentaten der Shriners of North America gewählt.
Lloyd wurde 1955 der Rang und die Auszeichnung eines Knight Commander Court of Honor verliehen und 1965 zum Inspector General Honorary, 33°, gekrönt.
Als ersten Schritt hieß es, Lloyd werde die Geschichte seines Lebens für Simon und Schuster schreiben.
Bekannt wurde er durch seine Aktfotografien von Models wie Bettie Page und der Stripperin Dixie Evans für verschiedene Männermagazine.
Wir hatten nie vor, sie mit Klavieren zu spielen.“
Sie waren nahe dran, aber noch nicht ganz nach oben.“
In den frühen 1960er Jahren produzierte Lloyd zwei Kompilationsfilme mit Szenen aus seinen alten Komödien: Harold Lloyd's World of Comedy und The Funny Side of Life.
Time-Life veröffentlichte mehrere der Spielfilme mehr oder weniger intakt und verwendete auch einige von Lloyd in Auftrag gegebene Partituren von Scharf.
Der Dokumentarfilm von Brownlow und Gill wurde als Teil der PBS-Serie American Masters gezeigt und weckte in den Vereinigten Staaten erneutes Interesse an Lloyds Werk, die Filme waren jedoch größtenteils nicht verfügbar.
Im September 1930 adoptierten sie auch Gloria Freeman (1924–1986), die sie in Marjorie Elizabeth Lloyd umbenannten, die aber die meiste Zeit ihres Lebens als „Peggy“ bekannt war.
Davis starb 1969, zwei Jahre vor Lloyds Tod, an einem Herzinfarkt.
Im Jahr 1925, auf dem Höhepunkt seiner Filmkarriere, trat Lloyd der Freimaurerei in der Alexander Hamilton Lodge Nr. bei.
Im Jahr 1926 wurde er Freimaurer des schottischen Ritus 32 im Valley of Los Angeles, Kalifornien.
Ein Teil von Lloyds persönlichem Inventar seiner Stummfilme (der damals auf 2 Millionen US-Dollar geschätzt wurde) wurde im August 1943 zerstört, als sein Filmtresor Feuer fing.
Der Brand verschonte das Haupthaus und die Nebengebäude.
Lloyd wurde 1960 für seinen Beitrag zum Kinofilm mit einem Stern auf dem Hollywood Walk of Fame in der Vine Street 1503 geehrt.
Das zweite Zitat war eine Brüskierung für Chaplin, der zu diesem Zeitpunkt mit dem McCarthyismus in Konflikt geraten war und dessen Einreisevisum für die Vereinigten Staaten widerrufen worden war.
Gladys Marie Smith (8. April 1892 – 29. Mai 1979), beruflich bekannt als Mary Pickford, war eine kanadisch-amerikanische Filmschauspielerin und Produzentin mit einer Karriere, die sich über fünf Jahrzehnte erstreckte.
Ihr Vater, John Charles Smith, war der Sohn englischer methodistischer Einwanderer und arbeitete in verschiedenen Gelegenheitsjobs.
Um den Verwandten ihres Mannes zu gefallen, taufte Pickfords Mutter ihre Kinder auf Methodisten, die Religion ihres Vaters.
Gladys, ihre Mutter und zwei jüngere Geschwister reisten mit der Bahn durch die Vereinigten Staaten und traten in drittklassigen Unternehmen und Theaterstücken auf.
Gladys bekam schließlich eine Nebenrolle in einem Broadway-Stück von 1907, The Warrens of Virginia.
Nach Abschluss der Broadway-Aufführung und Tournee mit dem Stück war Pickford jedoch erneut arbeitslos.
Sie erkannte schnell, dass Filmschauspiel einfacher war als das stilisierte Bühnenschauspiel der damaligen Zeit.
Wie Pickford über ihren Erfolg bei Biograph sagte: „Ich spielte Wäscherinnen, Sekretärinnen und Frauen aller Nationalitäten ... Ich beschloss, dass ich bekannt werden würde, wenn ich in so vielen Filmen wie möglich mitspielen könnte, und dass es eine Nachfrage nach meinem Film geben würde.“ arbeiten.
Im Januar 1910 reiste Pickford mit einer Biograph-Crew nach Los Angeles.
Die Schauspieler in Griffiths Begleitung wurden im Abspann nicht aufgeführt.
Pickford verließ Biograph im Dezember 1910.
Sie kehrte in der David Belasco-Produktion von A Good Little Devil (1912) an den Broadway zurück.
1913 beschloss sie, ausschließlich im Film zu arbeiten.
Pickford verließ die Bühne, um sich Zukors Staraufgebot anzuschließen.
Komödien wie In the Bishop's Carriage (1913), Caprice (1913) und insbesondere Hearts Adrift (1914) machten sie für Kinogänger unwiderstehlich.
Tess of the Storm Country wurde fünf Wochen später veröffentlicht.
Nur Charlie Chaplin, der 1916 Pickfords Popularität leicht übertraf, hatte eine ähnlich fesselnde Anziehungskraft auf Kritiker und Publikum.
Sie wurde außerdem Vizepräsidentin der Pickford Film Corporation.
Da sie keine normale Kindheit hatte, machte es ihr Spaß, diese Bilder zu machen.
Im August 1918 lief Pickfords Vertrag aus und als sie Zukors Bedingungen für eine Verlängerung ablehnte, wurden ihr 250.000 US-Dollar angeboten, um das Filmgeschäft zu verlassen.
Über United Artists produzierte und trat Pickford weiterhin in ihren eigenen Filmen auf; sie konnte sie auch nach Belieben verteilen.
In dieser Zeit drehte sie auch „Little Annie Rooney“ (1925), einen weiteren Film, in dem Pickford ein Kind spielte, „Sparrows“ (1926), der den Stil von Dickens mit dem neuen deutschen expressionistischen Stil verband, und „My Best Girl“ (1927), eine romantische Komödie mit ihrem zukünftigen Ehemann Charles „Buddy“ Rogers.
Sie spielte eine rücksichtslose Gesellschaftsdame in „Coquette“ (1929), ihrem ersten Tonfilm, eine Rolle, für die ihre berühmten Locken in einen Bob aus den 1920er-Jahren geschnitten wurden.
In den anspruchsvolleren Rollen reagierte das Publikum nicht auf sie.
Etablierte Hollywood-Schauspieler gerieten angesichts der bevorstehenden Einführung des Tonfilms in Panik.
Nach drei kostspieligen Misserfolgen zog sie sich 1933 von der Filmschauspielerei zurück; ihr letzter Filmauftritt war „Secrets“.
Während des Ersten Weltkriegs förderte sie den Verkauf von Liberty Bonds und hielt eine Reihe intensiver Spendenreden, beginnend in Washington, D.C., wo sie zusammen mit Charlie Chaplin, Douglas Fairbanks, Theda Bara und Marie Dressler Anleihen verkaufte.
In einer einzigen Rede in Chicago verkaufte sie Anleihen im geschätzten Wert von fünf Millionen Dollar.
Am Ende des Ersten Weltkriegs gründete Pickford den Motion Picture Relief Fund, eine Organisation zur Unterstützung finanziell bedürftiger Schauspieler.
Infolgedessen konnte der Fonds 1940 Grundstücke erwerben und das Motion Picture Country House and Hospital in Woodland Hills, Kalifornien, errichten.
Sie forderte (und erhielt) diese Befugnisse im Jahr 1916, als sie bei Zukors Famous Players in Famous Plays (später Paramount) unter Vertrag stand.
Die Mary Pickford Corporation war kurzzeitig Pickfords Filmproduktionsfirma.
Verleiher (ebenfalls Teil der Studios) sorgten dafür, dass die Produktionen des Unternehmens in den Kinosälen des Unternehmens gezeigt wurden.
Es handelte sich lediglich um eine Vertriebsgesellschaft, die unabhängigen Filmproduzenten den Zugang zu eigenen Kinosälen sowie die Vermietung von vorübergehend nicht ausgebuchten Kinosälen anderer Unternehmen ermöglichte.
Als Mitbegründerin sowie Produzentin und Star ihrer eigenen Filme wurde Pickford zur mächtigsten Frau, die jemals in Hollywood gearbeitet hat.
Sie und Chaplin blieben jahrzehntelang Partner im Unternehmen.
Es wird gemunkelt, dass sie Anfang der 1910er Jahre von Moore schwanger wurde und eine Fehlgeburt oder einen Schwangerschaftsabbruch erlitt.
Das Paar lebte mehrere Jahre lang mit Unterbrechungen zusammen.
Ungefähr zu dieser Zeit litt Pickford während der Grippepandemie von 1918 auch an der Grippe.
Für ihre Flitterwochen reisten sie nach Europa; Fans in London und Paris verursachten Aufstände, die versuchten, an das berühmte Paar heranzukommen.
Pickford verkörperte weiterhin das tugendhafte, aber feurige Mädchen von nebenan.
Ausländische Staatsoberhäupter und Würdenträger, die das Weiße Haus besuchten, fragten oft, ob sie auch Pickfair, die Villa des Paares in Beverly Hills, besuchen könnten.
Weitere Gäste waren George Bernard Shaw, Albert Einstein, Elinor Glyn, Helen Keller, H. G. Wells, Lord Mountbatten, Fritz Kreisler, Amelia Earhart, F. Scott Fitzgerald, Noël Coward, Max Reinhardt, Baron Nishi, Vladimir Nemirovich-Danchenko und Sir Arthur Conan Unter anderem Doyle, Austen Chamberlain, Sir Harry Lauder und Meher Baba.
Sie waren auch ständig als inoffizielle Botschafter Amerikas in der Welt zu sehen, führten Paraden an, durchschnitten Bänder und hielten Reden.
Sie ließen sich am 10. Januar 1936 scheiden.
Sie kritisierte ihre körperlichen Unvollkommenheiten, darunter Ronnies kleine Statur und Roxannes schiefe Zähne.
Ihre Geschwister Lottie und Jack starben beide 1936 bzw. 1933 an alkoholbedingten Ursachen.
Pickford zog sich zurück und wurde nach und nach ein Einsiedler, blieb fast ausschließlich in Pickfair und erlaubte nur Lillian Gish, ihrem Stiefsohn Douglas Fairbanks Jr. und wenigen anderen Menschen Besuche.
Sie erschien 1959 vor Gericht in einer Angelegenheit, die ihre Miteigentümerschaft am Fernsehsender WSJS-TV in North Carolina betraf.
Charles „Buddy“ Rogers gab seinen Gästen oft Führungen durch Pickfair, einschließlich der Besichtigung einer echten Westernbar, die Pickford für Douglas Fairbanks gekauft hatte, und eines Porträts von Pickford im Salon.
Sie besaß auch ein Haus in Toronto, Ontario, Kanada.
Ihre Hand- und Fußabdrücke werden im Grauman's Chinese Theatre in Hollywood, Kalifornien, ausgestellt.
Das Mary Pickford Theatre im James Madison Memorial Building der Library of Congress ist ihr zu Ehren benannt.
Ein erstes Kino in Cathedral City, Kalifornien, heißt The Mary Pickford Theatre und wurde am 25. Mai 2001 gegründet.
Darunter sind ein seltenes und spektakuläres Perlenkleid, das sie in dem von Mitchell Leisen entworfenen Film „Dorothy Vernon of Haddon Hall“ (1924) trug, ihr besonderer Oscar und eine Schmuckschatulle.
Das Haus der Familie war 1943 abgerissen und viele der Ziegel nach Pickford in Kalifornien geliefert worden.
1993 wurde ihr ein Golden Palm Star auf dem Palm Springs Walk of Stars gewidmet.
Von Januar 2011 bis Juli 2011 stellte das Toronto International Film Festival eine Sammlung von Erinnerungsstücken von Mary Pickford in der Canadian Film Gallery des TIFF Bell LightBox-Gebäudes aus.
Es wurde dem Keene State College gespendet und wird derzeit von der Library of Congress zur Ausstellung restauriert.
Das Google Doodle vom 8. April 2017 erinnerte an Mary Pickfords 125. Geburtstag.
Gloria Josephine May Swanson (27. März 1899 – 4. April 1983) war eine US-amerikanische Schauspielerin, Produzentin und Geschäftsfrau.
Ihre Schwärmerei als Schulmädchen für den Essanay-Studios-Schauspieler Francis X. Bushman führte dazu, dass ihre Tante sie mit auf eine Tour durch das Chicagoer Studio des Schauspielers nahm.
Ihr Tonfilmdebüt in „The Trespasser“ von 1929 brachte ihr eine zweite Oscar-Nominierung ein.
Ihr Vater war ein schwedischer Amerikaner und ihre Mutter war deutscher, französischer und polnischer Abstammung.
In beiden Versionen wurde sie bald als Statistin engagiert.
Ihre erste Rolle war eine kurze Walk-on-Rolle mit der Schauspielerin Gerda Holmes, die damals enorme 3,25 Dollar einbrachte.
Im Jahr 1915 spielte sie zusammen mit ihrem späteren ersten Ehemann Wallace Beery in „Sweedie Goes to College“ die Hauptrolle.
Vernon und Swanson projizierten eine großartige Chemie auf der Leinwand, die beim Publikum großen Anklang fand.
Badger war von Swanson so beeindruckt, dass er sie 1918 dem Regisseur Jack Conway für „Her Decision“ und „You Can't Believe Everything“ empfahl.
Bald darauf folgten „Something to Think About“ (1920) und „The Affairs of Anatol“ (1921).
Er war 1921 durch seinen Auftritt in „Die vier Reiter der Apokalypse“ zum Star geworden, aber Swanson kannte ihn seit seiner Zeit als aufstrebender Schauspieler, der kleine Rollen bekam und scheinbar keine Hoffnung auf eine berufliche Zukunft hatte.
An vielen historischen Orten im Zusammenhang mit Napoleon war das Filmen erstmals erlaubt.
Zu dieser Zeit galt Swanson als der zahlungsfähigste Star ihrer Zeit.
Die Produktion war eine Katastrophe, da Parker unentschlossen war und die Schauspieler nicht erfahren genug waren, um die von ihr gewünschten Leistungen zu erbringen.
Die Mitglieder unternahmen weitere Schritte, indem sie ihre Unzufriedenheit bei Will H. Hays, dem Vorsitzenden der Motion Picture Producers and Distributors of America, zum Ausdruck brachten.
Hays war von der Grundgeschichte begeistert, hatte jedoch spezifische Probleme, die vor der Veröffentlichung des Films geklärt wurden.
Er schlug vor, ihr nächstes Bild persönlich zu finanzieren und führte eine gründliche Prüfung ihrer Finanzunterlagen durch.
Kennedy riet ihr jedoch, Erich von Stroheim mit der Regie eines weiteren Stummfilms, The Swamp, zu engagieren, der später den Titel Queen Kelly erhielt.
Stroheim arbeitete mehrere Monate am Schreiben des Grundskripts.
Die Dreharbeiten wurden im Januar eingestellt und Stroheim wurde entlassen, nachdem sich Swanson über ihn und die allgemeine Ausrichtung des Films beschwert hatte.
„The Trespasser“ aus dem Jahr 1929 war eine Tonproduktion und brachte Swanson ihre zweite Oscar-Nominierung ein.
Die Weltpremiere fand in London statt, als erste amerikanische Tonproduktion überhaupt.
Perfect Understanding, eine Tonproduktionskomödie aus dem Jahr 1933, war der einzige von dieser Firma produzierte Film.
Sie begann in Bühnenproduktionen aufzutreten und spielte 1948 die Hauptrolle in „The Gloria Swanson Hour“ auf WPIX-TV.
Die Handlung des Films handelt von der verblassten Stummfilmschauspielerin Norma Desmond (Swanson), die in den gescheiterten Drehbuchautor Joe Gillis (William Holden) verliebt ist.
Norma spielt ein Kartenspiel Bridge mit einer Gruppe von Schauspielern, die auch als „die Wachsfiguren“ bekannt sind.
Normas Träume von einem Comeback werden zunichte gemacht, und als Gillis versucht, mit ihr Schluss zu machen, droht sie, sich umzubringen, tötet ihn aber stattdessen.
Obwohl Swanson Einwände dagegen hatte, sich einer Probeaufnahme für den Film zu unterziehen, war sie froh, viel mehr Geld zu verdienen als im Fernsehen und auf der Bühne.
Swanson moderierte später Crown Theatre with Gloria Swanson, eine Fernseh-Anthologieserie, in der sie gelegentlich mitspielte.
Sie war der „Mystery Guest“ bei What's My Line.
Sie hatte einen bemerkenswerten Auftritt in einer Folge von The Beverly Hillbillies aus dem Jahr 1966, in der sie sich selbst spielt.
Der Schauspieler und Dramatiker Harold J. Kennedy, der das Handwerk in Yale und am Mercury Theatre von Orson Wells gelernt hatte, schlug Swanson eine Tour durch „Reflected Glory“ vor, eine Komödie, die mit Tallulah Bankhead als Hauptdarsteller auf der Broadway-Bühne lief .
Nach ihrem Erfolg mit „Sunset Boulevard“ spielte sie am Broadway in einer Wiederaufnahme von „Twentieth Century“ mit José Ferrer und in „Nina“ mit David Niven.
Als Republikanerin unterstützte sie die Präsidentschaftskampagnen von Wendell Willkie in den Jahren 1940 und 1944 sowie die Präsidentschaftskampagne von Barry Goldwater im Jahr 1964.
Sie nahm von Beery Medikamente gegen morgendliche Übelkeit, trieb den Fötus ab und wurde bewusstlos ins Krankenhaus gebracht.
1923 adoptierte sie den einjährigen Sonny Smith, den sie nach ihrem Vater in Joseph Patrick Swanson umbenannte.
Sie hatte mit ihm ein Kind gezeugt, bevor ihre Scheidung von Somborn rechtskräftig war, eine Situation, die zu einem öffentlichen Skandal und möglicherweise zum Ende ihrer Filmkarriere geführt hätte.
Nach einer viermonatigen Erholung von ihrer Abtreibung kehrten sie als europäische Adlige in die Vereinigten Staaten zurück.
Er wurde Filmmanager und vertrat Pathé (USA) in Frankreich.
Swanson beschrieb sich selbst als „geistige Vampirin“, als jemand mit einer tiefen Neugier, wie Dinge funktionieren, und die nach Möglichkeiten suchte, diese Ideen in die Realität umzusetzen.
Sie lernten sich zufällig in Paris kennen, als Swanson 1931 von Coco Chanel für ihren Film „Tonight or Never“ angepasst wurde.
Ihre Freunde, von denen einige ihn offenkundig nicht mochten, dachten, sie würde einen Fehler machen.
Swanson hatte zunächst geglaubt, sie könne sich von der Schauspielerei zurückziehen, doch die Ehe wurde von Anfang an durch Daveys Alkoholismus getrübt.
Er war Co-Autor (Ghostwriter) von Billie Holidays Autobiografie „Lady Sings the Blues“, Autor von „Sugar Blues“, einem immer noch gedruckten Bestseller-Gesundheitsbuch aus dem Jahr 1975, und Autor der englischen Version von Georges Ohsawas „You Are All Sanpaku“.
Swanson und ihr Mann lernten John Lennon und Yoko Ono kennen, weil sie Fans von Duftys Werken waren.
Sie wurde eingeäschert und ihre Asche in der Episcopal Church of the Heavenly Rest an der Fifth Avenue in New York City beigesetzt, wo nur ein kleiner Familienkreis anwesend war.
1974 war Swanson einer der Preisträger des ersten Telluride Film Festivals.
Aufgrund des erotischen Charakters ihrer Auftritte wurden Nielsens Filme in den Vereinigten Staaten zensiert und ihre Arbeit blieb für das amerikanische Publikum relativ unbekannt.
Nielsens Familie zog während ihrer Kindheit mehrmals um, während ihr Vater eine Arbeit suchte.
Nielsens Vater starb, als sie vierzehn Jahre alt war.
Im Jahr 1901 wurde die 21-jährige Nielsen schwanger und gebar eine Tochter, Jesta.
Nielsen schloss 1902 die Theaterschule ab.
Nielsens minimalistischer Schauspielstil zeigte sich in ihrer gelungenen Darstellung einer naiven jungen Frau, die in ein tragisches Leben gelockt wird.
Nielsen und Gad heirateten und drehten dann vier weitere Filme zusammen.
Mir wurde klar, dass das Zeitalter des Kurzfilms vorbei war.
Durch den internationalen Filmverkauf erhielt Union acht Nielsen-Filme pro Jahr.
Ich habe alle verfügbaren Mittel genutzt – und viele neue erfunden –, um die Asta Nielsen-Filme der Welt zugänglich zu machen.“
In einer russischen Beliebtheitsumfrage von 1911 wurde Nielsen hinter Linder und vor ihrem dänischen Landsmann Valdemar Psilander zur weltbesten Filmstarin gewählt.
Im Jahr 1921 trat Nielsen über ihre eigene Filmvertriebsfirma Asta Films in Svend Gade auf, und Heinz Schall führte Regie bei Hamlet.
In wissenschaftlichen Arbeiten wie der 2010 vom Filmarchiv Austria veröffentlichten maßgeblichen Filmographie wird ein solcher Film jedoch nicht erwähnt.
Sie arbeitete bis zum Aufkommen des Tonfilms im deutschen Film.
Danach spielte Nielsen nur noch auf der Bühne.
Nielsen erkannte die Konsequenzen, lehnte ab und verließ Deutschland 1936.
Sie wurden 1919 geschieden, als Nielsen den schwedischen Schiffbauer Freddy Windgårdh heiratete.
Sie begannen eine langjährige Ehe nach dem Common Law, die von 1923 bis in die späten 1930er Jahre dauerte.
Fred Astaire (geb. Frederick Austerlitz; 10. Mai 1899 – 22. Juni 1987) war ein US-amerikanischer Tänzer, Schauspieler, Sänger, Choreograf und Fernsehmoderator.
Er spielte in mehr als zehn Broadway- und West End-Musicals mit, drehte 31 Musikfilme, vier Fernsehspecials und zahlreiche Aufnahmen.
Astaires Mutter wurde in den USA als Tochter lutherischer deutscher Einwanderer aus Ostpreußen und dem Elsass geboren.
Fritz suchte Arbeit im Braugewerbe und zog nach Omaha, Nebraska, wo er bei der Storz Brewing Company angestellt war.
Johanna plante für ihre beiden Kinder einen „Bruder-Schwester-Act“, wie er damals im Varieté üblich war.
Sie begannen ihre Ausbildung an der Alviene Master School of the Theatre and Academy of Cultural Arts.
Als Vorbereitung für die Entwicklung einer Darbietung wurden ihnen Tanz, Sprechen und Gesang beigebracht.
In einem Interview bemerkte Astaires Tochter Ava Astaire McKenzie, dass sie Fred oft einen Zylinder aufsetzten, um ihn größer aussehen zu lassen.
Dank des Verkaufsgeschicks ihres Vaters bekamen Fred und Adele einen Großauftrag und spielten auf dem Orpheum Circuit im Mittleren Westen, im Westen und in einigen Städten im Süden der USA.
Im Jahr 1912 wurde Fred Episkopalkirche.
Vom Varieté-Tänzer Aurelio Coccia lernten sie Tango, Walzer und andere von Vernon und Irene Castle populäre Gesellschaftstänze.
Er traf George Gershwin zum ersten Mal im Jahr 1916, der als Song Plugger für Jerome H. Remicks Musikverlag arbeitete.
Über ihre Arbeit in The Passing Show von 1918 schrieb Heywood Broun: „An einem Abend, an dem es reichlich gute Tänze gab, stach Fred Astaire heraus ... Er und seine Partnerin Adele Astaire sorgten dafür, dass die Show früh unterbrochen wurde.“ Abend mit einem wunderschönen lockeren Tanz.“
Aber zu diesem Zeitpunkt begannen Astaires Tanzfähigkeiten die seiner Schwester zu übertreffen.
Astaires Stepptanz galt damals als einer der besten.
Nach dem Ende von „Funny Face“ reisten die Astaires nach Hollywood, um bei Paramount Pictures einen (heute verlorenen) Probefilm zu machen, doch Paramount hielt sie für ungeeignet für Filme.
Das Ende der Partnerschaft war für Astaire traumatisch, regte ihn jedoch dazu an, sein Sortiment zu erweitern.
Sie liehen ihn 1933 für ein paar Tage an MGM für sein bedeutendes Hollywood-Debüt im erfolgreichen Musicalfilm Dancing Lady.
Er schrieb an seinen Agenten: „Es macht mir nichts aus, noch ein Foto mit ihr zu machen, aber was diese ‚Team‘-Idee betrifft, sie ist ‚out!‘
Die Partnerschaft und die Choreografie von Astaire und Hermes Pan trugen dazu bei, Tanzen zu einem wichtigen Element des Hollywood-Filmmusicals zu machen.
Sechs der neun Astaire-Rogers-Musicals wurden für RKO zu den größten Geldbringern; Alle Filme brachten ein gewisses Prestige und eine gewisse Kunstfertigkeit mit sich, die alle Studios damals begehrten.
Dadurch entstand die Illusion einer fast stationären Kamera, die einen ganzen Tanz in einer einzigen Einstellung filmt.
Astaires Stil der Tanzsequenzen ermöglichte es dem Zuschauer, die Tänzer und die Choreografie in ihrer Gesamtheit zu verfolgen.
Astaires zweite Innovation betraf den Kontext des Tanzes; Er bestand darauf, dass alle Gesangs- und Tanzeinlagen integraler Bestandteil der Handlungsstränge des Films sein sollten.
Eine davon wäre ein Soloauftritt von Astaire, den er sein „Sockensolo“ nannte.
Ich glaube, Ginger Rogers war es.
Sie hat es sehr vorgetäuscht.
1976 fragte der britische Talkshow-Moderator Sir Michael Parkinson Astaire, wer sein Lieblingstanzpartner bei „Parkinson“ sei.
Trotz ihres Erfolgs war Astaire nicht bereit, seine Karriere ausschließlich an eine Partnerschaft zu binden.
Während dieser Zeit schätzte Astaire weiterhin den Input choreografischer Mitarbeiter.
Sie spielten die Hauptrolle in „Broadway Melody“ von 1940, in dem sie eine gefeierte erweiterte Tanzeinlage zu Cole Porters „Begin the Beguine“ aufführten.
Er spielte an der Seite von Bing Crosby in „Holiday Inn“ (1942) und später in „Blue Skies“ (1946).
Der letztgenannte Film zeigte „Puttin‘ On the Ritz“, eine innovative Gesangs- und Tanzroutine, die unauslöschlich mit ihm verbunden ist.
Der erste Film, You'll Never Get Rich (1941), katapultierte Hayworth zum Star.
Es enthielt ein Duett zu Kerns „I'm Old Fashioned“, das 1983 zum Kernstück von Jerome Robbins‘ New York City Ballet-Hommage an Astaire wurde.
Astaire hat diesen Film alleine choreografiert und einen bescheidenen Kassenerfolg erzielt.
Die Fantasie Yolanda und der Dieb (1945) war ein avantgardistisches surrealistisches Ballett.
Astaire war immer unsicher und glaubte, seine Karriere würde ins Wanken geraten. Er überraschte sein Publikum, indem er während der Produktion seines nächsten Films Blue Skies (1946) seinen Rücktritt ankündigte.
Beide Filme belebten Astaires Popularität wieder und 1950 spielte er in zwei Musicals mit.
Während „Three Little Words“ an den Kinokassen recht gut abschnitt, war „Let's Dance“ eine finanzielle Enttäuschung.
Aufgrund der hohen Kosten konnte das Unternehmen jedoch bei seiner Erstveröffentlichung keinen Gewinn erzielen.
Dann wurde seine Frau Phyllis krank und starb plötzlich an Lungenkrebs.
„Daddy Long Legs“ schnitt an den Kinokassen nur mäßig gut ab.
Auch Astaires nächstes Projekt – sein letztes Musical bei MGM, Silk Stockings (1957), in dem er zusammen mit Cyd Charisse die Hauptrolle spielte – verlor an den Kinokassen Geld.
Das erste dieser Programme, An Evening with Fred Astaire aus dem Jahr 1958, gewann neun Emmy Awards, darunter „Beste Einzeldarbietung eines Schauspielers“ und „Herausragendste Einzelsendung des Jahres“.
Die Wahl stieß auf kontroverse Gegenreaktionen, da viele glaubten, sein Tanz in der Sondersendung sei nicht die Art von „Schauspielerei“, für die der Preis gedacht sei.
Sie restaurierten das Originalvideoband, übertrugen dessen Inhalt in ein modernes Format und füllten Lücken, in denen das Band beschädigt war, mit Bildröhrenaufnahmen.
Von 1957 bis 1969 trat Astaire in drei weiteren Filmen und mehreren Fernsehserien in nicht-tänzerischen Rollen auf.
Astaires Tanzpartnerin war Petula Clark, die die skeptische Tochter seiner Figur spielte.
Astaire war auch in den 1970er Jahren weiterhin als Schauspieler tätig.
In der zweiten Zusammenstellung führte er im Alter von 76 Jahren kurze Tanzsequenzen mit Kelly auf, seine letzten Tanzaufführungen in einem Musikfilm.
1978 spielte er zusammen mit Helen Hayes die Hauptrolle in dem gut aufgenommenen Fernsehfilm „A Family Upside Down“, in dem sie ein älteres Ehepaar spielten, das mit gesundheitlichen Problemen zu kämpfen hat.
Astaire bat seinen Agenten, ihm eine Rolle bei Galactica zu besorgen, da seine Enkelkinder Interesse an der Serie zeigten, und die Produzenten freuten sich über die Gelegenheit, eine ganze Episode mit ihm zu produzieren.
Lange nachdem die Fotografie für die Solo-Tanznummer „I Want to Be a Dancin‘ Man“ für den Spielfilm „The Belle of New York“ von 1952 abgeschlossen war, wurde entschieden, dass Astaires bescheidenes Kostüm und das abgenutzte Bühnenbild unzureichend waren, ebenso wie die gesamte Sequenz neu gedreht.
Bild für Bild sind die beiden Darbietungen bis in die subtilste Geste identisch.
Es handelte sich um einen Tanzstil mit einzigartigem Wiedererkennungswert, der den amerikanischen Smooth-Stil des Gesellschaftstanzes stark beeinflusste und Maßstäbe setzte, an denen spätere Filmtanzmusicals gemessen wurden.
Er stellt fest, dass Astaires Tanzstil in späteren Filmen, die mit oder ohne Pans Hilfe gedreht wurden, konsistent war.
Dies blieb jedoch fast immer auf den Bereich erweiterter Fantasy-Sequenzen, sogenannter „Traumballette“, beschränkt.
Später gab er zu: „Das meiste musste ich selbst machen.“
Viele Tanzroutinen basieren auf einem „Gimmick“, etwa dem Tanzen auf den Wänden in „Royal Wedding“ oder dem Tanzen mit seinen Schatten in „Swing Time“.
Sie arbeiteten mit einem Probenpianisten (häufig dem Komponisten Hal Borne) zusammen, der seinerseits den musikalischen Orchestratoren Änderungen mitteilte.
Wenn alle Vorbereitungen abgeschlossen sind, können die eigentlichen Dreharbeiten schnell vonstattengehen und Kosten sparen.
Er wird nicht einmal hingehen, um sich seine Binsen anzusehen... Er denkt immer, dass er nicht gut ist.“
Michael Kidd, Astaires Co-Choreograph des Films The Band Wagon aus dem Jahr 1953, stellte fest, dass Astaire seine eigene Besorgnis über die emotionale Motivation hinter dem Tanz nicht teilte.
„Lass uns die Looks später hinzufügen.“ "
Irving Berlin hielt Astaire für gleichwertig mit jedem männlichen Interpreten seiner Lieder – „so gut wie Jolson, Crosby oder Sinatra, nicht unbedingt wegen seiner Stimme, sondern wegen seiner Vorstellung, ein Lied zu projizieren.“
In seiner Blütezeit wurde Astaire in den Texten der Songwriter Cole Porter, Lorenz Hart und Eric Maschwitz erwähnt und inspiriert auch heute noch moderne Songwriter.
Im Jahr 1952 nahm Astaire The Astaire Story auf, ein vierbändiges Album mit einem Quintett unter der Leitung von Oscar Peterson.
Bogart begann mit der Schauspielerei in Broadway-Shows, begann seine Filmkarriere mit Up the River (1930) für Fox und trat im nächsten Jahrzehnt in Nebenrollen auf, manchmal in der Darstellung von Gangstern.
Bogarts Privatdetektive Sam Spade (in „The Maltese Falcon“) und Phillip Marlowe (in „The Big Sleep“ von 1946) wurden zu Vorbildern für Detektive in anderen Noir-Filmen.
Kurz nach den Dreharbeiten zu „The Big Sleep“ (1946, ihrem zweiten gemeinsamen Film) reichte er die Scheidung von seiner dritten Frau ein und heiratete Bacall.
Er wiederholte diese unruhigen, instabilen Charaktere als Kommandeur eines Marineschiffs im Zweiten Weltkrieg in „The Caine Mutiny“ (1954), der ein kritischer und kommerzieller Erfolg war und ihm eine weitere Nominierung als Bester Schauspieler einbrachte.
Der Name „Bogart“ leitet sich vom niederländischen Nachnamen „Bogaert“ ab.
Maud war eine Episkopalkirche englischer Abstammung und ein Nachkomme des Mayflower-Passagiers John Howland.
Clifford McCarty schrieb, dass die Werbeabteilung von Warner Bros. das Datum auf den 23. Januar 1900 geändert hatte, „um die Ansicht zu fördern, dass ein Mann, der am Weihnachtstag geboren wurde, nicht wirklich so schurkisch sein konnte, wie er auf der Leinwand zu sein schien“.
Lauren Bacall schrieb in ihrer Autobiografie, dass Bogarts Geburtstag immer am Weihnachtstag gefeiert wurde, und sagte, dass er jedes Jahr Witze darüber machte, dass er um ein Geschenk betrogen wurde.
Maud war eine kommerzielle Illustratorin, die ihre künstlerische Ausbildung in New York und Frankreich erhielt, einschließlich eines Studiums bei James Abbott McNeill Whistler.
Auf dem Höhepunkt ihrer Karriere verdiente sie über 50.000 US-Dollar pro Jahr – damals eine sehr große Summe und deutlich mehr als die 20.000 US-Dollar ihres Mannes.
Er hatte zwei jüngere Schwestern: Frances („Pat“) und Catherine Elizabeth („Kay“).
Ein Kuss war in unserer Familie ein Ereignis.
Von seinem Vater erbte er eine Vorliebe fürs Nadeln, eine Vorliebe fürs Angeln, eine lebenslange Liebe zum Bootfahren und eine Vorliebe für willensstarke Frauen.
Bogart besuchte später die Phillips Academy, ein Internat, in das er aufgrund familiärer Beziehungen aufgenommen wurde.
Es wurden mehrere Gründe angegeben; Einem Bericht zufolge wurde er von der Schule verwiesen, weil er den Schulleiter (oder einen Platzwart) in den Rabbit Pond auf dem Campus geworfen hatte.
Anschließend meldete er sich 1944 freiwillig für die vorübergehende Reserve der Küstenwache und patrouillierte mit seiner Yacht, der Santana, an der kalifornischen Küste.
In einem Fall wurde seine Lippe durch Granatsplitter aufgeschnitten, als sein Schiff (die ) beschossen wurde.
Beim Umsteigen in Boston soll der mit Handschellen gefesselte Gefangene Bogart um eine Zigarette gebeten haben.
Als Bogart von einem Arzt behandelt wurde, hatte sich eine Narbe gebildet.
Anstatt es zuzunähen, hat er es vermasselt.
Sein Charakter und seine Werte entwickelten sich während seiner Zeit bei der Marine getrennt von seiner Familie und er begann zu rebellieren.
Bogart nahm seine Freundschaft mit Bill Brady Jr. wieder auf (dessen Vater Verbindungen zum Showbusiness hatte) und bekam einen Bürojob bei William A. Bradys neuer Firma World Films.
Einige Monate später gab er sein Bühnendebüt als japanischer Butler in Alices Stück „Drifting“ von 1921 (in dem er nervös eine Dialogzeile vortrug) und trat in mehreren ihrer nachfolgenden Stücke auf.
Eine Kneipenschlägerei zu dieser Zeit war angeblich auch die Ursache für Bogarts Lippenverletzung, was mit dem Bericht von Louise Brooks übereinstimmt.
Bogart mochte seine trivialen, verweichlichten Rollen zu Beginn seiner Karriere nicht und nannte sie „White Pants Willie“-Rollen.
Menken sagte in ihrem Scheidungsantrag, dass Bogart seine Karriere mehr schätze als die Ehe und verwies auf Vernachlässigung und Missbrauch.
Dort traf er Spencer Tracy, einen Broadway-Schauspieler, den Bogart mochte und bewunderte, und sie wurden enge Freunde und Trinkgefährten.
Tracy erhielt die höchste Auszeichnung, aber Bogart erschien auf den Plakaten des Films.
Ein Vierteljahrhundert später planten die beiden Männer, gemeinsam „The Desperate Hours“ zu machen.
Von 1930 bis 1935 pendelte Bogart zwischen Hollywood und der New Yorker Bühne hin und her und war längere Zeit arbeitslos.
Obwohl Leslie Howard der Star war, sagte Brooks Atkinson, Kritiker der New York Times, dass das Stück „ein Pfirsich … ein tosendes Western-Melodram … Humphrey Bogart leistet die beste Arbeit seiner Karriere als Schauspieler.“
Warner Bros. kaufte 1935 die Filmrechte an „The Petrified Forest“.
Howard, der die Produktionsrechte besaß, machte deutlich, dass er wollte, dass Bogart mit ihm die Hauptrolle spielte.
Als Warner Bros. sah, dass Howard nicht nachgeben würde, gaben sie nach und besetzten Bogart.
Laut Variety lässt „Bogarts Bedrohung nichts zu wünschen übrig“.
In meinem Tonfall oder in diesem arroganten Gesicht muss etwas sein – etwas, das alle verärgert.
Trotz seines Erfolgs hatte Warner Bros. kein Interesse daran, Bogarts Bekanntheit zu steigern.
Bogart nutzte diese Jahre, um seine Filmpersönlichkeit zu entwickeln: ein verwundeter, stoischer, zynischer, charmanter, verletzlicher, selbstironischer Einzelgänger mit einem Ehrenkodex.
Seine Streitigkeiten mit Warner Bros. über Rollen und Geld ähnelten denen, die das Studio mit etablierteren und weniger formbaren Stars wie Bette Davis und James Cagney führte.
Seine einzige Hauptrolle in dieser Zeit war in Dead End (1937, ausgeliehen an Samuel Goldwyn), als Gangster nach dem Vorbild von Baby Face Nelson.
In Black Legion (1937), einem Film, den Graham Greene als „intelligent und aufregend, wenn auch eher ernst“ beschrieb, spielte er einen guten Mann, der von einer rassistischen Organisation erfasst (und von ihr zerstört) wurde.
Das Problem war, dass sie meins tranken und ich diesen stinkenden Film drehte.
Am 21. August 1938 ging Bogart eine turbulente dritte Ehe mit der Schauspielerin Mayo Methot ein, einer lebhaften, freundlichen Frau, wenn sie nüchtern war, aber paranoid und aggressiv, wenn sie betrunken war.
Sie zündete ihr Haus an, stach mit einem Messer auf ihn ein und schnitt sich mehrmals die Handgelenke auf.
Laut ihrem Freund Julius Epstein war „die Bogart-Methot-Ehe die Fortsetzung des Bürgerkriegs“.
Methots Einfluss wurde jedoch immer destruktiver und auch Bogart trank weiterhin.
Wenn er dachte, ein Schauspieler, Regisseur oder Studio hätte etwas Schlimmes getan, äußerte er sich öffentlich darüber.
Paul Muni, George Raft, Cagney und Robinson lehnten die Hauptrolle ab und gaben Bogart die Gelegenheit, einen Charakter mit einiger Tiefe zu spielen.
Er arbeitete gut mit Ida Lupino zusammen und löste bei Mayo Methot Eifersucht aus.
Er konnte Platon, Pope, Ralph Waldo Emerson und über tausend Zeilen von Shakespeare zitieren und abonnierte die Harvard Law Review.
Basierend auf dem Roman von Dashiell Hammett wurde es erstmals 1929 in der Pulp-Zeitschrift Black Mask veröffentlicht und bildete die Grundlage für zwei frühere Filmversionen; der zweite war Satan Met a Lady (1936) mit Bette Davis in der Hauptrolle.
Huston akzeptierte Bogart dann eifrig als seinen Sam Spade.
Der von Michael Curtiz inszenierte und von Hal Wallis produzierte Film zeigte Ingrid Bergman, Claude Rains, Sydney Greenstreet, Paul Henreid, Conrad Veidt, Peter Lorre und Dooley Wilson.
Berichten zufolge war Bogart für die Idee verantwortlich, dass Rick Blaine als Schachspieler dargestellt werden sollte, eine Metapher für die Beziehungen, die er zu Freunden, Feinden und Verbündeten pflegte.
Bogart wurde als bester Hauptdarsteller nominiert, verlor jedoch für seine Leistung in „Watch on the Rhine“ gegen Paul Lukas.
Bogart unternahm 1943 und 1944 mit Methot Touren zu United Service Organizations und War Bonds und unternahm beschwerliche Reisen nach Italien und Nordafrika (einschließlich Casablanca).
Als sie sich trafen, war Bacall 19 und Bogart 44; er gab ihr den Spitznamen „Baby“.
Wir werden viel Spaß zusammen haben.“
Von mir selbst und noch mehr, HarperCollins, New York, 2005.
Er betrachtete sich als Bacalls Beschützer und Mentor, und Bogart usurpierte diese Rolle.
Außerdem hat er einen Sinn für Humor, der einen ätzenden Unterton der Verachtung enthält.“
Die Dialoge, insbesondere in den von Hawks beigesteuerten Zusatzszenen, waren voller sexueller Anspielungen und Bogart überzeugt als Privatdetektiv Philip Marlowe.
Die Ehe verlief glücklich, mit Spannungen aufgrund ihrer Differenzen.
Laut Bogarts Biographen Stefan Kanfer handelte es sich um „einen Produktionsfilm Noir ohne besondere Unterscheidung“.
Da es kein Liebesinteresse oder ein Happy End gab, galt es als riskantes Projekt.
James Agee schrieb: „Bogart leistet mit dieser Figur einen wunderbaren Job … der sehr guten Arbeit, die er zuvor geleistet hat, meilenweit voraus.“
Bogart trat in seinen letzten Warners-Filmen auf: Chain Lightning (1950) und The Enforcer (1951).
Santana drehte auch zwei Filme ohne ihn: And Baby Makes Three (1949) und The Family Secret (1951).
Mehrere Bogart-Biographen und die Schauspielerin und Autorin Louise Brooks waren der Meinung, dass diese Rolle dem echten Bogart am nächsten kommt.
Beat the Devil war eine Art Parodie auf „The Maltese Falcon“ und der letzte Film für Bogart und John Huston.
Hustons Liebe zum Abenteuer, seine tiefe, langjährige Freundschaft (und sein Erfolg) mit Bogart und die Chance, mit Hepburn zusammenzuarbeiten, überzeugten den Schauspieler, Hollywood zu verlassen, um einen schwierigen Dreh vor Ort in Belgisch-Kongo zu machen.
Bacall kam für mehr als vier Monate und ließ ihren kleinen Sohn in Los Angeles zurück.
Sie hat meine Unterwäsche im dunkelsten Afrika verschönert.
Hepburn (einem Abstinenzler) erging es unter den schwierigen Bedingungen schlechter, er verlor an Gewicht und wurde irgendwann sehr krank.
Trotz der Unbequemlichkeit, vom Boot in Sümpfe, Flüsse und Sümpfe zu springen, entfachte The African Queen offenbar Bogarts frühe Liebe zu Booten; Als er nach Kalifornien zurückkehrte, kaufte er einen klassischen Hacker-Craft-Flitzer aus Mahagoni, den er bis zu seinem Tod behielt.
Als Bogart jedoch gewann, sagte er: „Von Belgisch-Kongo bis zur Bühne dieses Theaters ist es ein weiter Weg.“
Wie beim Tennis brauchen Sie einen guten Gegner oder Partner, der das Beste aus Ihnen herausholt.
Obwohl er etwas von seiner alten Verbitterung darüber bewahrt hatte, lieferte er an der Spitze eine starke Leistung ab; Er erhielt seine letzte Oscar-Nominierung und war Gegenstand einer Titelgeschichte des Time-Magazins vom 7. Juni 1954.
Er ist der Typ Regisseur, mit dem ich nicht gerne zusammenarbeite ... der Film ist ein Haufen Mist.
Trotz der Schärfe war der Film erfolgreich; Laut einer Rezension in der New York Times war Bogart „unglaublich geschickt … die Fähigkeit, mit der dieser alte, steinharte Schauspieler die Gags und solche Doppelzüngigkeiten mit einer männlichen Art des Schmelzens mischt, ist eine der unberechenbaren Freuden der Show.“ .
Ava Gardner in der weiblichen Hauptrolle bereitete ihm Unbehagen; Sie hatte gerade mit seinem Rat Pack-Kumpel Frank Sinatra Schluss gemacht, und Bogart ärgerte sich über ihre unerfahrene Leistung.
Als Bacall sie zusammen fand, entlockte sie ihrem Mann einen teuren Einkaufsbummel; Die drei reisten nach der Schießerei zusammen.
Er trat auch in der Jack Benny Show auf, wo ihn eine erhaltene Bildröhre der Live-Übertragung bei seinem einzigen TV-Sketch-Comedy-Auftritt (25. Oktober 1953) festhält.
Stephen wurde Autor und Biograf und moderierte eine Fernsehsondersendung über seinen Vater bei Turner Classic Movies.
Im Gefolge von Santana hatte Bogart eine neue Firma gegründet und Pläne für einen Film (Melville Goodwin, USA), in dem er einen General und Bacall einen Pressemagnaten spielen sollte.
Er sprach nicht über seinen Gesundheitszustand und suchte nach erheblicher Überredung durch Bacall im Januar 1956 einen Arzt auf.
Im November 1956 unterzog er sich einer weiteren Operation, als der Krebs metastasiert war.
Darauf stand: „Wenn Sie etwas wollen, pfeifen Sie einfach.“
Nachdem er in den 1940er Jahren bei Stella Adler studiert hatte, gilt er als einer der ersten Schauspieler, die das Stanislawski-Schauspielsystem und das aus dem Stanislawski-System abgeleitete Method Acting dem Mainstream-Publikum zugänglich machten.
Er führte Regie und spielte die Hauptrolle in dem Kult-Western „One-Eyed Jacks“, einem kritischen und kommerziellen Flop, woraufhin er eine Reihe bemerkenswerter Kassenschläger hinlegte, beginnend mit „Meuterei auf der Bounty“ (1962).
Er lehnte die Auszeichnung wegen angeblicher Misshandlung und Fehldarstellung der amerikanischen Ureinwohner durch Hollywood ab.
Laut Guinness-Buch der Rekorde erhielt Brando die Rekordsumme von 3,7 Millionen US-Dollar (inflationsbereinigt Millionen US-Dollar) und 11,75 % des Bruttogewinns für 13 Tage Arbeit an „Superman“.
Seine Vorfahren waren überwiegend Deutsche, Holländer, Engländer und Iren.
Brando wurde als christlicher Wissenschaftler erzogen.
Allerdings war sie Alkoholikerin und musste oft von ihrem Mann aus Bars in Chicago nach Hause gebracht werden.
Brando hegte weitaus größere Feindseligkeit gegenüber seinem Vater und erklärte: „Ich war sein Namensvetter, aber nichts, was ich getan habe, hat ihn jemals erfreut oder auch nur interessiert.“
Um 1930 zogen Brandos Eltern nach Evanston, Illinois, als die Arbeit seines Vaters ihn nach Chicago führte. Sie trennten sich jedoch 1935, als Brando 11 Jahre alt war.
Brando, dessen Spitzname in seiner Kindheit „Bud“ war, war ein Nachahmer aus seiner Jugend.
In der TCM-Biografie „Brando: The Documentary“ aus dem Jahr 2007 erinnert sich George Englund, ein Freund aus Kindertagen, an Brandos erstes Verhalten, indem er die Kühe und Pferde auf der Familienfarm nachahmte, um seine Mutter vom Trinken abzulenken.
Brandos Schwester Frances verließ das College in Kalifornien, um in New York Kunst zu studieren.
Brando war ein hervorragender Theaterspieler und schnitt in der Schule gut ab.
Die Fakultät stimmte für seinen Ausschluss, obwohl er von den Studenten unterstützt wurde, die den Ausschluss für zu hart hielten.
In einem Dokumentarfilm aus dem Jahr 1988, Marlon Brando: The Wild One, erinnerte sich Brandos Schwester Jocelyn: „Er war in einer Schulaufführung und hat es genossen ... Also beschloss er, nach New York zu gehen und Schauspiel zu studieren, weil das das Einzige war, was er hatte.“ genossen.
Eine Zeit lang lebte er mit Roy Somlyo zusammen, der später ein vierfacher Emmy-prämierter Broadway-Produzent wurde.
Brandos bemerkenswerte Einsicht und sein Sinn für Realismus waren schon früh offensichtlich.
Laut Dustin Hoffman in seiner Online-Meisterklasse sprach Brando oft mit Kameramännern und Schauspielerkollegen über ihr Wochenende, selbst nachdem der Regisseur zum Handeln aufgerufen hatte.
Sein Verhalten führte dazu, dass er aus der Besetzung der New School-Produktion in Sayville ausgeschlossen wurde, aber bald darauf wurde er dort in einem lokal produzierten Stück entdeckt.
Cornell besetzte ihn im selben Jahr auch als Boten in ihrer Inszenierung von Jean Anouilhs Antigone.
Bankhead hatte die Rolle der Blanche Dubois in „A Streetcar Named Desire“, die Williams für sie geschrieben hatte, abgelehnt, um das Stück für die Saison 1946–1947 auf Tournee zu bringen.
Wilson war gegenüber Brandos Verhalten weitgehend tolerant, erreichte jedoch seine Grenzen, als Brando kurz vor der Eröffnung am 28. November 1946 während einer Generalprobe murmelte. "
„Es war wunderbar“, erinnerte sich ein Darsteller. „
Kritiker waren jedoch nicht so freundlich.
Bei den folgenden Tourstopps erhielt er zwar bessere Kritiken, aber was seine Kollegen in Erinnerung hatten, waren nur vereinzelte Hinweise auf das Talent, das er später unter Beweis stellen würde. "
Brando zeigte seine Apathie gegenüber der Produktion, indem er auf der Bühne einige schockierende Manieren an den Tag legte.
Nach mehreren Wochen unterwegs erreichten sie Boston, als Bankhead bereit war, ihn zu entlassen.
Pierpont schreibt, dass John Garfield die erste Wahl für die Rolle gewesen sei, aber „unmögliche Forderungen gestellt“ habe.
Es vermenschlicht den Charakter von Stanley, indem er eher zur Brutalität und Gefühllosigkeit der Jugend wird als zu einem bösartigen alten Mann … Ein neuer Wert ergab sich aus Brandos Lektüre, die bei weitem die beste Lektüre war, die ich je gehört habe.“
Er sagte: „Der Vorhang ging auf und auf der Bühne steht dieser Hurensohn aus dem Fitnessstudio, und er spielt mit mir.“
Brandos erste Filmrolle war die eines verbitterten querschnittsgelähmten Veteranen in The Men (1950).
Nach Brandos eigenen Angaben könnte es an diesem Film gelegen haben, dass sein Draft-Status von 4-F auf 1-A geändert wurde. Er hatte sich einer Operation am falschen Knie unterzogen und die körperliche Beeinträchtigung war nicht mehr so stark, dass er vom Draft ausgeschlossen werden musste.
Zufälligerweise kannte der Psychiater einen Arzt, der mit Brando befreundet war.
Die Rolle gilt als eine der größten von Brando.
Regie führte Elia Kazan, in der Hauptrolle spielte Anthony Quinn.
Während unserer gemeinsamen Szenen spürte ich eine Bitterkeit mir gegenüber, und wenn ich ihm nach der Arbeit einen Drink vorschlug, lehnte er entweder ab oder war mürrisch und sagte wenig.
Nachdem er den gewünschten Effekt erzielt hatte, erzählte Kazan Quinn nie, dass er ihn in die Irre geführt hatte.
Gielgud war so beeindruckt, dass er Brando eine komplette Spielzeit am Hammersmith Theatre anbot, ein Angebot, das er ablehnte.
Es war, als würde sich eine Ofentür öffnen – die Hitze entwich vom Schirm.
Angeblich war Brando über die Entscheidung seines Mentors verärgert, arbeitete aber in „On The Waterfront“ erneut mit ihm zusammen. "
Die Importeure von Triumph reagierten ambivalent auf die Enthüllung, da es sich um die Übernahme einer Kleinstadt durch lautstarke Motorradbanden handelte.
Als ihm die Rolle zunächst angeboten wurde, lehnte Brando – immer noch verärgert über Kazans Aussage gegenüber HUAC – ab und die Rolle des Terry Malloy ging beinahe an Frank Sinatra.
Brando gewann den Oscar für seine Rolle als irisch-amerikanischer Stauer Terry Malloy in „On the Waterfront“.
In seiner Rezension vom 29. Juli 1954 lobte der Kritiker der New York Times, A. H. Weiler, den Film und nannte ihn „einen ungewöhnlich kraftvollen, aufregenden und einfallsreichen Einsatz der Leinwand durch begabte Profis“.
Er verkörperte Napoleon im Film Désirée aus dem Jahr 1954.
Besonders verächtlich äußerte Brando den Regisseur Henry Koster.
Auch die Beziehungen zwischen Brando und Co-Star Frank Sinatra waren frostig, und Stefan Kanfer bemerkte: „Die beiden Männer waren diametrale Gegensätze: Marlon brauchte mehrere Takes; Frank verabscheute es, sich zu wiederholen.“
Frank Sinatra bezeichnete Brando als „den am meisten überbewerteten Schauspieler der Welt“ und bezeichnete ihn als „Mumbles“.
Pauline Kael war von dem Film nicht besonders beeindruckt, bemerkte jedoch: „Marlon Brando hat sich gehungert, um die Pixie-Dolmetscherin Sakini zu spielen, und er sieht aus, als würde ihm der Stunt Spaß machen – er spricht mit verrücktem Akzent, grinst jungenhaft, beugt sich nach vorne und macht knifflige Bewegungen.“ mit seinen Beinen.
Newsweek hielt den Film für eine „langweilige Geschichte über das Treffen der beiden“, war aber dennoch ein Kassenerfolg.
Der Film gewann vier Oscars.
Allen Berichten zufolge war Brando durch ihren Tod am Boden zerstört, und der Biograph Peter Manso erzählte A&E's Biography: „Sie war diejenige, die ihm Zustimmung geben konnte wie kein anderer, und nach dem Tod seiner Mutter scheint es, dass Marlon sich nicht mehr darum kümmert.“
„The Young Lions“ zeigt auch Brandos einzigen Auftritt in einem Film mit seinem Freund und Rivalen Montgomery Clift (obwohl sie keine gemeinsamen Szenen hatten).
Brando spielt die Hauptfigur Rio und Karl Malden spielt seinen Partner „Dad“ Longworth.
Brandos Unerfahrenheit als Cutter verzögerte auch die Postproduktion und Paramount übernahm schließlich die Kontrolle über den Film.
Zu diesem Zeitpunkt war mir das ganze Projekt langweilig und ich habe es aufgegeben.“
Berichten zufolge überkochte Brandos Abscheu gegenüber der Filmindustrie am Set seines nächsten Films, Metro-Goldwyn-Mayers Remake von Mutiny on the Bounty, der in Tahiti gedreht wurde.
Mutiny-Regisseur Lewis Milestone behauptete, dass die Führungskräfte „verdienen, was sie bekommen, wenn sie einem Hobbyschauspieler, einem gereizten Kind, die vollständige Kontrolle über einen teuren Film geben.“
Der hässliche Amerikaner (1963) war der erste dieser Filme.
Alle anderen Universal-Filme von Brando in dieser Zeit, darunter Bedtime Story (1964), The Appaloosa (1966), A Countess from Hong Kong (1967) und The Night of the Following Day (1969), waren ebenfalls kritische und kommerzielle Misserfolge.
Brando war 1965 auch im Spionagethriller Morituri aufgetreten; Auch das konnte kein Publikum anlocken.
Candy war für viele besonders entsetzlich; Ein Sex-Farce-Film aus dem Jahr 1968 unter der Regie von Christian Marquand, der auf dem Roman von Terry Southern aus dem Jahr 1958 basiert. Der Film persifliert pornografische Geschichten anhand der Abenteuer seiner naiven Heldin Candy, gespielt von Ewa Aulin.
In der Märzausgabe 1966 von The Atlantic schrieb Pauline Kael, dass Brando in seinen rebellischen Tagen „asozial war, weil er wusste, dass die Gesellschaft Mist war; er war ein Held für die Jugend, weil er stark genug war, den Mist nicht zu ertragen“, aber jetzt Brando und andere wie er seien zu „Blöcken geworden, die ihren öffentlichen Ruf schamlos und erbärmlich verspotten“.
Ich war in meiner Haltung der Gleichgültigkeit sehr überzeugend, aber ich war sehr empfindlich und es tat sehr weh.
Insgesamt erhielt der Film gemischte Kritiken.
Brando widmete dem Film in seinen Memoiren ein ganzes Kapitel und erklärte, dass der Regisseur Gillo Pontecorvo neben Kazan und Bernardo Bertolucci der beste Regisseur sei, mit dem er je zusammengearbeitet habe.
1971 führte Michael Winner Regie in dem britischen Horrorfilm The Nightcomers mit Stephanie Beacham, Thora Hird, Harry Andrews und Anna Palk.
Bei den New York Film Critics Circle Awards 1972 besiegte er Brando.
Brando hatte auch One-Eyed Jacks gegen sich, eine problematische Produktion, die Paramount bei der Veröffentlichung im Jahr 1961 Geld einbüßte.
Coppola überzeugte Brando zu einem auf Video aufgezeichneten „Make-up“-Test, bei dem Brando sein eigenes Make-up machte (er benutzte Wattebällchen, um die aufgeblähten Wangen der Figur zu simulieren).
Brando hatte selbst Zweifel und erklärte in seiner Autobiografie: „Ich hatte noch nie zuvor Italienisch gespielt und glaubte nicht, dass ich das erfolgreich hinbekommen würde.“
Brando wurde für ein geringes Honorar von 50.000 US-Dollar unter Vertrag genommen, in seinem Vertrag wurde ihm jedoch ein Prozentsatz des Bruttoeinkommens auf einer gleitenden Skala zugewiesen: 1 % des Bruttoeinkommens für jede 10 Millionen US-Dollar über einem Schwellenwert von 10 Millionen US-Dollar, bis zu 5 %, wenn das Bild angegeben ist über 60 Millionen US-Dollar.
In einem Interview aus dem Jahr 1994, das auf der Website der Academy of Achievement zu finden ist, betonte Coppola: „Der Pate war ein sehr unbeachteter Film, als wir ihn drehten.“
Ihnen gefiel die Art und Weise, wie ich es drehte, nicht.
In einem Fernsehinterview mit Larry King im Jahr 2010 sprach Al Pacino auch darüber, wie Brandos Unterstützung ihm half, die Rolle des Michael Corleone im Film zu behalten – obwohl Coppola ihn feuern wollte.
Er brach das Eis, indem er mit einem Glas Wein auf die Gruppe anstieß.
Caan fügt hinzu: „Am ersten Tag, als wir Brando trafen, waren alle voller Ehrfurcht.“
Und weil er so viel Macht und unbestrittene Autorität hatte, dachte ich, es wäre ein interessanter Kontrast, ihn als sanften Mann zu spielen, im Gegensatz zu Al Capone, der Leute mit Baseballschlägern verprügelte.“
Es gab wirklich keinen Anfang.
Er boykottierte die Preisverleihung und schickte stattdessen die Aktivistin für die Rechte indigener Amerikaner, Sacheen Littlefeather, die in voller Apache-Kleidung auftrat, um Brandos Gründe darzulegen, die auf seinem Einwand gegen die Darstellung indigener Amerikaner durch Hollywood und das Fernsehen beruhten.
Wie bei früheren Filmen weigerte sich Brando für viele Szenen, seinen Text auswendig zu lernen; Stattdessen schrieb er seine Zeilen auf Stichwortkarten und hängte sie zum leichteren Nachschlagen rund um das Set, sodass Bertolucci das Problem hatte, sie aus dem Bilderrahmen herauszuhalten.
Sein Bruttobeteiligungsvertrag brachte ihm 3 Millionen Dollar ein.
Pauline Kael schrieb in der Rezension des New Yorker: „Der Durchbruch des Films ist endlich gekommen.“
1973 war Brando durch den Tod seines besten Freundes aus Kindertagen, Wally Cox, am Boden zerstört.
Während der ersten Stunde des Films abwesend, kommt Clayton zu Pferd herein, kopfüber baumelnd, mit weißem Wildleder im Littlefeather-Stil geschmückt.
Penn, der davon überzeugt war, dass man die Schauspieler ihr Ding machen lassen sollte, hat Marlon voll und ganz nachgegeben.
1978 erzählte Brando die englische Version von Raoni, einem französisch-belgischen Dokumentarfilm unter der Regie von Jean-Pierre Dutilleux und Luiz Carlos Saldanha, der sich auf das Leben von Raoni Metuktire und Fragen rund um das Überleben der indigenen Indianerstämme im Norden Zentralbrasiliens konzentrierte.
1979 hatte er einen seltenen Fernsehauftritt in der Miniserie Roots: The Next Generations, in der er George Lincoln Rockwell porträtierte; Für seine Leistung gewann er einen Primetime Emmy Award als herausragender Nebendarsteller in einer Miniserie oder einem Film.
Brando erhielt für dreiwöchige Arbeit eine Million Dollar pro Woche.
In der Dokumentation spricht Coppola darüber, wie erstaunt er war, als der übergewichtige Brando zu seinen Szenen auftauchte und sich verzweifelt dazu entschloss, Kurtz, der in der Originalgeschichte abgemagert wirkt, als einen Mann darzustellen, der sich in jeder Hinsicht verwöhnt hatte.
Allerdings kehrte er 1989 in A Dry White Season zurück, basierend auf André Brinks Anti-Apartheid-Roman von 1979.
Brando wurde für seine Leistung gelobt, erhielt eine Oscar-Nominierung als bester Nebendarsteller und gewann den Preis als bester Hauptdarsteller beim Tokyo Film Festival.
Variety lobte auch Brandos Leistung als Sabatini und bemerkte: „Marlon Brandos großartige komödiantische Leistung erhebt The Freshman von einer Screwball-Komödie zu einer skurrilen Nische in der Filmgeschichte.“
Der Drehbuchautor von The Island of Dr. Moreau, Ron Hutchinson, sagte später in seinen Memoiren „Clinging to the Iceberg: Writing for a Living on the Stage and in Hollywood“ (2017), dass Brando die Produktion des Films sabotiert habe, indem er sich mit seinen Kollegen gestritten und die Zusammenarbeit verweigert habe und das Filmteam.
Dies war seine letzte Rolle und seine einzige Rolle als weibliche Figur.
Der Sohn des Schauspielers, Miko, war mehrere Jahre lang Jacksons Leibwächter und Assistent und ein Freund des Sängers.
Papa hatte in seinen letzten Tagen Schwierigkeiten beim Atmen und war die meiste Zeit mit Sauerstoff versorgt.
Also besorgte Michael seinem Vater einen Golfwagen mit einer tragbaren Sauerstoffflasche, damit er herumfahren und Nimmerland genießen konnte.
Außerdem litt er an Diabetes und Leberkrebs.
Seine einzige aufgenommene Zeile wurde als Hommage an den Schauspieler in das letzte Spiel aufgenommen.
Ein verzweifelter Brando erzählte Malden, dass er immer wieder hinfiel.
Kurz vor seinem Tod hatte er offenbar die Einführung von Sauerstoffschläuchen in seine Lunge verweigert, da dies, wie ihm gesagt wurde, die einzige Möglichkeit sei, sein Leben zu verlängern.
1976 sagte er zu einem französischen Journalisten: „Homosexualität ist so sehr in Mode, dass sie keine Schlagzeilen mehr macht.“
Er behauptete auch, zahlreiche andere Romanzen gehabt zu haben, obwohl er in seiner Autobiografie nicht auf seine Ehen, seine Frauen oder seine Kinder einging.
Brando lernte 1954 die Schauspielerin Rita Moreno kennen und sie begannen eine Liebesbeziehung.
Jahre nach ihrer Trennung spielte Moreno seine Liebe in dem Film „Die Nacht des folgenden Tages“ aus.
Sie soll die Tochter eines walisischen Stahlarbeiters irischer Abstammung, William O'Callaghan, gewesen sein, der Superintendent bei den indischen Staatsbahnen gewesen war.
Brando und Kashfi bekamen am 11. Mai 1958 einen Sohn, Christian Brando; Sie ließen sich 1959 scheiden.
Sie hatten zwei gemeinsame Kinder: Miko Castaneda Brando (geb. 1961) und Rebecca Brando (geb. 1966).
Da Teriipaia Französisch als Muttersprache hatte, beherrschte Brando die Sprache fließend und gab zahlreiche Interviews auf Französisch.
Brando und Teriipaia ließen sich im Juli 1972 scheiden.
Brando hatte eine langjährige Beziehung mit seiner Haushälterin Maria Cristina Ruiz, mit der er drei Kinder hatte: Ninna Priscilla Brando (geboren am 13. Mai 1989), Myles Jonathan Brando (geboren am 16. Januar 1992) und Timothy Gahan Brando (geboren im Januar). 6, 1994).
Zu seinen zahlreichen Enkelkindern gehören auch Prudence Brando und Shane Brando, Kinder von Miko C. Brando; die Kinder von Rebecca Brando; und unter anderem die drei Kinder von Teihotu Brando.
Sein Verhalten während der Dreharbeiten zu „Meuterei auf der Bounty“ (1962) schien seinen Ruf als schwieriger Star zu festigen.
Galella war Brando, der von Talkshow-Moderator Dick Cavett begleitet wurde, nach einer Aufzeichnung der Dick Cavett Show in New York City gefolgt.
Die Dreharbeiten zu „Meuterei auf der Bounty“ beeinflussten Brandos Leben tiefgreifend, da er sich in Tahiti und seine Menschen verliebte.
Der Hurrikan von 1983 zerstörte viele Gebäude, darunter auch sein Resort.
Um seine Privatsphäre zu schützen, wurde er in den Aufzeichnungen der Federal Communications Commission (FCC) als Martin Brandeaux aufgeführt.
Er nahm an einigen Spendenaktionen für John F. Kennedy bei den Präsidentschaftswahlen 1960 teil.
Im Herbst 1967 besuchte Brando Helsinki, Finnland, auf einer von UNICEF organisierten Wohltätigkeitsparty im Helsinki City Theatre.
Er sprach sich für Kinderrechte und Entwicklungshilfe in Entwicklungsländern aus.
Ich hatte das Gefühl, ich sollte besser herausfinden, wo es ist; was es heißt, in diesem Land schwarz zu sein; „Worum es bei dieser Wut geht“, sagte Brando in der Late-Night-Talkshow Joey Bishop Show von ABC-TV.
Es war eine der unglaublichsten mutigen Taten, die ich je gesehen habe, und sie bedeutete viel und hat viel bewirkt.“
Im Jahr 1964 wurde Brando bei einem „Fish-in“ festgenommen, um gegen einen gebrochenen Vertrag zu protestieren, der den amerikanischen Ureinwohnern Fischereirechte im Puget Sound versprochen hatte.
Brando beendete seine finanzielle Unterstützung für die Gruppe aufgrund seiner Wahrnehmung einer zunehmenden Radikalisierung, insbesondere einer Passage in einer Panther-Broschüre von Eldridge Cleaver, in der wahllose Gewalt „für die Revolution“ befürwortet wurde.
Sacheen Littlefeather vertrat ihn bei der Zeremonie.
Die Veranstaltung erregte die Aufmerksamkeit der Medien in den USA und weltweit.
Er war auch ein Aktivist gegen die Apartheid.
Er wird vom American Film Institute als viertgrößter männlicher Star aufgeführt, dessen Filmdebüt vor oder während des Jahres 1950 stattfand (es geschah im Jahr 1950).
Abgerufen am 19. August 2009. Encyclopedia Britannica beschreibt ihn als „den berühmtesten Methodschauspieler, und seine undeutliche, murmelnde Darbietung markierte seine Ablehnung der klassischen Schauspielausbildung.“
Er war eine Weiterentwicklung des Gangsterführers und des Gesetzlosen.
Seine Darstellung des Bandenführers Johnny Strabler in „The Wild One“ ist zu einem ikonischen Bild geworden, das sowohl als Symbol der Rebellion als auch als Modeaccessoire verwendet wird, zu dem eine Motorradjacke im Perfecto-Stil, eine Schirmmütze, Jeans und eine Sonnenbrille gehören.
Die „Ich hätte ein Anwärter sein können“-Szene aus „On the Waterfront“ ist laut dem Autor von „Brooklyn Boomer“, Martin H. Levinson, „eine der berühmtesten Szenen in der Filmgeschichte, und die Zeile selbst ist Teil der amerikanischen Kultur geworden.“ Lexikon."
Du musst sie glauben machen, dass du stirbst ... Versuchen Sie, an den intimsten Moment zu denken, den Sie jemals in Ihrem Leben hatten.
1999 wählte ihn das American Film Institute auf Platz acht seiner Liste der größten männlichen Stars des Goldenen Zeitalters Hollywoods.
Er verbrachte mehrere Jahre als Tänzer und Komiker im Varieté, bis er 1925 seine erste große Schauspielrolle bekam.
Nach begeisterten Kritiken verpflichtete Warner Bros. ihn für einen ersten Dreiwochenvertrag über 400 US-Dollar pro Woche; Als die Verantwortlichen des Studios die ersten Tageszeitungen zum Film sahen, wurde Cagneys Vertrag sofort verlängert.
1955 wurde er ein drittes Mal für „Love Me or Leave Me“ mit Doris Day nominiert.
Cagney verließ Warner Bros. im Laufe seiner Karriere mehrmals und kehrte jedes Mal mit deutlich verbesserten persönlichen und künstlerischen Bedingungen zurück.
Während die Klage beigelegt wurde, arbeitete er ein Jahr lang für eine unabhängige Filmfirma und gründete 1942 seine eigene Produktionsfirma, Cagney Productions, bevor er sieben Jahre später zu Warner zurückkehrte.
Cagney war das zweite von sieben Kindern, von denen zwei innerhalb weniger Monate nach ihrer Geburt starben.
Als er noch jung war, zog die Familie zweimal um, zuerst in die East 79th Street und dann in die East 96th Street.
Es tut mir leid für das Kind, das es zu bequem hat.
Er war ein guter Straßenkämpfer und verteidigte seinen älteren Bruder Harry, einen Medizinstudenten, wenn es nötig war.
Er engagierte sich im Amateurtheater und begann als Bühnenbildner für eine chinesische Pantomime im Lenox Hill Neighborhood House (einem der ersten Siedlungshäuser des Landes), wo sein Bruder Harry auftrat und Florence James Regie führte.
Mit der Show begann Cagneys zehnjährige Zusammenarbeit mit Varieté und Broadway.
Schließlich borgten sie sich etwas Geld und kehrten über Chicago und Milwaukee nach New York zurück. Dabei scheiterten sie, als sie versuchten, auf der Bühne Geld zu verdienen.
Wie Pitter Patter ging Cagney mit wenig Vertrauen zum Vorsprechen, dass er die Rolle bekommen würde.
Für Cagney war dies eine verheerende Wendung; Abgesehen von den damit verbundenen logistischen Schwierigkeiten befand sich das Gepäck des Paares im Frachtraum des Schiffes und sie hatten ihre Wohnung aufgegeben.
Er beschloss, dass er sich einen Job suchen würde, in dem er etwas anderes tun würde.
Cagney gründete außerdem eine Tanzschule für Profis und bekam dann eine Rolle in dem Theaterstück Women Go On Forever unter der Regie von John Cromwell, das vier Monate lang lief.
Die Show erhielt begeisterte Kritiken und wurde von Grand Street Follies aus dem Jahr 1929 gefolgt.
Der Film mit dem neuen Titel „Sinners‘ Holiday“ erschien 1930.
Allerdings erlaubte der Vertrag Warners, ihn am Ende jedes 40-Wochen-Zeitraums zu entlassen, was ihm faktisch nur ein Einkommen von jeweils 40 Wochen garantierte.
Aufgrund der guten Kritiken, die er in seiner Kurzfilmkarriere erhalten hatte, wurde Cagney als netter Kerl Matt Doyle besetzt, neben Edward Woods als Tom Powers.
Produzent Darryl Zanuck behauptete, er habe bei einer Drehbuchkonferenz darüber nachgedacht; Wellman sagte, die Idee sei ihm gekommen, als er während der Dreharbeiten die Grapefruit auf dem Tisch sah; und die Autoren Glasmon und Bright behaupteten, es basiere auf dem wahren Leben des Gangsters Hymie Weiss, der seiner Freundin ein Omelett ins Gesicht warf.
Ich hätte nie gedacht, dass es im Film gezeigt werden würde.
Er sah sich den Film immer wieder an, nur um diese Szene zu sehen, und wurde oft von verärgerten Zuschauern zum Schweigen gebracht, wenn sein entzücktes Lachen zu laut wurde.
Warner Bros. stellte schnell seine beiden aufstrebenden Gangsterstars Edward G. Robinson und Cagney für den Film Smart Money aus dem Jahr 1931 zusammen.
Als er die Dreharbeiten beendete, füllte „The Public Enemy“ die Kinos die ganze Nacht über mit Vorführungen.
Die Studioleiter bestanden außerdem darauf, dass Cagney weiterhin für ihre Filme wirbt, auch für solche, in denen er nicht mitspielt, was er ablehnt.
Der Erfolg von „The Public Enemy“ und „Blonde Crazy“ zwang Warner Bros. dazu, Hand.
Dem Film folgten schnell „The Crowd Roars“ und „Winner Take All“.
Historiker debattieren auch über die Natur der Geschichte als Selbstzweck sowie über ihren Nutzen, um eine Perspektive auf die Probleme der Gegenwart zu geben.
Allerdings haben antike kulturelle Einflüsse dazu beigetragen, unterschiedliche Interpretationen der Natur der Geschichte hervorzubringen, die sich im Laufe der Jahrhunderte entwickelt haben und sich auch heute noch ändern.
Herodot, ein griechischer Historiker aus dem 5. Jahrhundert v. Chr., wird in der westlichen Tradition oft als „Vater der Geschichte“ angesehen, obwohl er auch als „Vater der Lügen“ kritisiert wurde.
Im Mittelenglischen war die Bedeutung von Geschichte im Allgemeinen „Geschichte“.
Im modernen Deutsch, Französisch und den meisten germanischen und romanischen Sprachen, die stark synthetisch und stark flektiert sind, wird dasselbe Wort immer noch für die Bedeutung von „Geschichte“ und „Geschichte“ verwendet.
Mit den Worten von Benedetto Croce: „Alle Geschichte ist Zeitgeschichte.“
Daher ist die Konstituierung des Archivs des Historikers das Ergebnis der Eingrenzung eines allgemeineren Archivs durch die Ungültigmachung der Verwendung bestimmter Texte und Dokumente (durch die Verfälschung ihres Anspruchs, die „wahre Vergangenheit“ darzustellen).
Das Studium der Geschichte wurde manchmal als Teil der Geisteswissenschaften und manchmal als Teil der Sozialwissenschaften klassifiziert.
Im 20. Jahrhundert revolutionierte der französische Historiker Fernand Braudel das Studium der Geschichte, indem er externe Disziplinen wie Wirtschaftswissenschaften, Anthropologie und Geographie in das Studium der globalen Geschichte einbezog.
Im Allgemeinen lassen sich die Quellen historischen Wissens in drei Kategorien einteilen: was geschrieben steht, was gesagt wird und was physisch erhalten bleibt. Historiker ziehen häufig alle drei Kategorien zu Rate.
Archäologische Funde stehen selten allein da, ihre Entdeckungen werden durch narrative Quellen ergänzt.
Beispielsweise hat Mark Leone, der Ausgräber und Interpret des historischen Annapolis, Maryland, USA, versucht, den Widerspruch zwischen Textdokumenten, die „Freiheit“ idealisieren, und den materiellen Aufzeichnungen zu verstehen, die den Besitz von Sklaven und die Ungleichheiten des Reichtums belegen, die dadurch deutlich werden Studium der gesamten historischen Umgebung.
Es ist für Historiker möglich, sich sowohl mit dem sehr Spezifischen als auch mit dem sehr Allgemeinen zu befassen, obwohl der moderne Trend zur Spezialisierung geht.
Drittens kann es sich darauf beziehen, warum Geschichte produziert wird: die Philosophie der Geschichte.
Von wem wurde es erstellt (Autorschaft)?
Welchen Beweiswert haben die Inhalte (Glaubwürdigkeit)?
Die historische Methode umfasst die Techniken und Richtlinien, nach denen Historiker Primärquellen und andere Beweise nutzen, um zu recherchieren und dann Geschichte zu schreiben.
Im Gegensatz zu Herodot betrachtete Thukydides die Geschichte als Produkt menschlicher Entscheidungen und Handlungen und betrachtete Ursache und Wirkung und nicht das Ergebnis göttlicher Intervention (obwohl Herodot dieser Idee selbst nicht ganz verpflichtet war).
Im alten und mittelalterlichen China gab es historische Traditionen und einen raffinierten Einsatz historischer Methoden.
Chinesische Historiker späterer Dynastieperioden in China verwendeten sein Shiji als offizielles Format für historische Texte sowie für biografische Literatur.
Um 1800 brachte der deutsche Philosoph und Historiker Georg Wilhelm Friedrich Hegel Philosophie und einen säkulareren Ansatz in die Geschichtswissenschaft ein.
Die Originalität von Ibn Khaldun bestand darin, zu behaupten, dass der kulturelle Unterschied eines anderen Zeitalters die Bewertung von relevantem historischem Material bestimmen muss, die Prinzipien zu unterscheiden, nach denen es möglich sein könnte, die Bewertung zu versuchen, und schließlich das Bedürfnis nach Erfahrung zu verspüren, zusätzlich zu rationalen Prinzipien, um eine Kultur der Vergangenheit zu beurteilen.
Seine historische Methode legte auch den Grundstein für die Beobachtung der Rolle von Staat, Kommunikation, Propaganda und systematischer Voreingenommenheit in der Geschichte,H. Mowlana (2001). "
Dr. S.W. Akhtar (1997). "
Für Ranke sollten historische Daten sorgfältig gesammelt, objektiv untersucht und mit kritischer Strenge zusammengestellt werden.
Im 20. Jahrhundert konzentrierten sich akademische Historiker weniger auf epische nationalistische Erzählungen, die oft dazu neigten, die Nation oder große Männer zu verherrlichen, sondern auf objektivere und komplexere Analysen sozialer und intellektueller Kräfte.
Viele der Befürworter der Geschichte als Sozialwissenschaft waren oder sind für ihren multidisziplinären Ansatz bekannt.
Bisher stammte nur eine Geschichtstheorie aus der Feder eines professionellen Historikers.
Geisteshistoriker wie Herbert Butterfield, Ernst Nolte und George Mosse haben für die Bedeutung von Ideen in der Geschichte argumentiert.
Wissenschaftler wie Martin Broszat, Ian Kershaw und Detlev Peukert wollten untersuchen, wie der Alltag der einfachen Leute im Deutschland des 20. Jahrhunderts, insbesondere in der Nazizeit, aussah.
Feministische Historikerinnen wie Joan Wallach Scott, Claudia Koonz, Natalie Zemon Davis, Sheila Rowbotham, Gisela Bock, Gerda Lerner, Elizabeth Fox-Genovese und Lynn Hunt haben dafür plädiert, die Erfahrungen von Frauen in der Vergangenheit zu untersuchen.
Eine weitere Verteidigung der Geschichte vor postmoderner Kritik war das 1994 erschienene Buch The Killing of History des australischen Historikers Keith Windschuttle.
Historische Auslassungen können auf viele Arten auftreten und tiefgreifende Auswirkungen auf historische Aufzeichnungen haben.
Alte Geschichte: das Studium vom Beginn der Menschheitsgeschichte bis zum frühen Mittelalter.
Vergleichende Geschichte: historische Analyse sozialer und kultureller Einheiten, die nicht auf nationale Grenzen beschränkt sind.
Kulturgeschichte: das Studium der Kultur in der Vergangenheit.
Geistesgeschichte: Das Studium von Ideen im Kontext der Kulturen, die sie hervorgebracht haben, und ihrer Entwicklung im Laufe der Zeit.
Neuere Geschichte: das Studium der Neuzeit, der Zeit nach dem Mittelalter.
Paläographie: Studium antiker Texte.
Psychohistorie: Untersuchung der psychologischen Motivationen historischer Ereignisse.
Frauengeschichte: die Geschichte weiblicher Menschen.
Jahrhunderte und Jahrzehnte sind häufig verwendete Zeiträume und die Zeit, die sie darstellen, hängt vom verwendeten Datierungssystem ab.
Um dies zu erreichen, wenden sich Historiker häufig der Geographie zu.
Um beispielsweise zu erklären, warum die alten Ägypter eine erfolgreiche Zivilisation entwickelten, ist das Studium der Geographie Ägyptens unerlässlich.
Die Geschichte Amerikas ist die kollektive Geschichte Nord- und Südamerikas, einschließlich Mittelamerikas und der Karibik.
Die Geschichte der Karibik beginnt mit den ältesten Funden von 7.000 Jahre alten Überresten.
Die Geschichte Eurasiens ist die kollektive Geschichte mehrerer verschiedener peripherer Küstenregionen: des Nahen Ostens, Südasiens, Ostasiens, Südostasiens und Europas, verbunden durch die innere Masse der eurasischen Steppe Zentralasiens und Osteuropas.
Geschichte Ostasiens ist das Studium der Vergangenheit, die in Ostasien von Generation zu Generation weitergegeben wurde.
Die Geschichte Südostasiens wurde als Interaktion zwischen regionalen Akteuren und ausländischen Mächten charakterisiert.
Die „alte“ Sozialgeschichte vor den 1960er Jahren war ein Sammelsurium von Themen ohne zentrales Thema und umfasste häufig politische Bewegungen wie den Populismus, die „sozial“ in dem Sinne waren, dass sie außerhalb des Elitesystems standen.
Es untersucht die Aufzeichnungen und narrativen Beschreibungen vergangener Kenntnisse, Bräuche und Künste einer Gruppe von Menschen.
Unter dieser Form der politischen Geschichte versteht man die Untersuchung der Entwicklung internationaler Beziehungen zwischen Staaten oder über Staatsgrenzen hinweg im Laufe der Zeit.
In den USA, Japan und anderen Ländern gewann es nach den 1980er Jahren an Popularität, da man erkannte, dass Studenten im Zuge der fortschreitenden Globalisierung einen umfassenderen Einblick in die Welt benötigen.
Obwohl es sich um ein relativ neues Fachgebiet handelt, hat die Geschlechtergeschichte einen erheblichen Einfluss auf die allgemeine Geschichtswissenschaft gehabt.
In Oxford und Cambridge wurde die Wissenschaft heruntergespielt.
Bis nach dem Zweiten Weltkrieg dominierten die Tutoren die Debatte.
In den Vereinigten Staaten entstand nach dem Ersten Weltkrieg auf Universitätsebene eine starke Bewegung, Kurse in westlicher Zivilisation anzubieten, um den Studenten ein gemeinsames Erbe mit Europa zu vermitteln.
Viele betrachten das Feld aus beiden Perspektiven.
In den Vereinigten Staaten unterscheiden sich die Inhalte von Lehrbüchern, die von derselben Firma herausgegeben werden, häufig von Bundesstaat zu Bundesstaat.
Akademische Historiker haben oft, teilweise mit Erfolg, gegen die Politisierung der Lehrbücher gekämpft.
Eine Zivilisation (oder Zivilisation) ist eine komplexe Gesellschaft, die durch Stadtentwicklung, soziale Schichtung, eine Regierungsform und symbolische Kommunikationssysteme (z. B. Schrift) gekennzeichnet ist.
In diesem weiten Sinne steht eine Zivilisation im Gegensatz zu nicht zentralisierten Stammesgesellschaften, einschließlich der Kulturen nomadischer Hirten, neolithischer Gesellschaften oder Jäger und Sammler; Manchmal steht es jedoch auch im Gegensatz zu den Kulturen, die innerhalb der Zivilisationen selbst zu finden sind.
Die grundlegende Abhandlung ist Norbert Elias‘ „Der Zivilisationsprozess“ (1939), der soziale Sitten von der mittelalterlichen höfischen Gesellschaft bis in die Frühe Neuzeit nachzeichnet.
Verwandte Wörter wie „Höflichkeit“ entwickelten sich Mitte des 16. Jahrhunderts.
Im späten 18. und frühen 19. Jahrhundert, während der Französischen Revolution, wurde „Zivilisation“ im Singular und nie im Plural verwendet und bezeichnete den Fortschritt der Menschheit als Ganzes.
Erst in diesem verallgemeinerten Sinne wird es möglich, von einer „mittelalterlichen Zivilisation“ zu sprechen, die im Sinne von Elias ein Oxymoron gewesen wäre.
Hier steht die Zivilisation, die rationaler und sozial motivierter ist, nicht vollständig im Einklang mit der menschlichen Natur, und „die menschliche Ganzheit ist nur durch die Wiederherstellung oder Annäherung an eine ursprüngliche diskursive oder prärationale natürliche Einheit erreichbar“ (siehe „Edler Wilder“).
Zivilisationen wurden durch ihre Lebensgrundlagen, Lebensunterhaltsarten, Siedlungsmuster, Regierungsformen, soziale Schichtung, Wirtschaftssysteme, Alphabetisierung und andere kulturelle Merkmale unterschieden.
Alle Zivilisationen waren für ihren Lebensunterhalt auf die Landwirtschaft angewiesen, möglicherweise mit Ausnahme einiger früher Zivilisationen in Peru, die möglicherweise auf Meeresressourcen angewiesen waren.
Getreideüberschüsse waren besonders wichtig, da Getreide lange gelagert werden kann.
An einigen Orten hatten Jäger und Sammler jedoch Zugang zu Nahrungsüberschüssen, beispielsweise bei einigen indigenen Völkern im pazifischen Nordwesten und möglicherweise während der mesolithischen Natufian-Kultur.
Das Wort „Zivilisation“ wird manchmal einfach als „Leben in Städten“ definiert.
Staatliche Gesellschaften sind stärker geschichtet als andere Gesellschaften; Es gibt einen größeren Unterschied zwischen den sozialen Schichten.
Zivilisationen mit komplexen sozialen Hierarchien und organisierten, institutionellen Regierungen.
Manche Menschen erwerben auch Grundeigentum oder Privateigentum an dem Land.
In der frühen Eisenzeit entwickelten moderne Zivilisationen Geld als Tauschmittel für immer komplexere Transaktionen.
Diese Menschen kennen sich möglicherweise nicht persönlich und ihre Bedürfnisse treten möglicherweise nicht alle gleichzeitig auf.
Der Übergang von einfacheren zu komplexeren Volkswirtschaften bedeutet nicht unbedingt eine Verbesserung des Lebensstandards der Bevölkerung.
Die durchschnittliche Statur einer Bevölkerung ist ein gutes Maß für die Angemessenheit ihres Zugangs zu lebensnotwendigen Gütern, insbesondere zu Nahrungsmitteln.
Wie Geld war auch das Schreiben durch die Bevölkerungszahl einer Stadt und die Komplexität ihres Handels unter Menschen erforderlich, die sich nicht alle persönlich kennen.
Dazu gehören organisierte Religion, Entwicklung in den Künsten und unzählige neue Fortschritte in Wissenschaft und Technologie.
Diese Kulturen werden von einigen als „primitiv“ bezeichnet, ein Begriff, der von anderen als abwertend angesehen wird. "
Heutzutage verwenden Anthropologen den Begriff „nicht gebildet“, um diese Völker zu beschreiben.
Aber die Zivilisation verbreitet sich auch durch die technische, materielle und soziale Dominanz, die die Zivilisation hervorbringt.
Zivilisationen neigen dazu, komplizierte Kulturen zu entwickeln, einschließlich eines staatlichen Entscheidungsapparats, einer Literatur, professioneller Kunst, Architektur, einer organisierten Religion und komplexen Bräuchen der Bildung, des Zwanges und der Kontrolle, die mit der Aufrechterhaltung der Elite verbunden sind.
Die Zivilisation, in der jemand lebt, ist die umfassendste kulturelle Identität dieser Person.
Ziel ist die Bewahrung des kulturellen Erbes der Menschheit und auch der kulturellen Identität, insbesondere im Falle von Krieg und bewaffneten Konflikten.
Der Philosoph Oswald Spengler aus dem frühen 20. Jahrhundert,Spengler, Oswald verwendet in seinem Werk „Decline of the West: Perspectives of World History“ (1919) das deutsche Wort Kultur für das, was viele als „Zivilisation“ bezeichnen.
Spengler bezeichnet die Zivilisation als den Beginn des Niedergangs einer Kultur als „der äußerlichsten und künstlichsten Zustände, zu denen eine Art entwickelter Menschheit fähig ist“.
Laut Toynbee verfielen Zivilisationen im Allgemeinen aufgrund des Versagens einer „kreativen Minderheit“ durch moralischen oder religiösen Verfall, einer wichtigen Herausforderung zu begegnen, und nicht nur aus wirtschaftlichen oder ökologischen Gründen.
Beispielsweise waren Handelsnetzwerke bis zum 19. Jahrhundert viel größer als kulturelle oder politische Sphären.
Guillermo Algaze hat argumentiert, dass die Handelsbeziehungen während der Uruk-Zeit Ägypten, Mesopotamien, Iran und Afghanistan verbanden.
Verschiedene Zivilisationen und Gesellschaften auf der ganzen Welt sind in vielerlei Hinsicht wirtschaftlich, politisch und sogar kulturell voneinander abhängig.
Die Zentralzivilisation dehnte sich später auf den gesamten Nahen Osten und Europa aus und dehnte sich dann mit der europäischen Kolonisierung auf globaler Ebene aus, wobei sie im 19. Jahrhundert Amerika, Australien, China und Japan umfasste.
Dies förderte eine Revolution der Sekundärprodukte, bei der die Menschen domestizierte Tiere nicht nur für Fleisch, sondern auch für Milch, Wolle, Mist und zum Ziehen von Pflügen und Karren verwendeten – eine Entwicklung, die sich im gesamten eurasischen Ökumene ausbreitete.
Es wurde festgestellt, dass dieses Gebiet „einige der wichtigsten Entwicklungen in der Geschichte der Menschheit inspiriert hat, darunter die Erfindung des Rades, der Anbau der ersten Getreidepflanzen und die Entwicklung der Kursivschrift“.
Dieser Klimawandel veränderte das Kosten-Nutzen-Verhältnis der endemischen Gewalt zwischen den Gemeinschaften, was zur Aufgabe nicht ummauerter Dorfgemeinschaften und zur Entstehung ummauerter Städte führte, die mit den ersten Zivilisationen in Verbindung gebracht wurden.
Die zivilisierte städtische Revolution wiederum hing von der Entwicklung der Sesshaftigkeit, der Domestizierung von Getreide und Tieren, der Beständigkeit von Siedlungen und der Entwicklung von Lebensstilen ab, die Skaleneffekte und die Anhäufung von Überproduktion durch bestimmte soziale Sektoren ermöglichten.
Einige konzentrieren sich auf historische Beispiele, andere auf die allgemeine Theorie.
Für Gibbon war „der Niedergang Roms die natürliche und unvermeidliche Folge maßloser Größe.“
Theodor Mommsen schlug in seiner Geschichte Roms vor, dass Rom mit dem Zusammenbruch des Weströmischen Reiches im Jahr 476 n. Chr. zusammenbrach, und tendierte auch zu einer biologischen Analogie von „Entstehung“, „Wachstum“, „Seneszenz“, „Zusammenbruch“ und „Verfall“.
Arnold J. Toynbee schlug in seinem Werk „A Study of History“ vor, dass es eine viel größere Anzahl von Zivilisationen gegeben habe, darunter auch eine kleine Anzahl verhafteter Zivilisationen, und dass alle Zivilisationen dazu neigten, den von Mommsen identifizierten Zyklus zu durchlaufen.
In der Zwischenphase führt das zunehmende Bevölkerungswachstum zu einem Rückgang der Pro-Kopf-Produktion und des Pro-Kopf-Konsums, es wird immer schwieriger, Steuern einzutreiben, und die Staatseinnahmen hören auf zu wachsen, während die Staatsausgaben aufgrund des kontrollierten Bevölkerungswachstums steigen vom Staat.
Säkulare Zyklen und tausendjährige Trends.
Die Tatsache, dass Rom immer größere Einnahmen erwirtschaften musste, um Armeen auszurüsten und wieder aufzurüsten, die zum ersten Mal wiederholt im Feld besiegt wurden, führte zur Zerstückelung des Reiches.
Er argumentiert, dass der Zusammenbruch der Maya Lehren für die heutige Zivilisation habe.
Das Verhältnis von Energieaufwand zu Energieertrag ist von zentraler Bedeutung für die Begrenzung des Überlebens von Zivilisationen.
Koneczny behauptete, dass Zivilisationen nicht zu Hybriden vermischt werden können, eine minderwertige Zivilisation werde sie überwinden, wenn sie innerhalb einer hochentwickelten Zivilisation gleiche Rechte erhält.
Der Kulturhistoriker Morris Berman weist in „Dark Ages America: the End of Empire“ darauf hin, dass in den konzernorientierten, konsumorientierten Vereinigten Staaten genau die Faktoren, die sie einst zur Größe trieben – extremer Individualismus, territoriale und wirtschaftliche Expansion und das Streben nach materiellem Reichtum – die Entwicklung vorangetrieben haben Die Vereinigten Staaten haben eine kritische Schwelle erreicht, an der ein Zusammenbruch unvermeidlich ist.
Jacobs argumentiert, dass der Zerfall dieser Säulen mit gesellschaftlichen Missständen wie Umweltkrisen, Rassismus und der wachsenden Kluft zwischen Arm und Reich zusammenhängt.
Dieses Bedürfnis der Zivilisationen, immer mehr Ressourcen zu importieren, sei seiner Ansicht nach auf die übermäßige Ausbeutung und Verminderung ihrer eigenen lokalen Ressourcen zurückzuführen.
In der Grafik bedeutet Ma „vor einer Million Jahren“.)
Ein Großteil der Erde war aufgrund häufiger Kollisionen mit anderen Körpern geschmolzen, was zu extremem Vulkanismus führte.
Erkennbare Menschen entstanden vor höchstens 2 Millionen Jahren, einem verschwindend kleinen Zeitraum auf der geologischen Skala.
Schätzungen zufolge sind 99 Prozent aller Arten, die jemals auf der Erde gelebt haben, also über fünf Milliarden, ausgestorben.
Die Erdkruste hat sich seit ihrer Entstehung ständig verändert, ebenso wie das Leben seit seinem ersten Auftreten.
Der Mond entstand um diese Zeit, wahrscheinlich durch die Kollision eines Protoplaneten mit der Erde.
Die Atmosphäre besteht aus Vulkan- und Treibhausgasen.
Bakterien beginnen, Sauerstoff zu produzieren und formen so die dritte und aktuelle Atmosphäre der Erde.
Die frühen Kontinente Columbia, Rodinia und Pannotia (in dieser Reihenfolge) könnten in diesem Äon existiert haben.
Allmählich breitet sich das Leben auf das Land aus und es tauchen bekannte Formen von Pflanzen, Tieren und Pilzen auf, darunter Ringelwürmer, Insekten und Reptilien, daher der Name des Äons, der „sichtbares Leben“ bedeutet.
Es bestand aus Wasserstoff und Helium, die kurz nach dem Urknall 13,8 Ga (vor Milliarden Jahren) entstanden waren, sowie aus schwereren Elementen, die von Supernovae ausgestoßen wurden.
Als die Wolke zu beschleunigen begann, wurde sie durch ihren Drehimpuls, ihre Schwerkraft und ihre Trägheit zu einer protoplanetaren Scheibe senkrecht zu ihrer Rotationsachse abgeflacht.
Nach weiterer Kontraktion entzündete sich ein T-Tauri-Stern und entwickelte sich zur Sonne.
Die Erde entstand auf diese Weise vor etwa 4,54 Milliarden Jahren (mit einer Unsicherheit von 1 %) und wurde innerhalb von 10–20 Millionen Jahren weitgehend fertiggestellt.
Die Proto-Erde wuchs durch Akkretion, bis ihr Inneres heiß genug war, um die schweren, siderophilen Metalle zu schmelzen.
Aus Kraterzählungen auf anderen Himmelskörpern lässt sich ableiten, dass eine Periode intensiver Meteoriteneinschläge, das so genannte „Late Heavy Bombardment“, etwa 4,1 Ga begann und etwa 3,8 Ga am Ende des Hadean endete.
Zu Beginn des Archäikums hatte sich die Erde deutlich abgekühlt.
Neue Erkenntnisse deuten darauf hin, dass sich der Mond noch später, vor 4,48 ± 0,02 Ga, oder 70–110 Millionen Jahre nach der Entstehung des Sonnensystems, gebildet hat.
Bei der Kollision wurde etwa 100 Millionen Mal mehr Energie freigesetzt als beim jüngsten Chicxulub-Einschlag, der vermutlich zum Aussterben der Nicht-Vogel-Dinosaurier geführt hat.
Die Rieseneinschlagshypothese sagt voraus, dass dem Mond kein metallisches Material mehr vorhanden war, was seine abnormale Zusammensetzung erklärt.
Die anfängliche Kruste, die sich gebildet hatte, als sich die Erdoberfläche zum ersten Mal verfestigte, verschwand vollständig durch eine Kombination aus dieser schnellen Hadean-Plattentektonik und den heftigen Einschlägen des späten schweren Bombardements.
Diese Stücke der späten hadäischen und frühen archaischen Kruste bilden die Kerne, um die herum die heutigen Kontinente wuchsen.
Kratone bestehen hauptsächlich aus zwei abwechselnden Arten von Terranen.
Aus diesem Grund werden Grünsteine manchmal als Beweis für die Subduktion während des Archaikums angesehen.
Nun gilt es als wahrscheinlich, dass viele der flüchtigen Stoffe während der Akkretion durch einen als Aufprallentgasung bekannten Prozess abgegeben wurden, bei dem ankommende Körper beim Aufprall verdampfen.
Planetesimale in einer Entfernung von 1 Astronomischen Einheit (AE), der Entfernung der Erde von der Sonne, trugen wahrscheinlich kein Wasser zur Erde bei, da der Sonnennebel zu heiß war, als dass sich Eis bilden könnte und die Hydratisierung von Gesteinen durch Wasserdampf dies tun würde hat zu lange gedauert.
Jüngste Erkenntnisse deuten darauf hin, dass die Bildung der Ozeane bereits vor 4,4 Milliarden Jahren begonnen haben könnte. Zu Beginn des Archaikums bedeckten sie bereits einen Großteil der Erde.
Somit ist die Sonne in den letzten 4,5 Milliarden Jahren um 30 % heller geworden.
Es gibt viele Modelle, aber wenig Konsens darüber, wie Leben aus nicht lebenden Chemikalien entstand; Im Labor erstellte chemische Systeme erreichen bei weitem nicht die Mindestkomplexität eines lebenden Organismus.
Obwohl sich die Zusammensetzung der Atmosphäre wahrscheinlich von der von Miller und Urey verwendeten unterschied, gelang es späteren Experimenten mit realistischeren Zusammensetzungen auch, organische Moleküle zu synthetisieren.
RNA wurde später durch DNA ersetzt, die stabiler ist und daher längere Genome bilden kann, wodurch die Bandbreite der Fähigkeiten, die ein einzelner Organismus haben kann, erweitert wird.
Eine Schwierigkeit beim Szenario, bei dem der Stoffwechsel an erster Stelle steht, besteht darin, einen Weg für die Evolution von Organismen zu finden.
Untersuchungen aus dem Jahr 2003 ergaben, dass Montmorillonit auch die Umwandlung von Fettsäuren in „Blasen“ beschleunigen könnte und dass die Blasen am Ton befestigte RNA einkapseln könnten.
Diese LUA-Zelle ist der Vorfahre allen Lebens auf der heutigen Erde.
Der Übergang zu einer sauerstoffreichen Atmosphäre war eine entscheidende Entwicklung.
Sie nutzten die Fermentation, den Abbau komplexerer Verbindungen in weniger komplexe Verbindungen mit weniger Energie, und nutzten die so freigesetzte Energie für Wachstum und Fortpflanzung.
Der größte Teil des Lebens auf der Erdoberfläche hängt direkt oder indirekt von der Photosynthese ab.
Um die Elektronen im Kreislauf bereitzustellen, wird Wasserstoff aus dem Wasser entfernt, wobei Sauerstoff als Abfallprodukt zurückbleibt.
Die einfachere anoxygene Form entstand etwa 3,8 Ga, nicht lange nach der Entstehung des Lebens.
Der freigesetzte Sauerstoff war zunächst an Kalkstein, Eisen und andere Mineralien gebunden.
Obwohl jede Zelle nur eine winzige Menge Sauerstoff produzierte, veränderte der gemeinsame Stoffwechsel vieler Zellen über einen langen Zeitraum die Erdatmosphäre in ihren heutigen Zustand.
Die Ozonschicht absorbierte und absorbiert immer noch einen erheblichen Teil der ultravioletten Strahlung, die einst durch die Atmosphäre gelangte.
Infolgedessen begann die Erde im Proterozoikum mehr Wärme von der Sonne zu empfangen.
In Südafrika gefundene Gletscherablagerungen stammen aus der Zeit vor 2,2 Milliarden Jahren und müssen sich zu diesem Zeitpunkt, basierend auf paläomagnetischen Beweisen, in der Nähe des Äquators befunden haben.
Die Huron-Eiszeit könnte durch die erhöhte Sauerstoffkonzentration in der Atmosphäre verursacht worden sein, die zu einem Rückgang von Methan (CH4) in der Atmosphäre führte.
Der Begriff „Schneeballerde“ wird jedoch häufiger zur Beschreibung späterer extremer Eiszeiten während der kryogenen Periode verwendet.
Kohlendioxid verbindet sich mit Regen, verwittert Gestein und bildet Kohlensäure, die dann ins Meer ausgewaschen wird und so der Atmosphäre das Treibhausgas entzieht.
Die Bakteriendomäne hat sich wahrscheinlich zunächst von den anderen Lebensformen (manchmal auch Neomura genannt) abgespalten, diese Annahme ist jedoch umstritten.
Die frühesten Fossilien mit pilztypischen Merkmalen stammen aus dem Paläoproterozoikum, also vor etwa 2,4 Jahren; Diese vielzelligen benthischen Organismen hatten fadenförmige Strukturen, die zur Anastomose fähig waren.
Möglicherweise hat die große Zelle versucht, die kleinere zu verdauen, scheiterte jedoch (möglicherweise aufgrund der Entwicklung der Beuteabwehr).
Mithilfe von Sauerstoff verstoffwechselte es die Abfallprodukte der größeren Zelle und gewann mehr Energie.
Bald entwickelte sich eine stabile Symbiose zwischen der großen Zelle und den darin befindlichen kleineren Zellen.
Ein ähnliches Ereignis ereignete sich, als photosynthetische Cyanobakterien in große heterotrophe Zellen eindrangen und zu Chloroplasten wurden.
Neben der gut etablierten endosymbiotischen Theorie des zellulären Ursprungs von Mitochondrien und Chloroplasten gibt es Theorien, dass Zellen zu Peroxisomen führten, Spirochäten zu Zilien und Flagellen und dass möglicherweise ein DNA-Virus zum Zellkern führte, obwohl keine davon weit verbreitet ist akzeptiert.
Um 1,1 Ga entstand der Superkontinent Rodinia.
Obwohl die Trennung zwischen einer Kolonie mit spezialisierten Zellen und einem mehrzelligen Organismus nicht immer klar ist, entstanden vor etwa einer Milliarde Jahren die ersten mehrzelligen Pflanzen, wahrscheinlich Grünalgen.
Paläomagnetische Pole werden durch geologische Beweise wie orogene Gürtel, die die Ränder antiker Platten markieren, und frühere Verbreitungsgebiete von Flora und Fauna ergänzt.
Vor etwa 1000 bis 830 Ma war der größte Teil der Kontinentalmasse im Superkontinent Rodinia vereint.
Der hypothetische Superkontinent wird manchmal als Pannotia oder Vendia bezeichnet.
Die Intensität und der Mechanismus beider Vergletscherungen werden noch untersucht und sind schwerer zu erklären als die frühe proterozoische Schneeballerde.
Da CO2 ein wichtiges Treibhausgas ist, kühlte sich das Klima weltweit ab.
Eine erhöhte vulkanische Aktivität resultierte aus dem etwa zeitgleichen Zerfall von Rodinia.
Die neuen Lebensformen, Ediacara biota genannt, waren größer und vielfältiger als je zuvor.
Es besteht aus drei Epochen: dem Paläozoikum, dem Mesozoikum und dem Känozoikum und ist die Zeit, in der sich das vielzellige Leben stark in fast alle heute bekannten Organismen diversifizierte.
Dadurch steigt der Meeresspiegel.
Spuren der Vereisung aus dieser Zeit finden sich nur im ehemaligen Gondwana.
Die Kontinente Laurentia und Baltica kollidierten zwischen 450 und 400 Ma während der kaledonischen Orogenese und bildeten Laurussia (auch Euramerica genannt).
Die Kollision von Sibirien mit Laurussia verursachte die Uralische Orogenese, die Kollision von Gondwana mit Laurussia wird in Europa als Variszische oder Hercynische Orogenese oder in Nordamerika als Alleghenische Orogenese bezeichnet.
Während die Ediacara-Lebensformen noch primitiv erscheinen und sich nicht leicht einer modernen Gruppe zuordnen lassen, waren am Ende des Kambriums die meisten modernen Phyla bereits vorhanden.
Einige dieser kambrischen Gruppen erscheinen komplex, unterscheiden sich aber scheinbar deutlich vom modernen Leben. Beispiele sind Anomalocaris und Haikouichthys.
Ein Lebewesen, das der Vorfahre der Fische gewesen sein könnte oder wahrscheinlich eng mit ihnen verwandt war, war Pikaia.
Fische, die frühesten Wirbeltiere, entwickelten sich etwa vor 530 Ma in den Ozeanen.
Die ältesten Fossilien von Landpilzen und Pflanzen stammen aus der Zeit vor 480–460 Ma, obwohl molekulare Beweise darauf hindeuten, dass die Pilze das Land bereits vor 1000 Ma und die Pflanzen vor 700 Ma besiedelt haben könnten.
Flossen entwickelten sich zu Gliedmaßen, mit denen die ersten Tetrapoden ihren Kopf aus dem Wasser hoben, um Luft zu atmen.
Einige von ihnen gewöhnten sich schließlich so gut an das Landleben, dass sie ihr erwachsenes Leben an Land verbrachten, obwohl sie im Wasser schlüpften und zurückkehrten, um ihre Eier zu legen.
Etwa zu dieser Zeit (um etwa 360 Ma) entwickelten Pflanzen Samen, die ihre Ausbreitung an Land dramatisch beschleunigten.
Weitere 30 Millionen Jahre (310 Ma) erlebten die Divergenz der Synapsiden (einschließlich Säugetiere) von den Sauropsiden (einschließlich Vögeln und Reptilien).
Das Trias-Jura-Aussterben vor 200 Millionen Jahren verschonte viele der Dinosaurier und sie dominierten bald die Wirbeltiere.
60 % der wirbellosen Meerestiere und 25 % aller Familien starben aus.
Das dritte Massenaussterben war das Ereignis Perm-Trias oder das Große Sterben und wurde möglicherweise durch eine Kombination aus dem Vulkanereignis Siberian Traps, einem Asteroideneinschlag, Methanhydratvergasung, Schwankungen des Meeresspiegels und einem großen anoxischen Ereignis verursacht.
Dies war mit Abstand das tödlichste Aussterben aller Zeiten, bei dem etwa 57 % aller Familien und 83 % aller Gattungen getötet wurden.
Im frühen Paläozän erholte sich die Erde vom Aussterben und die Säugetiervielfalt nahm zu.
Graslose Savannen begannen, einen Großteil der Landschaft zu dominieren, und Säugetiere wie Andrewsarchus stiegen zum größten bekannten landlebenden Raubtier aller Zeiten auf, und frühe Wale wie Basilosaurus übernahmen die Kontrolle über die Meere.
Riesige Huftiere wie Paraceratherium und Deinotherium haben sich entwickelt, um das Grasland zu beherrschen.
Das Tethys-Meer wurde durch die Kollision von Afrika und Europa verschlossen.
Die Landbrücke ermöglichte den isolierten Lebewesen Südamerikas die Migration nach Nordamerika und umgekehrt.
Die Eiszeiten führten zur Entwicklung und Expansion des modernen Menschen im Sahara-Afrika.
Viele gehen davon aus, dass entlang der Beringia-Region eine riesige Wanderung stattgefunden hat, weshalb es heute Kamele (die sich in Nordamerika entwickelten und ausgestorben sind), Pferde (die sich in Nordamerika entwickelten und ausgestorben sind) und amerikanische Ureinwohner gibt.
Die Gehirngröße nahm rasch zu und vor 2 Millionen Jahren tauchten die ersten Tiere auf, die der Gattung Homo zugeordnet wurden.
Die Fähigkeit, Feuer zu kontrollieren, begann wahrscheinlich beim Homo erectus (oder Homo ergaster), wahrscheinlich vor mindestens 790.000 Jahren, vielleicht aber schon vor 1,5 Millionen Jahren.
Es ist schwieriger, den Ursprung der Sprache festzustellen; Es ist unklar, ob der Homo erectus sprechen konnte oder ob diese Fähigkeit erst beim Homo sapiens begann.
Die sozialen Fähigkeiten wurden komplexer, die Sprache anspruchsvoller und die Werkzeuge ausgefeilter.
Die ersten Menschen, die Anzeichen von Spiritualität zeigten, waren die Neandertaler (normalerweise als eigenständige Spezies ohne überlebende Nachkommen klassifiziert); Sie begruben ihre Toten, oft ohne Anzeichen von Essen oder Werkzeugen.
Als die Sprache komplexer wurde, führte die Fähigkeit, sich Informationen zu merken und zu kommunizieren, nach einer von Richard Dawkins vorgeschlagenen Theorie zu einem neuen Replikator: dem Mem.
Zwischen 8500 und 7000 v. Chr. begannen die Menschen im Fruchtbaren Halbmond im Nahen Osten mit der systematischen Haltung von Pflanzen und Tieren: der Landwirtschaft.
In den Zivilisationen, die die Landwirtschaft einführten, ermöglichte die relative Stabilität und erhöhte Produktivität der Landwirtschaft jedoch ein Bevölkerungswachstum.
Dies führte zwischen 4000 und 3000 v. Chr. zur ersten Zivilisation der Erde in Sumer im Nahen Osten.
Der Mensch musste nicht mehr seine ganze Zeit arbeiten, um zu überleben, was die ersten spezialisierten Berufe ermöglichte (z. B. Handwerker, Kaufleute, Priester usw.).
Um 500 v. Chr. gab es im Nahen Osten, im Iran, in Indien, China und Griechenland Hochkulturen, die zeitweise expandierten, zeitweise im Niedergang begriffen waren.
Diese Zivilisation entwickelte sich in den Bereichen Kriegsführung, Kunst, Wissenschaft, Mathematik und Architektur.
Das Römische Reich wurde im frühen 4. Jahrhundert von Kaiser Konstantin christianisiert und verfiel Ende des 5. Jahrhunderts.
Das Haus der Weisheit wurde im Bagdad der Abbasiden im Irak gegründet.
Im 14. Jahrhundert begann in Italien die Renaissance mit Fortschritten in Religion, Kunst und Wissenschaft.
Ab dem Jahr 1500 begann sich die europäische Zivilisation zu verändern, was zu wissenschaftlichen und industriellen Revolutionen führte.
Von 1914 bis 1918 und 1939 bis 1945 waren Nationen auf der ganzen Welt in Weltkriege verwickelt.
Nach dem Krieg wurden viele neue Staaten gegründet, die in einer Zeit der Dekolonisierung ihre Unabhängigkeit erklärten oder erhielten.
Zu den technologischen Entwicklungen zählen Atomwaffen, Computer, Gentechnik und Nanotechnologie.
Große Sorgen und Probleme wie Krankheiten, Krieg, Armut, gewalttätiger Radikalismus und in jüngster Zeit auch der vom Menschen verursachte Klimawandel nehmen mit zunehmender Weltbevölkerung zu.
Die Menschheitsgeschichte oder aufgezeichnete Geschichte ist die Erzählung der Vergangenheit der Menschheit.
Im Neolithikum begann die Agrarrevolution zwischen 10.000 und 5.000 v. Chr. im Fruchtbaren Halbmond des Nahen Ostens.
Mit der Entwicklung der Landwirtschaft wurde der Getreideanbau immer ausgefeilter und führte zu einer Arbeitsteilung bei der Lagerung von Nahrungsmitteln zwischen den Vegetationsperioden.
Der Hinduismus entwickelte sich in der späten Bronzezeit auf dem indischen Subkontinent.
Die postklassische Geschichte (das „Mittelalter“, ca. 500–1500 n. Chr.) war Zeuge des Aufstiegs des Christentums, des islamischen Goldenen Zeitalters (ca. 750 n. Chr. – ca. 1258 n. Chr.) und der timuridischen und italienischen Renaissance (ab ca 1300 n. Chr.).
Im 18. Jahrhundert hatte die Anhäufung von Wissen und Technologie eine kritische Masse erreicht, die die industrielle Revolution auslöste und die Spätmoderne einläutete, die um 1800 begann und bis in die Gegenwart andauert.
Anatomisch moderne Menschen entstanden vor etwa 300.000 Jahren in Afrika und erreichten vor etwa 50.000 Jahren ein modernes Verhalten.
Vielleicht schon vor 1,8 Millionen Jahren, sicherlich aber schon vor 500.000 Jahren, begann der Mensch, Feuer zum Heizen und Kochen zu nutzen.
Die Menschen der Altsteinzeit lebten als Jäger und Sammler und waren im Allgemeinen Nomaden.
Die rasante Ausbreitung der Menschheit nach Nordamerika und Ozeanien erfolgte auf dem Höhepunkt der jüngsten Eiszeit.
Im Tal des Gelben Flusses in China wurden bereits um 7000 v. Chr. Hirse und andere Getreidearten angebaut; Das Jangtse-Tal domestiziert Reis bereits seit mindestens 8000 v. Chr.
Die Metallverarbeitung wurde erstmals um 6000 v. Chr. zur Herstellung von Kupferwerkzeugen und -ornamenten eingesetzt.
Städte waren Zentren des Handels, der Produktion und der politischen Macht.
Die Entwicklung der Städte war gleichbedeutend mit dem Aufstieg der Zivilisation.
Diese Kulturen erfanden auf verschiedene Weise das Rad, die Mathematik, die Bronzeverarbeitung, Segelboote, die Töpferscheibe, gewebte Stoffe, den Bau monumentaler Gebäude und die Schrift.
Typisch für das Neolithikum war die Tendenz, anthropomorphe Gottheiten zu verehren.
Diese Siedlungen konzentrierten sich auf fruchtbare Flusstäler: den Tigris und den Euphrat in Mesopotamien, den Nil in Ägypten, den Indus auf dem indischen Subkontinent sowie den Jangtsekiang und den Gelben Fluss in China.
Die Keilschrift begann als System von Piktogrammen, deren bildliche Darstellungen schließlich vereinfacht und abstrakter wurden.
Der Transport wurde über Wasserstraßen erleichtert – über Flüsse und Meere.
Diese Entwicklungen führten zur Entstehung von Territorialstaaten und Imperien.
Auf Kreta trat die minoische Zivilisation um 2700 v. Chr. in die Bronzezeit ein und gilt als die erste Zivilisation Europas.
Im Laufe der folgenden Jahrtausende entwickelten sich auf der ganzen Welt Zivilisationen.
In Indien war diese Ära die vedische Periode (1750-600 v. Chr.), die den Grundstein für den Hinduismus und andere kulturelle Aspekte der frühen indischen Gesellschaft legte und im 6. Jahrhundert v. Chr. endete.
Während der Entstehungsphase in Mesoamerika (etwa 1500 v. Chr. bis 500 n. Chr.) begannen sich komplexere und zentralisierte Zivilisationen zu entwickeln, vor allem im heutigen Mexiko, Mittelamerika und Peru.
Die Theorie des Achsenzeitalters von Karl Jaspers umfasst auch den persischen Zoroastrismus, aber andere Gelehrte bestreiten seine Zeitachse für den Zoroastrismus.)
Dies waren Taoismus, Legalismus und Konfuzianismus.
Die großen Reiche waren auf die militärische Annexion von Territorien und auf die Bildung befestigter Siedlungen angewiesen, die zu landwirtschaftlichen Zentren wurden.
In dieser Zeit gab es eine Reihe regionaler Reiche.
Das Medianreich wich aufeinanderfolgenden iranischen Reichen, darunter dem Achämenidenreich (550–330 v. Chr.), dem Partherreich (247 v. Chr.–224 n. Chr.) und dem Sasanidenreich (224–651 n. Chr.).
Später gründete Alexander der Große (356–323 v. Chr.) aus Mazedonien ein Eroberungsreich, das sich vom heutigen Griechenland bis zum heutigen Indien erstreckte.
Ab dem 3. Jahrhundert n. Chr. herrschte die Gupta-Dynastie über den Zeitraum, der als das Goldene Zeitalter des alten Indien bezeichnet wird.
Die daraus resultierende Stabilität trug dazu bei, das goldene Zeitalter der hinduistischen Kultur im 4. und 5. Jahrhundert einzuläuten.
Zur Zeit von Augustus (63 v. Chr. – 14 n. Chr.), dem ersten römischen Kaiser, hatte Rom bereits die Herrschaft über den größten Teil des Mittelmeerraums erlangt.
Das Weströmische Reich fiel 476 n. Chr. unter Odoaker unter deutschen Einfluss.
Die Han-Dynastie war in Macht und Einfluss mit dem Römischen Reich vergleichbar, das am anderen Ende der Seidenstraße lag.
Wie andere Reiche der klassischen Periode machte Han-China in den Bereichen Regierung, Bildung, Mathematik, Astronomie, Technologie und vielen anderen bedeutende Fortschritte.
Auch auf dem amerikanischen Kontinent entstanden erfolgreiche regionale Imperien, die aus Kulturen hervorgingen, die bereits im Jahr 2500 v. Chr. gegründet wurden.
Die großen Maya-Stadtstaaten nahmen langsam an Zahl und Bedeutung zu, und die Maya-Kultur verbreitete sich in ganz Yucatán und den umliegenden Gebieten.
Allerdings gab es in manchen Regionen auch Zeiten rasanten technischen Fortschritts.
Chinas Han-Dynastie geriet im Jahr 220 n. Chr. in einen Bürgerkrieg, der die Zeit der Drei Königreiche einläutete, während ihr römisches Gegenstück etwa zur gleichen Zeit in der sogenannten Krise des dritten Jahrhunderts zunehmend dezentralisiert und gespalten wurde.
Die Entwicklung des Steigbügels und die Zucht von Pferden, die stark genug waren, um einen voll bewaffneten Bogenschützen zu tragen, machten die Nomaden zu einer ständigen Bedrohung für die sesshafteren Zivilisationen.
Der verbleibende Teil des Römischen Reiches im östlichen Mittelmeerraum blieb als das sogenannte Byzantinische Reich bestehen.
Die Ära wird üblicherweise auf den Untergang des Weströmischen Reiches im 5. Jahrhundert datiert, das in viele einzelne Königreiche zerfiel, von denen einige später unter dem Heiligen Römischen Reich konföderiert wurden.
In Südasien gab es eine Reihe mittlerer Königreiche Indiens, gefolgt von der Gründung islamischer Reiche in Indien.
Dadurch konnte Afrika dem südostasiatischen Handelssystem beitreten und mit Asien in Kontakt kommen. Dies führte zusammen mit der muslimischen Kultur zur Swahili-Kultur.
Dies war auch ein kultureller Kampf, bei dem die byzantinische hellenistische und christliche Kultur gegen die persisch-iranischen Traditionen und die zoroastrische Religion konkurrierte.
Von ihrem Zentrum auf der Arabischen Halbinsel aus begannen die Muslime in der frühen postklassischen Ära mit ihrer Expansion.
Ein Großteil dieses Lernens und dieser Entwicklung kann mit der Geographie verknüpft werden.
Der Einfluss muslimischer Kaufleute auf die afrikanisch-arabischen und arabisch-asiatischen Handelsrouten war enorm.
Motiviert durch Religion und Eroberungsträume starteten europäische Führer eine Reihe von Kreuzzügen, um zu versuchen, die Macht der Muslime zurückzudrängen und das Heilige Land zurückzuerobern.
Die arabische Herrschaft über die Region endete Mitte des 11. Jahrhunderts mit der Ankunft der Seldschuken-Türken, die aus den türkischen Heimatländern in Zentralasien nach Süden wanderten.
Die Region wird später „Berberküste“ genannt und wird Piraten und Freibeuter beherbergen, die mehrere nordafrikanische Häfen für ihre Raubzüge gegen die Küstenstädte mehrerer europäischer Länder nutzen werden, auf der Suche nach Sklaven, die als Teil der Barbarensklave auf nordafrikanischen Märkten verkauft werden sollen Handel.
Im 8. Jahrhundert begann der Islam in die Region einzudringen und wurde bald zum alleinigen Glauben des Großteils der Bevölkerung, obwohl der Buddhismus im Osten stark blieb.
Nach dem Tod von Dschingis Khan im Jahr 1227 wurde der größte Teil Zentralasiens weiterhin von einem Nachfolgestaat, dem Chagatai Khanat, dominiert.
Die Region wurde dann in eine Reihe kleinerer Khanate aufgeteilt, die von den Usbeken gegründet wurden.
Die barbarischen Eindringlinge gründeten auf den Überresten des Weströmischen Reiches ihre eigenen neuen Königreiche.
Das Christentum breitete sich in Westeuropa aus und Klöster wurden gegründet.
Manorialismus, die Organisation von Bauern in Dörfern, die den Adligen Pacht und Arbeitsdienst schuldeten, und der Feudalismus, eine politische Struktur, in der Ritter und Adlige mit niedrigerem Status ihren Oberherren Militärdienst schuldeten, als Gegenleistung für das Recht auf Pacht von Ländereien und Herrenhäusern über die Organisationsformen der mittelalterlichen Gesellschaft, die sich im Hochmittelalter entwickelten.
Italienische Kaufleute importierten Sklaven, um in Haushalten oder in der Zuckerverarbeitung zu arbeiten.
Hungersnot, Pest und Krieg verwüsteten die Bevölkerung Westeuropas.
Sie machten schließlich der Zagwe-Dynastie Platz, die für ihre Felsarchitektur in Lalibela berühmt ist.
Sie kontrollierten den transsaharischen Handel mit Gold, Elfenbein, Salz und Sklaven.
In Zentralafrika entstanden mehrere Staaten, darunter das Königreich Kongo.
Sie bauten große Verteidigungsanlagen aus Stein ohne Mörtel wie Groß-Simbabwe, die Hauptstadt des Königreichs Simbabwe, Khami, die Hauptstadt des Königreichs Butua, und Danangombe (Dhlo-Dhlo), die Hauptstadt des Rozvi-Reiches.
Im neunten Jahrhundert kam es zu einem dreigliedrigen Kampf um die Kontrolle über Nordindien zwischen dem Pratihara-Reich, dem Pala-Reich und dem Rashtrakuta-Reich.
Die Tang-Dynastie zerfiel jedoch schließlich, und nach einem halben Jahrhundert des Aufruhrs vereinte die Song-Dynastie China wieder, als es laut William McNeill das „reichste, fähigste und bevölkerungsreichste Land der Welt“ war.
Nach etwa einem Jahrhundert der Herrschaft der mongolischen Yuan-Dynastie erlangten die ethnischen Chinesen mit der Gründung der Ming-Dynastie (1368) wieder die Kontrolle.
Die Nara-Zeit des 8. Jahrhunderts markierte die Entstehung eines starken japanischen Staates und wird oft als goldenes Zeitalter dargestellt.
Die feudale Periode der japanischen Geschichte, die von mächtigen Regionalherren (Daimyos) und der Militärherrschaft von Kriegsherren (Shoguns) wie dem Ashikaga-Shogunat und dem Tokugawa-Shogunat dominiert wurde, erstreckte sich von 1185 bis 1868.
Silla eroberte Baekje im Jahr 660 und Goguryeo im Jahr 668 und markierte damit den Beginn der Periode der Nord- und Südstaaten (남북 국third대) mit dem vereinten Silla im Süden und Balhae, einem Nachfolgestaat von Goguryeo, im Norden.
Ab dem 9. Jahrhundert erlangte das Bagan-Königreich im modernen Myanmar Bedeutung.
Die Pueblo-Vorfahren und ihre Vorgänger (9.–13. Jahrhundert) bauten umfangreiche dauerhafte Siedlungen, darunter Steinbauten, die bis zum 19. Jahrhundert die größten Gebäude Nordamerikas blieben.
In Südamerika kam es im 14. und 15. Jahrhundert zum Aufstieg der Inka.
Die wissenschaftliche Revolution erhielt ihren Anstoß durch die Einführung des Buchdrucks mit beweglichen Lettern durch Johannes Gutenberg in Europa sowie durch die Erfindung des Teleskops und des Mikroskops.
Die Spätmoderne reicht entweder bis zum Ende des Zweiten Weltkriegs im Jahr 1945 oder bis in die Gegenwart.
Die frühe Neuzeit war geprägt vom Aufstieg der Wissenschaft, einem immer schnelleren technischen Fortschritt, einer säkularisierten Bürgerpolitik und dem Nationalstaat.
In der frühen Neuzeit konnte Europa seine Vorherrschaft zurückgewinnen; Historiker diskutieren immer noch über die Ursachen.
Um 1000 n. Chr. hatte es eine fortschrittliche Geldwirtschaft entwickelt.
Es verfügte über einen technologischen Vorsprung und hatte ein Monopol in der Gusseisenproduktion, den Kolbenbälgen, dem Hängebrückenbau, der Druckerei und dem Kompass.
Eine Theorie zum Aufstieg Europas besagt, dass die Geographie Europas eine wichtige Rolle für seinen Erfolg spielte.
Dies verschaffte Europa einen gewissen Schutz vor der Gefahr zentralasiatischer Eindringlinge.
Das Goldene Zeitalter des Islam endete 1258 mit der Plünderung Bagdads durch die Mongolen.
Die Geographie trug zu wichtigen geopolitischen Unterschieden bei.
Im Gegensatz dazu war Europa fast immer in mehrere verfeindete Staaten gespalten.
Fast alle landwirtschaftlichen Zivilisationen wurden durch ihre Umwelt stark eingeschränkt.
Der technologische Fortschritt und der durch den Handel geschaffene Wohlstand führten nach und nach zu einer Erweiterung der Möglichkeiten.
Es überrascht nicht, dass Europas maritime Expansion – angesichts der Geographie des Kontinents – größtenteils das Werk seiner Atlantikstaaten war: Portugal, Spanien, England, Frankreich und die Niederlande.
In Nordafrika blieb das Saadi-Sultanat bis 1659 ein unabhängiger Berberstaat.
Die Swahili-Küste verfiel, nachdem sie unter das portugiesische Reich und später an das omanische Reich fiel.
Das südafrikanische Königreich Simbabwe wich kleineren Königreichen wie Mutapa, Butua und Rozvi.
Andere Zivilisationen in Afrika entwickelten sich in dieser Zeit weiter.
Japan erlebte seine Azuchi-Momoyama-Zeit (1568–1603), gefolgt von der Edo-Zeit (1603–1868).
Das Sultanat Johor mit Sitz an der Südspitze der Malaiischen Halbinsel wurde zur dominierenden Handelsmacht in der Region.
Russland drang an der Nordwestküste Nordamerikas vor, mit einer ersten Kolonie im heutigen Alaska im Jahr 1784 und dem Außenposten Fort Ross im heutigen Kalifornien im Jahr 1812.
Die industrielle Revolution begann in Großbritannien und nutzte neue Produktionsweisen – Fabrik, Massenproduktion und Mechanisierung –, um eine breite Palette von Gütern schneller und mit weniger Arbeitskräften herzustellen als zuvor.
Nachdem die Europäer Einfluss und Kontrolle über Amerika erlangt hatten, wandten sich die imperialen Aktivitäten den Ländern Asiens und Ozeaniens zu.
Die Briten kolonisierten auch Australien, Neuseeland und Südafrika, wobei eine große Zahl britischer Kolonisten in diese Kolonien auswanderte.
Innerhalb Europas schufen wirtschaftliche und militärische Herausforderungen ein System von Nationalstaaten, und ethnolinguistische Gruppierungen begannen, sich als eigenständige Nationen mit dem Streben nach kultureller und politischer Autonomie zu identifizieren.
Unterdessen haben sich die seit der Entdeckung des Feuers und dem Beginn der Zivilisation bestehenden industriellen Verschmutzungen und Umweltschäden drastisch beschleunigt.
Ein Großteil der übrigen Welt wurde von stark europäisierten Nationen beeinflusst: den Vereinigten Staaten und Japan.
Der Erste Weltkrieg führte zum Zusammenbruch von vier Reichen – Österreich-Ungarn, dem Deutschen Reich, dem Osmanischen Reich und dem Russischen Reich – und schwächte das Vereinigte Königreich und Frankreich.
Anhaltende nationale Rivalitäten, die durch die wirtschaftlichen Turbulenzen der Weltwirtschaftskrise verschärft wurden, trugen zur Auslösung des Zweiten Weltkriegs bei.
Der Kalte Krieg endete 1991 friedlich nach dem Paneuropäischen Picknick, dem anschließenden Fall des Eisernen Vorhangs und der Berliner Mauer sowie dem Zusammenbruch des Ostblocks und des Warschauer Pakts.
In den ersten Nachkriegsjahrzehnten erlangten die Kolonien der belgischen, britischen, niederländischen, französischen und anderen westeuropäischen Reiche in Asien und Afrika ihre formelle Unabhängigkeit.
Die Wirksamkeit der Europäischen Union wurde durch die Unreife ihrer gemeinsamen wirtschaftlichen und politischen Institutionen beeinträchtigt, die in gewisser Weise mit der Unzulänglichkeit der Institutionen der Vereinigten Staaten gemäß den Artikeln der Konföderation vor der Annahme der Verfassung der Vereinigten Staaten, die 1789 in Kraft trat, vergleichbar ist.
In den Jahrzehnten nach dem Zweiten Weltkrieg führten diese Fortschritte zu Jetreisen, künstlichen Satelliten mit unzähligen Anwendungen, darunter dem Global Positioning System (GPS), und dem Internet.
Der weltweite Wettbewerb um natürliche Ressourcen hat aufgrund des Bevölkerungswachstums und der Industrialisierung zugenommen, insbesondere in Indien, China und Brasilien.
Ein Archiv ist eine Ansammlung historischer Aufzeichnungen – in beliebigen Medien – oder der physischen Einrichtung, in der sie sich befinden.
Sie wurden metaphorisch als „Sekrete eines Organismus“ definiert und unterscheiden sich von Dokumenten, die bewusst geschrieben oder erstellt wurden, um der Nachwelt eine bestimmte Botschaft zu übermitteln.
Das bedeutet, dass sich Archive hinsichtlich ihrer Funktionen und Organisation deutlich von Bibliotheken unterscheiden, obwohl sich Archivbestände häufig innerhalb von Bibliotheksgebäuden befinden.
Archäologen haben an Orten wie Ebla, Mari, Amarna, Hattusas, Ugarit und Pylos Archive mit Hunderten (und manchmal Tausenden) Tontafeln aus dem dritten und zweiten Jahrtausend v. Chr. entdeckt.
Sie sind jedoch verloren gegangen, da Dokumente, die auf Materialien wie Papyrus und Papier geschrieben wurden, im Gegensatz zu ihren Gegenstücken auf Steintafeln schneller verfielen.
England entwickelte nach 1066 Archive und Archivforschungsmethoden.
Obwohl es viele Arten von Archiven gibt, identifiziert die jüngste Archivarchivarzählung in den Vereinigten Staaten fünf Haupttypen: akademische, geschäftliche (gewinnorientierte), staatliche, gemeinnützige und andere.
Der Zugang zu den Beständen dieser Archive erfolgt in der Regel nur nach vorheriger Terminvereinbarung; Einige haben Öffnungszeiten für Anfragen angegeben.
Beispiele für bedeutende Unternehmensarchive in den Vereinigten Staaten sind Coca-Cola (dem auch das separate Museum World of Coca-Cola gehört), Procter and Gamble, Motorola Heritage Services and Archives und Levi Strauss & Co. Diese Unternehmensarchive bewahren historische Dokumente und Dokumente auf Artikel im Zusammenhang mit der Geschichte und Verwaltung ihrer Unternehmen.
Mitarbeiter in solchen Archiven können über eine beliebige Kombination aus Ausbildung und Abschluss verfügen, entweder mit historischem oder bibliothekarischem Hintergrund.
In den Vereinigten Staaten unterhält die National Archives and Records Administration (NARA) zentrale Archiveinrichtungen im District of Columbia und im College Park, Maryland, wobei regionale Einrichtungen über die gesamten Vereinigten Staaten verteilt sind.
Im Vereinigten Königreich sind die National Archives (früher bekannt als Public Record Office) das Regierungsarchiv für England und Wales.
Zusammengenommen ist der Gesamtumfang der Archive unter der Aufsicht der französischen Archivverwaltung der größte der Welt.
Erzdiözesen, Diözesen und Pfarreien verfügen auch über Archive in der römisch-katholischen und anglikanischen Kirche.
Oftmals sind diese Institutionen sowohl auf Zuschüsse der Regierung als auch auf private Mittel angewiesen.
Viele Museen führen Archive, um die Herkunft ihrer Stücke nachzuweisen.
Dies war eine andere Zahl als die 1,3 %, die sich als Selbstständige bezeichneten.
Die Mission des Archivs besteht darin, Geschichten von Frauen zu sammeln, die sich ausdrücken möchten und möchten, dass ihre Geschichten gehört werden.
Die Archive einer Organisation (z. B. eines Unternehmens oder einer Regierung) enthalten in der Regel andere Arten von Aufzeichnungen, z. B. Verwaltungsakten, Geschäftsunterlagen, Memos, offizielle Korrespondenz und Sitzungsprotokolle.
Viele dieser Spenden müssen noch katalogisiert werden, werden aber derzeit digital konserviert und online der Öffentlichkeit zugänglich gemacht.
Internationale Partner für Archive sind die UNESCO und Blue Shield International gemäß der Haager Konvention zum Schutz von Kulturgut von 1954 und ihrem 2. Protokoll von 1999.
Page, Morgan M. „One from the Vaults: Gossip, Access, and Trans History-Telling.“
Ein Beispiel hierfür ist Morgan M. Pages Beschreibung der direkten Verbreitung der Transgender-Geschichte an Transgender-Menschen über verschiedene soziale Medien und Netzwerkplattformen wie Tumblr, Twitter und Instagram sowie über Podcasts.
Mit den Möglichkeiten der Gegenarchivierung besteht das Potenzial, „traditionelle Geschichtsauffassungen in Frage zu stellen“, wie sie in zeitgenössischen Archiven wahrgenommen werden, was Raum für Erzählungen schafft, die in vielen Archivmaterialien oft nicht vorhanden sind.
Eine Biografie, oder einfach Biografie, ist eine detaillierte Beschreibung des Lebens einer Person.
Biografische Werke sind in der Regel Sachbücher, Belletristik kann jedoch auch zur Darstellung des Lebens einer Person verwendet werden.
Eine weitere bekannte Sammlung antiker Biografien ist De vita Caesarum („Über das Leben der Cäsaren“) von Sueton, geschrieben um 121 n. Chr. zur Zeit Kaiser Hadrians.
Einsiedler, Mönche und Priester nutzten diese historische Zeit, um Biografien zu schreiben.
Ein bedeutendes weltliches Beispiel einer Biographie aus dieser Zeit ist das Leben Karls des Großen von seinem Höfling Einhard.
Sie enthielten mehr soziale Daten für einen großen Teil der Bevölkerung als andere Werke dieser Zeit.
Im späten Mittelalter wurden Biografien in Europa weniger kirchenorientiert, da Biografien von Königen, Rittern und Tyrannen auftauchten.
Nach Malory förderte die neue Betonung des Humanismus während der Renaissance die Konzentration auf weltliche Themen wie Künstler und Dichter und förderte das Schreiben in der Landessprache.
Bemerkenswert sind zwei weitere Entwicklungen: die Entwicklung des Buchdrucks im 15. Jahrhundert und die allmähliche Zunahme der Alphabetisierung.
A General History of the Pyrates (1724) von Charles Johnson hat großen Einfluss auf die Entstehung populärer Vorstellungen von Piraten und ist die Hauptquelle für die Biografien vieler bekannter Piraten.
Carlyle behauptete, dass das Leben großer Menschen für das Verständnis der Gesellschaft und ihrer Institutionen von wesentlicher Bedeutung sei.
Boswells Werk war einzigartig in seinem Rechercheniveau, das Archivstudien, Augenzeugenberichte und Interviews umfasste, seiner robusten und attraktiven Erzählung und seiner ehrlichen Darstellung aller Aspekte von Johnsons Leben und Charakter – eine Formel, die als Grundlage für die Biographie dient Literatur bis heute.
Die Zahl der gedruckten Biografien verzeichnete jedoch dank eines wachsenden Leserpublikums ein rasantes Wachstum.
Zeitschriften begannen mit der Veröffentlichung einer Reihe biografischer Skizzen.
Soziologische Biografien betrachteten die Handlungen ihrer Subjekte als Ergebnis der Umwelt und tendierten dazu, die Individualität herunterzuspielen.
Das konventionelle Konzept von Helden und Erfolgsnarrativen verschwand in der Obsession mit der psychologischen Erforschung der Persönlichkeit.
Bis zu diesem Zeitpunkt waren viktorianische Biografien, wie Strachey im Vorwort bemerkte, „so vertraut wie das Gefolge des Bestatters“ gewesen und hatten den gleichen Hauch von „langsamer, beerdigter Barbarei“.
Weltweite Berühmtheit erlangte das Buch durch seinen respektlosen und witzigen Stil, seinen prägnanten und sachlich korrekten Charakter sowie seine kunstvolle Prosa.
Robert Graves (I, Claudius, 1934) ragte unter denen heraus, die Stracheys Modell der „Entlarvung von Biografien“ folgten.
Bis zum Ersten Weltkrieg waren billige Hardcover-Nachdrucke populär geworden.
Neben biografischen Dokumentarfilmen produzierte Hollywood zahlreiche kommerzielle Filme, die auf dem Leben berühmter Persönlichkeiten basieren.
Im Gegensatz zu Büchern und Filmen erzählen sie oft keine chronologische Erzählung, sondern sind Archive vieler diskreter Medienelemente, die sich auf eine einzelne Person beziehen, darunter Videoclips, Fotos und Textartikel.
Allgemeine „Life Writing“-Techniken sind Gegenstand wissenschaftlicher Untersuchungen.
Die Informationen können aus „mündlichen Überlieferungen, persönlichen Erzählungen, Biografien und Autobiografien“ oder „Tagebüchern, Briefen, Memoranden und anderen Materialien“ stammen.
Burgen im europäischen Stil entstanden im 9. und 10. Jahrhundert, nachdem der Untergang des Karolingischen Reiches zur Aufteilung seines Territoriums unter einzelnen Herren und Fürsten führte.
Städtische Burgen dienten der Kontrolle der örtlichen Bevölkerung und wichtiger Reiserouten, und ländliche Burgen befanden sich oft in der Nähe von Einrichtungen, die für das Leben in der Gemeinde von wesentlicher Bedeutung waren, wie etwa Mühlen, fruchtbares Land oder eine Wasserquelle.
Im späten 12. und frühen 13. Jahrhundert entstand ein wissenschaftlicher Ansatz zur Burgverteidigung.
Diese Veränderungen in der Verteidigung wurden auf eine Mischung aus Burgtechnologie aus den Kreuzzügen, etwa konzentrischen Befestigungen, und Inspiration aus früheren Verteidigungsanlagen, etwa römischen Festungen, zurückgeführt.
Obwohl Schießpulver im 14. Jahrhundert in Europa eingeführt wurde, hatte es erst im 15. Jahrhundert wesentliche Auswirkungen auf den Burgbau, als die Artillerie stark genug wurde, um Steinmauern zu durchbrechen.
Der Feudalismus war die Verbindung zwischen einem Herrn und seinem Vasallen, wobei der Herr dem Vasallen als Gegenleistung für Militärdienst und die Erwartung von Loyalität Land gewährte.
Burgen dienten einer Reihe von Zwecken, von denen die wichtigsten militärischer, administrativer und häuslicher Natur waren.
Als Wilhelm der Eroberer durch England vorrückte, befestigte er Schlüsselpositionen, um das von ihm eroberte Land zu sichern.
Eine Burg konnte als Festung und Gefängnis fungieren, war aber auch ein Ort, an dem ein Ritter oder Lord seine Standesgenossen unterhalten konnte.
In verschiedenen Teilen der Welt teilten analoge Bauwerke Merkmale der Festung und andere charakteristische Merkmale, die mit dem Konzept einer Burg verbunden sind, obwohl sie in unterschiedlichen Zeiten und unter unterschiedlichen Umständen entstanden waren und unterschiedliche Entwicklungen und Einflüsse erlebten.
Im 16. Jahrhundert, als japanische und europäische Kulturen aufeinander trafen, ging die Befestigung in Europa über Burgen hinaus und stützte sich auf Innovationen wie die italienischen Trace-Italienne- und Sternfestungen.
Der Erdaushub zur Errichtung des Hügels hinterließ einen Graben um die Hügelkuppe herum, einen sogenannten Wassergraben (der entweder nass oder trocken sein konnte).
Es war ein gemeinsames Merkmal von Burgen und die meisten hatten mindestens eine.
Die Wasserversorgung erfolgte über einen Brunnen oder eine Zisterne.
Obwohl Burgen oft mit Burgruinen vom Typ Burgruine und Vorburg in Verbindung gebracht werden, können sie auch als unabhängige Verteidigungsanlagen gefunden werden.
„Bergfried“ war kein Begriff, der im Mittelalter verwendet wurde – der Begriff wurde ab dem 16. Jahrhundert verwendet – stattdessen wurde „Donjon“ verwendet, um sich auf große Türme oder auf Lateinisch Turris zu beziehen.
Obwohl der Bergfried oft der stärkste Teil einer Burg und ein letzter Zufluchtsort war, wenn die äußeren Verteidigungsanlagen fielen, blieb er im Falle eines Angriffs nicht leer, sondern diente dem Burgherrn, dem die Burg gehörte, oder seinen Gästen oder Vertretern als Wohnsitz.
Gehwege entlang der Oberseite der Ringmauern ermöglichten es den Verteidigern, Raketen auf die darunter liegenden Feinde abzufeuern, und Zinnen boten ihnen zusätzlichen Schutz.
Die Vorderseite des Tores war ein blinder Fleck und um dies zu überwinden, wurden auf jeder Seite des Tores vorspringende Türme in einem Stil hinzugefügt, der dem von den Römern entwickelten Stil ähnelte.
Der Durchgang durch das Torhaus wurde verlängert, um die Zeit zu verlängern, die ein Angreifer auf engstem Raum unter Beschuss verbringen musste und nicht in der Lage war, sich zu rächen.
Sie wurden höchstwahrscheinlich verwendet, um Gegenstände auf Angreifer abzuwerfen oder um Wasser auf Feuer zu gießen, um diese zu löschen.
Es könnte eine kleinere horizontale Öffnung hinzugefügt werden, um dem Bogenschützen eine bessere Sicht zum Zielen zu geben.
Die frühesten Befestigungsanlagen entstanden im Fruchtbaren Halbmond, im Indus-Tal, in Ägypten und in China, wo Siedlungen durch große Mauern geschützt wurden.
Viele Erdarbeiten sind heute noch erhalten, ebenso wie Hinweise auf Palisaden, die die Gräben begleiten.
Obwohl sie primitiv waren, waren sie oft effektiv und konnten nur durch den umfassenden Einsatz von Belagerungsmaschinen und anderen Belagerungskriegstechniken überwunden werden, beispielsweise in der Schlacht von Alesia.
In Diskussionen wurde der Aufstieg der Burg typischerweise auf eine Reaktion auf Angriffe von Magyaren, Muslimen und Wikingern sowie auf die Notwendigkeit einer privaten Verteidigung zurückgeführt.
An sicheren Orten gibt es teilweise hohe Konzentrationen von Burgen, während es in einigen Grenzregionen relativ wenige Burgen gab.
Der Bau der Halle aus Stein machte sie nicht unbedingt immun gegen Feuer, da sie immer noch über Fenster und eine Holztür verfügte.
Burgen dienten nicht nur der Verteidigung, sondern stärkten auch die Kontrolle eines Herrschers über sein Land.
Im Jahr 864 verbot der König von Westfranken, Karl der Kahle, den Bau von Castellas ohne seine Erlaubnis und befahl deren Zerstörung.
Die Schweiz ist ein Extremfall, in dem es keine staatliche Kontrolle darüber gibt, wer Burgen baut, und so gab es im Land nur 4.000 Burgen.
Im Jahr 950 gab es in der Provence zwölf Burgen, im Jahr 1000 waren es bereits 30 und im Jahr 1030 bereits über 100.
Im frühen 11. Jahrhundert war die Motte und der Bergfried – ein künstlicher Hügel mit einer Palisade und einem Turm darauf – die häufigste Form einer Burg in Europa, überall außer in Skandinavien.
Obwohl der Steinbau später anderswo üblich wurde, war er ab dem 11. Jahrhundert das Hauptbaumaterial für christliche Burgen in Spanien, während gleichzeitig Holz immer noch das vorherrschende Baumaterial in Nordwesteuropa war.
Vor dem 12. Jahrhundert waren Burgen in Dänemark ebenso selten wie in England vor der normannischen Eroberung.
Ihre Dekoration orientierte sich an der romanischen Architektur und enthielt manchmal Doppelfenster, ähnlich denen, die man in den Glockentürmen von Kirchen findet.
Obwohl sie von ihren steinernen Nachfolgern abgelöst wurden, waren Holz- und Erdburgen keineswegs nutzlos.
Bis zum Ende des 12. Jahrhunderts hatten Burgen im Allgemeinen nur wenige Türme; ein Tor mit wenigen Verteidigungselementen wie Schießscharten oder einem Fallgitter; ein großer Bergfried oder Bergfried, meist quadratisch und ohne Schießscharten; und die Form wäre durch die Lage des Landes bestimmt worden (das Ergebnis waren oft unregelmäßige oder krummlinige Strukturen).
Die Türme ragten aus den Mauern hervor und verfügten auf jeder Ebene über Schießscharten, damit Bogenschützen auf jeden zielen konnten, der sich der Vorhangfassade näherte oder sich an ihr befand.
Wo Festungen existierten, waren sie nicht mehr quadratisch, sondern vieleckig oder zylindrisch.
Die Türme wurden vermutlich im 12. Jahrhundert erbaut und dienten als flankierendes Feuer.
Es schien, dass die Kreuzfahrer aus ihren Konflikten mit den Sarazenen und der Begegnung mit der byzantinischen Architektur viel über die Befestigung gelernt hatten.
Legenden wurden diskreditiert, und im Fall von Jakobus von St. Georg wurde nachgewiesen, dass er aus Saint-Georges-d'Espéranche in Frankreich stammte.
Die Burgbauer Westeuropas waren sich des römischen Designs bewusst und wurden davon beeinflusst. Spätrömische Küstenfestungen an der englischen „Saxon Shore“ wurden wiederverwendet und in Spanien imitierte die Mauer um die Stadt Ávila bei ihrem Bau im Jahr 1091 die römische Architektur.
Ein Beispiel für diesen Ansatz ist Kerak.
Die Burgen, die sie zur Sicherung ihrer Erwerbungen gründeten, wurden größtenteils von syrischen Maurermeistern entworfen.
Während Burgen dazu dienten, einen Standort zu sichern und die Bewegung der Armeen zu kontrollieren, blieben im Heiligen Land einige wichtige strategische Positionen unbefestigt.
Die Gestaltung variierte nicht nur zwischen den Orden, sondern auch zwischen den einzelnen Burgen, obwohl die in dieser Zeit gegründeten Burgen häufig über konzentrische Verteidigungsanlagen verfügten.
Wenn es den Angreifern gelang, die erste Verteidigungslinie zu überwinden, gerieten sie in den Tötungsbereich zwischen der Innen- und Außenmauer und mussten die zweite Mauer angreifen.
Beispielsweise war es bei Kreuzfahrerburgen üblich, dass sich das Haupttor an der Seite eines Turms befand und der Durchgang zwei Wendungen aufwies, was die Zeit verlängerte, die jemand brauchte, um die Außenmauer zu erreichen.
Obwohl es in Preußen und Livland Hunderte von Holzburgen gab, war die Verwendung von Ziegeln und Mörtel in der Region vor den Kreuzfahrern unbekannt.
Pfeilschlitze beeinträchtigten nicht die Stärke der Mauer, doch erst mit dem Burgbauprogramm Eduards I. wurden sie in Europa weit verbreitet.
Obwohl Maschikulis denselben Zweck erfüllten wie die hölzernen Galerien, handelte es sich wahrscheinlich eher um eine Erfindung des Ostens als um eine Weiterentwicklung der hölzernen Form.
Konflikte und Interaktionen zwischen den beiden Gruppen führten zu einem Austausch architektonischer Ideen, und spanische Christen übernahmen die Verwendung freistehender Türme.
Der französische Historiker François Gebelin schrieb: „Die große Wiederbelebung der Militärarchitektur wurde, wie man es natürlich erwarten würde, von den mächtigen Königen und Fürsten der Zeit angeführt; von den Söhnen Wilhelms des Eroberers und ihren Nachkommen, den Plantagenets, als sie Herzöge wurden.“ der Normandie.
Die neuen Burgen waren im Allgemeinen leichter gebaut als frühere Bauwerke und stellten nur wenige Neuerungen dar, obwohl dennoch starke Stätten wie die von Raglan in Wales geschaffen wurden.
Diese Waffen waren zu schwer, als dass ein Mann sie hätte tragen und abfeuern können, aber wenn er das Kolbenende abstützte und die Mündung auf den Rand der Kanonenöffnung legte, konnte er die Waffe abfeuern.
Diese Anpassung ist in ganz Europa zu finden, und obwohl das Holz nur selten erhalten ist, gibt es auf Schloss Doornenburg in den Niederlanden ein intaktes Beispiel.
Andere, wenn auch weniger verbreitete Arten von Öffnungen waren horizontale Schlitze, die nur eine seitliche Bewegung ermöglichten, und große quadratische Öffnungen, die eine größere Bewegung ermöglichten.
Ham ist ein Beispiel für den Trend bei neuen Burgen, auf frühere Merkmale wie Pechnasen, hohe Türme und Zinnen zu verzichten.
Um sie effektiver zu machen, wurden die Geschütze immer größer gemacht, obwohl dies ihre Fähigkeit erschwerte, entlegene Burgen zu erreichen.
Während dies für neue Burgen ausreichte, mussten bereits bestehende Bauwerke einen Weg finden, den Kanonenbeschuss zu verkraften.
Eine Lösung hierfür bestand darin, die Spitze eines Turms abzureißen und den unteren Teil mit Schutt zu füllen, um eine Angriffsfläche für die Kanonen zu schaffen.
Daraus entwickelten sich Sternenfestungen, auch Trace Italienne genannt.
Die zweite Wahl erwies sich als beliebter, da sich herausstellte, dass es wenig Sinn machte, den Standort angesichts der Kanonen wirklich verteidigbar zu machen.
Einige echte Burgen wurden auf dem amerikanischen Kontinent von den spanischen und französischen Kolonien erbaut.
Neben anderen Verteidigungsanlagen (darunter Forts und Zitadellen) wurden in Neufrankreich gegen Ende des 17. Jahrhunderts auch Burgen errichtet.
Das Herrenhaus und die Stallungen befanden sich innerhalb einer befestigten Vorburg mit einem hohen runden Türmchen in jeder Ecke.
Obwohl der Bau von Burgen gegen Ende des 16. Jahrhunderts nachließ, wurden nicht zwangsläufig alle Burgen außer Betrieb genommen.
In anderen Fällen spielten sie immer noch eine Rolle in der Verteidigung.
In späteren Konflikten, wie dem Englischen Bürgerkrieg (1641–1651), wurden viele Burgen neu befestigt, später jedoch geschleift, um eine erneute Nutzung zu verhindern.
Renaissance- oder Scheinburgen wurden als Ausdruck eines romantischen Interesses am Mittelalter und an der Ritterlichkeit sowie als Teil der umfassenderen Neugotik in der Architektur populär.
Dies lag daran, dass die Häuser, wenn sie dem mittelalterlichen Design treu geblieben wären, nach zeitgenössischen Maßstäben kalt und dunkel geblieben wären.
Follies waren ähnlich, unterschieden sich jedoch von künstlichen Ruinen dadurch, dass sie nicht Teil einer geplanten Landschaft waren, sondern keinen Grund für ihren Bau zu haben schienen.
Eine Burg mit Erdwällen, einer Motte, Holzverteidigungen und Gebäuden könnte von ungelernten Arbeitskräften errichtet worden sein.
Die Kosten für den Bau einer Burg variierten je nach Faktoren wie Komplexität und Transportkosten für Material.
In der Mitte befanden sich Burgen wie Orford, das im späten 12. Jahrhundert für 1.400 britische Pfund erbaut wurde, und am oberen Ende befanden sich Burgen wie Dover, die zwischen 1181 und 1191 etwa 7.000 britische Pfund kosteten.
Die Kosten für den Bau eines großen Schlosses in dieser Zeit (irgendwo zwischen 1.000 und 10.000 britischen Pfund) würden die Einnahmen mehrerer Herrenhäuser verschlingen und die Finanzen eines Lords erheblich beeinträchtigen.
Mittelalterliche Maschinen und Erfindungen wie der Tretradkran wurden beim Bau unverzichtbar, und die Techniken zum Bau von Holzgerüsten wurden seit der Antike verbessert.
In vielen Ländern gab es sowohl Burgen aus Holz als auch aus Stein, in Dänemark hingegen gab es nur wenige Steinbrüche, weshalb die meisten Burgen aus Erde und Holz gebaut oder später aus Ziegeln gebaut wurden.
Als beispielsweise Tattershall Castle zwischen 1430 und 1450 erbaut wurde, gab es in der Nähe reichlich Stein, doch der Besitzer, Lord Cromwell, entschied sich für die Verwendung von Ziegeln.
Er verließ sich auf die Unterstützung seiner Untergebenen, da ein Lord ohne die Unterstützung seiner mächtigeren Pächter damit rechnen musste, dass seine Macht untergraben würde.
Dies galt insbesondere für Könige, die manchmal Land in verschiedenen Ländern besaßen.
Königliche Haushalte hatten im Wesentlichen die gleiche Form wie freiherrliche Haushalte, allerdings in viel größerem Umfang und mit prestigeträchtigeren Positionen.
Als gesellschaftliche Zentren waren Burgen wichtige Schauplätze.
Burgen wurden mit Kathedralen als Objekte architektonischen Stolzes verglichen, und einige Burgen bauten Gärten als Zierelemente ein.
Höfische Liebe war die Erotisierung der Liebe zwischen dem Adel.
Die Legende von Tristan und Iseult ist ein Beispiel für höfische Liebesgeschichten im Mittelalter.
Der Zweck der Ehe zwischen den mittelalterlichen Eliten bestand darin, sich Land zu sichern.
Dies ergibt sich aus dem Bild der Burg als kriegerischer Institution, doch die meisten Burgen in England, Frankreich, Irland und Schottland waren nie in Konflikte oder Belagerungen verwickelt, sodass das häusliche Leben ein vernachlässigter Aspekt ist.
Beispielsweise liegen viele Burgen in der Nähe von Römerstraßen, die auch im Mittelalter wichtige Transportwege blieben oder zu Veränderungen oder der Schaffung neuer Straßensysteme in der Region führen konnten.
Städtische Burgen waren besonders wichtig für die Kontrolle von Bevölkerungs- und Produktionszentren, insbesondere bei einer Invasionsmacht. Beispielsweise wurden nach der normannischen Eroberung Englands im 11. Jahrhundert die meisten königlichen Burgen in oder in der Nähe von Städten errichtet.
Ländliche Burgen wurden oft mit Mühlen und Feldsystemen in Verbindung gebracht, da sie bei der Verwaltung des Grundbesitzes des Grundherrn eine Rolle spielten und ihnen so einen größeren Einfluss auf die Ressourcen verschafften.
Sie waren nicht nur praktisch, da sie eine Wasserversorgung und frischen Fisch gewährleisteten, sondern sie waren auch ein Statussymbol, da ihr Bau und ihre Wartung teuer waren.
Die Vorteile des Burgbaus auf Siedlungen waren nicht auf Europa beschränkt.
Aufgrund der Vorteile der Nähe zu einem Wirtschaftszentrum in einer ländlichen Landschaft und der Sicherheit, die die Verteidigungsanlagen bieten, könnten Siedlungen auch auf natürliche Weise um eine Burg herum wachsen und nicht geplant werden.
Sie befanden sich in der Regel in der Nähe bestehender Stadtverteidigungsanlagen, wie z. B. römischer Mauern, obwohl dies manchmal dazu führte, dass Gebäude an der gewünschten Stelle abgerissen wurden.
Als die Normannen im 11. und 12. Jahrhundert in Irland, Schottland und Wales einfielen, erfolgte die Besiedlung dieser Länder überwiegend außerhalb der Städte, und die Gründung von Städten war oft mit der Errichtung einer Burg verbunden.
Dies bedeutete eine enge Beziehung zwischen den Feudalherren und der Kirche, einer der wichtigsten Institutionen der mittelalterlichen Gesellschaft.
Ein weiteres Beispiel ist das Bodiam Castle aus dem 14. Jahrhundert, ebenfalls in England; Obwohl es sich offenbar um eine hochmoderne, fortschrittliche Burg handelt, befindet sie sich an einem Standort von geringer strategischer Bedeutung. Der Graben war flach und diente eher dazu, dem Standort ein eindrucksvolles Aussehen zu verleihen, als zur Verteidigung gegen Bergbau.
Garnisonen waren teuer und daher oft klein, es sei denn, die Burg war wichtig.
Im Jahr 1403 verteidigte eine Truppe von 37 Bogenschützen Caernarfon Castle während einer langen Belagerung erfolgreich gegen zwei Angriffe von Owain Glyndŵrs Verbündeten und demonstrierte damit, dass eine kleine Streitmacht effektiv sein könnte.
Unter ihm wären Ritter gewesen, die aufgrund ihrer militärischen Ausbildung als eine Art Offiziersstand agiert hätten.
Es war effizienter, die Garnison auszuhungern, als sie anzugreifen, insbesondere an den am stärksten verteidigten Standorten.
Eine lange Belagerung könnte die Armee verlangsamen und es dem Feind ermöglichen, Hilfe zu erhalten oder eine größere Streitmacht für später vorzubereiten.
Wenn die Angreifer gezwungen waren, eine Burg anzugreifen, standen ihnen viele Möglichkeiten zur Verfügung.
Das Trebuchet, das vermutlich im 13. Jahrhundert aus der Petraria hervorgegangen ist, war vor der Entwicklung der Kanonen die wirksamste Belagerungswaffe.
Ballisten oder Springalds waren Belagerungsmaschinen, die nach den gleichen Prinzipien wie Armbrüste funktionierten.
Sie wurden häufiger gegen die Garnison als gegen die Gebäude einer Burg eingesetzt.
Eine Mine, die zur Mauer führte, wurde gegraben und sobald das Ziel erreicht war, wurden die Holzstützen, die den Tunnel vor dem Einsturz schützten, verbrannt.
In Richtung des Belagerertunnels könnte eine Gegenmine gegraben werden; Unter der Annahme, dass die beiden konvergierten, würde dies zu einem unterirdischen Nahkampf führen.
Sie wurden verwendet, um die Burgtore gewaltsam zu öffnen, obwohl sie manchmal mit geringerer Wirkung gegen Mauern eingesetzt wurden.
Eine sicherere Option für diejenigen, die eine Burg stürmten, war die Verwendung eines Belagerungsturms, manchmal auch Glockenturm genannt.
Die Reichsstände oder drei Stände waren die allgemeinen Ordnungen der sozialen Hierarchie, die in der Christenheit (dem christlichen Europa) vom Mittelalter bis zum frühneuzeitlichen Europa verwendet wurden.
Die Monarchie umfasste den König und die Königin, während das System aus Geistlichen (Erster Stand), Adligen (Zweiter Stand), Bauern und Bürgertum (Dritter Stand) bestand.
In England entwickelte sich ein Zwei-Stände-System, das Adel und Klerus in einem herrschaftlichen Stand mit „Commons“ als zweitem Stand vereinte.
In Schottland waren die drei Stände der Klerus (Erster Stand), der Adel (Zweiter Stand) und die Shire Commissioners oder „Bürger“ (Dritter Stand), die die Bourgeoisie, die Mittelschicht und die Unterschicht repräsentierten.
Da Geistliche nicht heiraten durften, war diese Mobilität theoretisch auf eine Generation beschränkt.
Huizinga Das Untergang des Mittelalters (1919, 1924:47).
Bürger galten allgemein als die unterste Schicht.
In vielen Regionen und Reichen gab es auch Bevölkerungsgruppen, die außerhalb dieser speziell definierten Wohnsitze geboren wurden.
Der wirtschaftliche und politische Wandel des ländlichen Raums in dieser Zeit wurde durch ein starkes Wachstum der Bevölkerung, der landwirtschaftlichen Produktion, technologischer Innovationen und städtischer Zentren begleitet; Reform- und Erneuerungsbewegungen versuchten, die Unterscheidung zwischen Geistlichem und Laienstatus zu verschärfen, und auch die von der Kirche anerkannte Macht hatte ihre Wirkung.
Die zweite Ordnung, die Kämpfer, war der Rang der politisch Mächtigen, Ehrgeizigen und Gefährlichen.
Darüber hinaus verließen sich der Erste und der Zweite Stand auf die Arbeit des Dritten, was dessen minderwertigen Status umso deutlicher machte.
Die meisten wurden in dieser Gruppe geboren und starben auch als Teil dieser Gruppe.
Im Mai 1776 wurde Finanzminister Turgot entlassen, weil er keine Reformen durchführte.
Als er sie nicht davon überzeugen konnte, seinem „idealen Programm“ zuzustimmen, versuchte Ludwig XVI., die Generalstände aufzulösen, doch der Dritte Stand hielt an ihrem Recht auf Vertretung fest.
Da das schottische Parlament ein Einkammersystem war, saßen alle Mitglieder in derselben Kammer, im Gegensatz zum englischen House of Lords und House of Commons.
Wie in England entwickelte sich das irische Parlament aus dem „Großen Rat“ Magnum Concilium, der vom Gouverneur Irlands einberufen wurde und an dem der Rat (curia regis), Magnaten (Feudalherren) und Prälaten (Bischöfe und Äbte) teilnahmen.
Im Jahr 1297 wurden Grafschaften erstmals durch gewählte Ritter des Auenlandes vertreten (zuvor waren Sheriffs sie vertreten).
Jeder war ein freier Mann und hatte bestimmte Rechte und Pflichten sowie das Recht, Vertreter in den Reichstag der Stände zu entsenden.
Vor dem 18. Jahrhundert hatte der König das Recht, eine entscheidende Stimme abzugeben, wenn die Stände gleichmäßig aufgeteilt waren.
Nach dem Landtag von Porvoo wurde der finnische Landtag jedoch erst 1863 wieder einberufen.
Um 1400 wurde das Briefpatent eingeführt, 1561 kamen die Ränge Graf und Baron hinzu und 1625 wurde das Adelshaus als Erster Stand des Landes kodifiziert.
Die Oberhäupter der Adelshäuser waren erbliche Mitglieder der Adelsversammlung.
Dadurch erlangte der Hochadel großen politischen Einfluss.
In späteren Jahrhunderten gehörten zum Nachlass Lehrer von Universitäten und bestimmten staatlichen Schulen.
Der Handel war in den Städten nur erlaubt, als die merkantilistische Ideologie die Oberhand gewonnen hatte, und die Bürger hatten das ausschließliche Recht, im Rahmen von Zünften Handel zu treiben.
Damit aus einer Siedlung eine Stadt werden konnte, war eine königliche Charta erforderlich, die das Marktrecht gewährte, und für den Außenhandel waren königlich gecharterte Stapelhafenrechte erforderlich.
Da der Großteil der Bevölkerung bis zum 19. Jahrhundert aus unabhängigen Bauernfamilien und nicht aus Leibeigenen oder Gutsbesitzern bestand, gibt es einen bemerkenswerten Unterschied in der Tradition im Vergleich zu anderen europäischen Ländern.
Ihre Vertreter im Landtag wurden indirekt gewählt: Jede Gemeinde entsandte Wähler, um den Vertreter eines Wahlbezirks zu wählen.
Sie hatten keine politischen Rechte und konnten nicht wählen.
In Schweden existierte der Reichstag der Stände, bis er 1866 durch einen Zweikammer-Rikstag ersetzt wurde, der jedem mit einem bestimmten Einkommen oder Vermögen politische Rechte einräumte.
In Finnland bestand diese Rechtsaufteilung bis 1906 und stützte sich noch auf die schwedische Verfassung von 1772.
Darüber hinaus waren die in der Stadt lebenden Industriearbeiter nicht durch das Vier-Stände-System vertreten.
Später im 15. und 16. Jahrhundert wurde Brüssel zum Versammlungsort der Generalstaaten.
Als Folge der Union von Utrecht im Jahr 1579 und der darauffolgenden Ereignisse erklärten die Generalstaaten, dass sie König Philipp II. von Spanien, der auch Oberherr der Niederlande war, nicht mehr gehorchten.
Es war die Regierungsebene, auf der alle Angelegenheiten geregelt wurden, die alle sieben Provinzen betrafen, die Teil der Republik der Vereinigten Niederlande wurden.
In den südlichen Niederlanden fanden in den Generalständen von 1600 und den Generalständen von 1632 die letzten Treffen der habsburgtreuen Generalstände statt.
Sie bestand nicht mehr aus Vertretern der Staaten, geschweige denn der Stände: Nach der Verfassung von 1798 galten alle Männer als gleich.
Im Jahr 1815, als die Niederlande mit Belgien und Luxemburg vereinigt wurden, wurden die Generalstaaten in zwei Kammern aufgeteilt: die Erste Kammer und die Zweite Kammer.
Ab 1848 sieht die niederländische Verfassung vor, dass die Mitglieder der Zweiten Kammer vom Volk gewählt werden (zunächst nur von einem begrenzten Teil der männlichen Bevölkerung; seit 1919 besteht allgemeines Wahlrecht für Männer und Frauen), während die Mitglieder der Ersten Kammer dies tun von den Mitgliedern der Landesprovinz gewählt.
Der Klerus wurde durch die unabhängigen Fürstbischöfe, Fürsterzbischöfe und Fürstäbte der zahlreichen Klöster vertreten.
Viele Völker, deren Gebiete innerhalb des Heiligen Römischen Reiches jahrhundertelang unabhängig waren, hatten keine Vertreter im Reichstag, darunter auch die Reichsritter und unabhängige Dörfer.
Die vier Hauptgüter waren: Adel (dvoryanstvo), Klerus, Landbewohner und Stadtbewohner, mit einer detaillierteren Schichtung darin.
Das Bürgertum im ursprünglichen Sinne ist eng mit der Existenz von Städten verbunden, die durch ihre Stadtrechte (z. B. Stadtrechte, Stadtprivilegien, deutsches Stadtrecht) als solche anerkannt werden, sodass es außer der Bürgerschaft der Städte kein Bürgertum gab.
Historisch gesehen bezeichnete das mittelalterliche französische Wort „bourgeois“ die Bewohner der Bourges (ummauerte Marktstädte), die Handwerker, Kunsthandwerker, Kaufleute und andere, die „die Bourgeoisie“ bildeten.
Zünfte entstanden, als einzelne Geschäftsleute (z. B. Handwerker, Kunsthandwerker und Kaufleute) mit ihren mietsuchenden Feudalherren in Konflikt gerieten, die höhere Mieten als zuvor vereinbart forderten.
Sie gehören meist einer Familie an, die seit drei oder mehr Generationen bürgerlich geprägt ist.
Die Namen dieser Familien sind in der Stadt, in der sie leben, allgemein bekannt und ihre Vorfahren haben oft zur Geschichte der Region beigetragen.
Dennoch leben diese Menschen verschwenderisch und genießen die Gesellschaft der großen Künstler ihrer Zeit.
In der französischen Sprache bezeichnet der Begriff „Bourgeoisie“ fast eine eigene Kaste, obwohl eine soziale Mobilität in diese sozioökonomische Gruppe möglich ist.
Hitler misstraute dem Kapitalismus, weil er aufgrund seines Egoismus unzuverlässig sei, und er bevorzugte eine staatlich gelenkte Wirtschaft, die den Interessen des Volkes untergeordnet ist.
Hitler sagte auch, dass die Wirtschaftsbourgeoisie „nichts außer ihrem Profit kennt“.
Der Nutzen dieser Dinge lag in ihren praktischen Funktionen.
Belle de Jour (Schönheit des Tages, 1967) erzählt die Geschichte einer bürgerlichen Frau, die von ihrer Ehe gelangweilt ist und beschließt, sich zu prostituieren.
In Europa wird der Kaisertitel seit dem Mittelalter verwendet. Damals galt er aufgrund seiner Stellung als sichtbares Oberhaupt der Kirche und geistlicher Führer des katholischen Teils Westeuropas als gleichwertig oder nahezu gleichwertig mit dem Titel eines Papstes .
Soweit es eine strenge Definition des Kaisers gibt, besagt diese, dass ein Kaiser keine Beziehungen hat, die die Überlegenheit eines anderen Herrschers andeuten könnten, und typischerweise über mehr als eine Nation herrscht.
Ihr Status wurde 1514 vom Kaiser des Heiligen Römischen Reiches offiziell anerkannt, von den russischen Monarchen jedoch erst 1547 offiziell genutzt.
Vorrömische Titel wie Großkönig oder König der Könige, die von den Königen Persiens und anderen verwendet wurden, werden oft als gleichwertig angesehen.
Mitte des 18. Jahrhunderts wurde das Imperium eher mit riesigen Territorien als mit dem Titel seines Herrschers identifiziert.
Die alten Römer verabscheuten den Namen Rex („König“) und es war für die politische Ordnung von entscheidender Bedeutung, die Formen und Vorwände der republikanischen Herrschaft aufrechtzuerhalten.
Augustus, der als erster römischer Kaiser gilt, etablierte seine Hegemonie, indem er Ämter, Titel und Ehren des republikanischen Roms, die traditionell an verschiedene Menschen verteilt waren, auf sich sammelte und die bisher verteilte Macht in einem Mann konzentrierte.
Es war jedoch die informelle Beschreibung von Imperator („Kommandeur“), die von seinen Nachfolgern zunehmend bevorzugt wurde.
Dies ist einer der beständigsten Titel: Caesar und seine Transliterationen erschienen jedes Jahr von der Zeit von Caesar Augustus bis zur Absetzung von Zar Simeon II. von Bulgarien im Jahr 1946.
Ausnahmen bilden der Titel der Augustan History, einer halbhistorischen Sammlung von Kaiserbiografien des 2. und 3. Jahrhunderts.
Allerdings wurde der Titel nur wenigen verliehen, und es war sicherlich nicht die Regel, dass alle Frauen der regierenden Kaiser ihn erhielten.
In der späten Republik, wie auch in den frühen Jahren der neuen Monarchie, war Imperator ein Titel, der römischen Generälen von ihren Truppen und dem römischen Senat nach einem großen Sieg verliehen wurde, ungefähr vergleichbar mit einem Feldmarschall (Oberhaupt oder Befehlshaber der gesamten Armee).
Die nachfolgende Nervan-Antonian-Dynastie, die den größten Teil des 2. Jahrhunderts regierte, stabilisierte das Reich.
Drei kurzlebige Sezessionsversuche hatten ihre eigenen Kaiser: das Gallische Reich, das Britanische Reich und das Palmyrenreich, wobei letzteres Rex regelmäßiger einsetzte.
Zu einem Zeitpunkt gab es bis zu fünf Teilhaber des Imperiums (siehe: Tetrarchie).
Die Stadt wird häufiger Konstantinopel genannt und heißt heute Istanbul.
Diese späteren römischen „byzantinischen“ Kaiser vollendeten den Übergang von der Idee des Kaisers als halbrepublikanischem Beamten zum Kaiser als absolutem Monarchen.
Kaiser der byzantinischen Zeit verwendeten auch das griechische Wort „Autokrator“, was „jemand, der sich selbst regiert“ oder „Monarch“ bedeutet, das traditionell von griechischen Schriftstellern zur Übersetzung des lateinischen Diktators verwendet wurde.
Tatsächlich wurde keines dieser (und anderer) zusätzlichen Beinamen und Titel jemals vollständig abgeschafft.
Nach der Tragödie der schrecklichen Plünderung der Stadt erklärten die Eroberer ein neues „Reich Rumänien“, das Historikern als Lateinisches Reich von Konstantinopel bekannt ist, und setzten Balduin IX., Graf von Flandern, als Kaiser ein.
Ab der Zeit Ottos des Großen wurde ein Großteil des ehemaligen karolingischen Königreichs Ostfranken zum Heiligen Römischen Reich.
Dieser jüngere König trug damals den Titel eines römischen Königs (König der Römer).
Der Kaiser des Heiligen Römischen Reiches galt als der erste unter den Machthabern.
Geographie wird oft in zwei Zweige unterteilt: Humangeographie und physische Geographie.
Geographie wird traditionell mit Kartographie und Ortsnamen in Verbindung gebracht.
Da Raum und Ort eine Vielzahl von Themen wie Wirtschaft, Gesundheit, Klima, Pflanzen und Tiere beeinflussen, ist die Geographie in hohem Maße interdisziplinär.
Ersteres konzentriert sich hauptsächlich auf die gebaute Umwelt und darauf, wie Menschen den Raum schaffen, betrachten, verwalten und beeinflussen.
Es erfordert ein Verständnis der traditionellen Aspekte der physischen und menschlichen Geographie, beispielsweise der Art und Weise, wie menschliche Gesellschaften die Umwelt konzipieren.
Die Untersuchung von Systemen, die größer als die Erde selbst sind, ist normalerweise Teil der Astronomie oder Kosmologie.
Regionalwissenschaft: In den 1950er Jahren entstand die von Walter Isard angeführte Regionalwissenschaftsbewegung, um im Gegensatz zu den deskriptiven Tendenzen traditioneller Geographieprogramme eine quantitativere und analytischere Grundlage für geografische Fragen bereitzustellen.
Die Kartographie hat sich von einer Sammlung von Zeichentechniken zu einer echten Wissenschaft entwickelt.
Zusätzlich zu allen anderen Teildisziplinen der Geographie müssen GIS-Spezialisten Informatik und Datenbanksysteme verstehen.
Geostatistik wird in zahlreichen Bereichen eingesetzt, darunter Hydrologie, Geologie, Erdölexploration, Wetteranalyse, Stadtplanung, Logistik und Epidemiologie.
Die von Eckhard Unger rekonstruierte Karte zeigt Babylon am Euphrat, umgeben von einer kreisförmigen Landmasse, die Assyrien, Urartu und mehrere Städte zeigt, wiederum umgeben von einem „bitteren Fluss“ (Oceanus) mit sieben Inseln, die so angeordnet sind, dass sie sich bilden ein siebenzackiger Stern.
Im Gegensatz zur Imago Mundi zeigte eine frühere babylonische Weltkarte aus dem 9. Jahrhundert v. Chr. Babylon als weiter nördlich vom Zentrum der Welt entfernt, obwohl nicht sicher ist, was dieses Zentrum darstellen sollte.
Thales wird auch die Vorhersage von Sonnenfinsternissen zugeschrieben.
Es gibt einige Debatten darüber, wer der erste Mensch war, der behauptete, die Erde habe eine Kugelform, wobei der Verdienst entweder Parmenides oder Pythagoras zu verdanken ist.
Eine der ersten Schätzungen des Erdradius erfolgte durch Eratosthenes.
Die Meridiane wurden in 360° unterteilt, wobei jeder Grad weiter in 60 (Minuten) unterteilt wurde.
Er erweiterte das Werk von Hipparchos, indem er auf seinen Karten ein Rastersystem verwendete und eine Länge von 56,5 Meilen für einen Grad annahm.
Im Mittelalter führte der Untergang des Römischen Reiches zu einer Verschiebung der geographischen Entwicklung von Europa in die islamische Welt.
Darüber hinaus übersetzten und interpretierten islamische Gelehrte die früheren Werke der Römer und Griechen und gründeten zu diesem Zweck das Haus der Weisheit in Bagdad.
Abu Rayhan Biruni (976–1048) beschrieb erstmals eine polare äquiazimutale äquidistante Projektion der Himmelssphäre.
Er entwickelte auch ähnliche Techniken, wenn es darum ging, die Höhe von Bergen, die Tiefe von Tälern und die Weite des Horizonts zu messen.
Das Problem, vor dem sowohl Entdecker als auch Geographen standen, bestand darin, den Breiten- und Längengrad eines geografischen Ortes zu ermitteln.
Das 18. und 19. Jahrhundert waren die Zeiten, in denen die Geographie als eigenständige akademische Disziplin anerkannt und Teil eines typischen Universitätslehrplans in Europa (insbesondere Paris und Berlin) wurde.
In den letzten zwei Jahrhunderten haben die Fortschritte in der Technologie mit Computern zur Entwicklung der Geomatik geführt und neue Praktiken wie teilnehmende Beobachtung und Geostatistik in das Werkzeugportfolio der Geographie integriert.
Arnold Henry Guyot (1807–1884) – beschrieb die Struktur von Gletschern und erweiterte das Verständnis der Gletscherbewegung, insbesondere des schnellen Eisflusses.
William Morris Davis (1850–1934) – Vater der amerikanischen Geographie und Entwickler des Erosionszyklus.
Ellen Churchill Semple (1863–1932) – erste weibliche Präsidentin der Association of American Geographers.
Walter Christaller (1893–1969) – Humangeograph und Erfinder der Zentralorttheorie.
David Harvey (geb. 1935) – marxistischer Geograph und Autor von Theorien zur Raum- und Stadtgeographie, Gewinner des Vautrin-Lud-Preises.
Teilweise wird zwischen der offiziellen (Verfassungs-)Hauptstadt und dem Regierungssitz, der sich an einem anderen Ort befindet, unterschieden.
Beispiele sind das antike Babylon, das abbasidische Bagdad, das antike Athen, Rom, Bratislava, Budapest, Konstantinopel, Chang'an, das antike Cusco, Kiew, Madrid, Paris, Podgorica, London, Peking, Prag, Tallinn, Tokio, Lissabon, Riga, Vilnius, und Warschau.
In einigen Ländern wurde die Hauptstadt aus geopolitischen Gründen geändert; Finnlands erste Stadt, Turku, die seit dem Mittelalter unter schwedischer Herrschaft als Hauptstadt des Landes gedient hatte, verlor ihr Recht während des Großherzogtums Finnland im Jahr 1812, als Helsinki vom Russischen Reich zur heutigen Hauptstadt Finnlands ernannt wurde.
In Kanada gibt es eine Bundeshauptstadt, während die zehn Provinzen und drei Territorien jeweils Hauptstädte haben.
In Australien wird der Begriff „Hauptstädte“ regelmäßig für die sechs Landeshauptstädte sowie die Bundeshauptstadt Canberra und Darwin, die Hauptstadt des Northern Territory, verwendet.
Anders als in Föderationen gibt es in der Regel keine separate Landeshauptstadt, sondern die Hauptstadt einer konstituierenden Nation ist auch die Hauptstadt des gesamten Staates, wie beispielsweise London, das die Hauptstadt Englands und des Vereinigten Königreichs ist.
Die Landeshauptstädte Deutschlands und Russlands (der Stadtstaat Berlin und die Bundesstadt Moskau) sind ebenfalls eigenständige Gliedstaaten beider Länder.
Frankfort, Kentucky, auf halbem Weg zwischen Louisville und Lexington.
Tallahassee, Florida, wurde als Mittelpunkt zwischen Pensacola und St. Augustine, Florida – den beiden damals größten Städten Floridas – ausgewählt.
Änderungen im politischen Regime eines Landes führen manchmal zur Ernennung einer neuen Hauptstadt.
Als die Kanarischen Inseln 1982 eine autonome Gemeinschaft wurden, erhielten Santa Cruz de Teneriffa und Las Palmas de Gran Canaria beide den Status einer Hauptstadt.
Estland: Der Oberste Gerichtshof und das Ministerium für Bildung und Forschung befinden sich in Tartu.
Im Notfall kann der Sitz der verfassungsmäßigen Befugnisse in eine andere Stadt verlegt werden, damit die Kammern des Parlaments am selben Ort tagen wie der Präsident und das Kabinett.
Der gesamte Staatsapparat wird alle sechs Monate von einer Stadt in eine andere verlagert.
Dharamshala, das auch Sitz der tibetischen Zentralverwaltung ist, ist die zweite Winterhauptstadt des Staates.
Die Stadt selbst wird als Unionsterritorium verwaltet.
Uttarakhand: Dehradun ist die Verwaltungs- und Gesetzgebungshauptstadt, während sich der Oberste Gerichtshof in Nainital befindet.
Der Bau begann 1960 und wurde 1966 abgeschlossen.
Der Präsidentenpalast (Malacanang-Palast) und der Oberste Gerichtshof befinden sich innerhalb der Hauptstadt, die beiden Kongresshäuser liegen jedoch in getrennten Vororten.
Sri Lanka: Sri Jayawardenepura Kotte wird als Verwaltungshauptstadt und Sitz des Parlaments bezeichnet, während die ehemalige Hauptstadt Colombo nun als „Handelshauptstadt“ bezeichnet wird.
Südafrika: Die Verwaltungshauptstadt ist Pretoria, die gesetzgebende Hauptstadt ist Kapstadt und die Gerichtshauptstadt ist Bloemfontein.
Schweiz: Bern ist die Bundesstadt der Schweiz und fungiert de facto als Hauptstadt der Schweiz.
Ähnlich wie in Illinois und im Bundesstaat New York arbeiten die meisten landesweit gewählten Beamten und Beamten mit Sitz im Südosten von Pennsylvania (Stadt Philadelphia, Bucks County, Montgomery County, Delaware County und Chester County) hauptsächlich in Philadelphia.
Israel und Palästina: Sowohl die Regierung Israels als auch die Palästinensische Autonomiebehörde beanspruchen Jerusalem als ihre Hauptstadt.
Eine symbolische Verlagerung einer Hauptstadt an einen geografisch oder demografisch peripheren Ort kann aus wirtschaftlichen oder strategischen Gründen erfolgen (manchmal auch als Vorwärtshauptstadt oder Speerspitzenhauptstadt bezeichnet).
Die Ming-Kaiser verlegten ihre Hauptstadt vom zentraler gelegenen Nanjing nach Peking, um bei der Überwachung der Grenze zu den Mongolen zu helfen.
Nach der Krönung von König-Kaiser Georg V. im Jahr 1911 wurde Delhi schließlich zur Kolonialhauptstadt und blieb ab 1947 die unabhängige Hauptstadt Indiens.
Manchmal wurde der Standort einer neuen Hauptstadt gewählt, um tatsächliche oder potenzielle Streitigkeiten zwischen verschiedenen Einheiten zu beenden, beispielsweise im Fall von Canberra, Ottawa, Washington, Wellington und Managua.
In der Zeit der Drei Königreiche fielen sowohl Shu als auch Wu, als ihre jeweiligen Hauptstädte Chengdu und Jianye fielen.
Nach dem Zusammenbruch der Qing-Dynastie ermöglichten die Dezentralisierung der Autorität und verbesserte Transport- und Kommunikationstechnologien sowohl den chinesischen Nationalisten als auch den chinesischen Kommunisten, ihre Hauptstädte schnell zu verlegen und ihre Führungsstrukturen während der großen Krise der japanischen Invasion intakt zu halten.
Es kann als dauerhafter und dicht besiedelter Ort mit administrativ definierten Grenzen definiert werden, dessen Mitglieder hauptsächlich nichtlandwirtschaftliche Aufgaben erledigen.
Historisch gesehen machten Stadtbewohner nur einen kleinen Teil der gesamten Menschheit aus, aber nach zwei Jahrhunderten beispielloser und rasanter Urbanisierung lebt heute mehr als die Hälfte der Weltbevölkerung in Städten, was tiefgreifende Folgen für die globale Nachhaltigkeit hatte.
Dieser erhöhte Einfluss bedeutet, dass Städte auch erheblichen Einfluss auf globale Themen wie nachhaltige Entwicklung, globale Erwärmung und globale Gesundheit haben.
Daher werden kompakte Städte oft als entscheidendes Element im Kampf gegen den Klimawandel bezeichnet.
Beispielsweise spiegeln Landeshauptstädte wie Peking, London, Mexiko-Stadt, Moskau, Nairobi, Neu-Delhi, Paris, Rom, Athen, Seoul, Tokio und Washington, D.C. die Identität und den Höhepunkt ihrer jeweiligen Nationen wider.
Die Stadt kann als Geschichte, als Beziehungsmuster zwischen menschlichen Gruppen, als Produktions- und Vertriebsraum, als Feld physischer Gewalt, als eine Reihe miteinander verbundener Entscheidungen oder als Konfliktschauplatz betrachtet werden.
Nationale Volkszählungen verwenden eine Vielzahl von Definitionen – unter Einbeziehung von Faktoren wie Bevölkerung, Bevölkerungsdichte, Anzahl der Wohnungen, wirtschaftliche Funktion und Infrastruktur –, um Bevölkerungen als städtisch zu klassifizieren.
Die gegenseitige Abhängigkeit von Stadt und Land hat eine Konsequenz, die so offensichtlich ist, dass sie leicht übersehen wird: Auf globaler Ebene sind Städte im Allgemeinen auf Gebiete beschränkt, die eine ständige landwirtschaftliche Bevölkerung ernähren können.
Mit zunehmender Komplexität der Städte dominierten auch die großen bürgerlichen Institutionen, von Regierungssitzen bis hin zu religiösen Gebäuden, diese Konvergenzpunkte.
Die physische Umgebung bestimmt im Allgemeinen die Form, in der eine Stadt gebaut wird.
Und es kann angesichts der umgebenden Landschaft für eine optimale Verteidigung eingerichtet werden.
Diese Form könnte sich durch sukzessives Wachstum über einen langen Zeitraum entwickelt haben, wobei konzentrische Spuren von Stadtmauern und Zitadellen ältere Stadtgrenzen markieren.
In Städten wie Moskau ist dieses Muster noch immer deutlich erkennbar.
Bei Ausgrabungen in diesen Gebieten wurden Ruinen von Städten gefunden, die unterschiedlich auf Handel, Politik oder Religion ausgerichtet waren.
Chinas geplante Städte wurden nach heiligen Prinzipien errichtet und dienten als himmlische Mikrokosmen.
Diese Standorte scheinen stark reglementiert und geschichtet geplant zu sein, mit einem minimalistischen Raumraster für die Arbeiter und zunehmend aufwändigeren Wohnmöglichkeiten für höhere Klassen.
In den folgenden Jahrhunderten entwickelten unabhängige Stadtstaaten Griechenlands, insbesondere Athen, die Polis, einen Zusammenschluss männlicher Landbesitzer, die gemeinsam die Stadt bildeten.
Unter der Autorität seines Reiches veränderte und gründete Rom viele Städte (coloniae) und brachte damit seine Prinzipien der Stadtarchitektur, des Designs und der Gesellschaft mit.
Die Norte Chico-Zivilisation umfasste bis zu 30 große Bevölkerungszentren in der heutigen Region Norte Chico an der nördlichen zentralen Küste Perus.
Der Machtzentrum im Westen verlagerte sich nach Konstantinopel und zur aufsteigenden islamischen Zivilisation mit ihren Großstädten Bagdad, Kairo und Córdoba.
Im 13. und 14. Jahrhundert wurden einige Städte zu mächtigen Staaten, die umliegende Gebiete unter ihre Kontrolle brachten oder ausgedehnte Seeimperien errichteten.
Die größeren Hauptstädte Westeuropas (London und Paris) profitierten vom Wachstum des Handels nach der Entstehung des Atlantikhandels.
England war führend, als London zur Hauptstadt eines Weltreichs wurde und Städte im ganzen Land an strategischen Standorten für die Produktion wuchsen.
Unternehmerische Führung manifestierte sich in Wachstumskoalitionen bestehend aus Bauherren, Immobilienmaklern, Entwicklern, den Medien, Regierungsakteuren wie Bürgermeistern und dominanten Unternehmen.
Das Ergebnis waren Bemühungen zur Revitalisierung der Innenstadt; innerstädtische Gentrifizierung; die Umwandlung des CBD in eine fortgeschrittene Dienstleistungsbeschäftigung; Unterhaltung, Museen und kulturelle Veranstaltungsorte; der Bau von Sportstadien und Sportanlagen; und Entwicklung am Wasser.“
Bis ins 18. Jahrhundert herrschte ein Gleichgewicht zwischen der bäuerlichen Landbevölkerung und den Städten mit Märkten und kleinbäuerlicher Produktion.
Auch die kulturelle Attraktivität von Städten spielt eine Rolle bei der Anziehung von Einwohnern.
Batam (Indonesien), Mogadischu (Somalia), Xiamen (China) und Niamey (Niger) gelten mit jährlichen Wachstumsraten von 5–8 % als die am schnellsten wachsenden Städte der Welt.
Die Vereinten Nationen prognostizieren bis 2050 weltweit zusätzlich 2,5 Milliarden Stadtbewohner (und 300 Millionen weniger Landbewohner), wobei 90 % der städtischen Bevölkerungszunahme in Asien und Afrika stattfinden wird.
In diesen Städten klafft eine tiefe Kluft zwischen Arm und Reich. In der Regel gibt es eine superreiche Elite, die in geschlossenen Wohnanlagen lebt, und große Menschenmassen, die in minderwertigen Wohnungen mit unzureichender Infrastruktur und ansonsten schlechten Bedingungen leben.
Dennoch erlassen Kommunen routinemäßig weitreichende Verordnungen, die sich auf unbestimmte (und unklar definierte) Straftaten wie Herumlungern und Behinderung richten, das Erfordernis einer Genehmigung für Proteste oder die Verpflichtung von Anwohnern und Hausbesitzern, Schnee von den Gehwegen der Stadt zu entfernen.“
Diese werden mehr oder weniger routinemäßig und mehr oder weniger gleichmäßig bereitgestellt.
Diese produktionsorientierten Kriterien führen häufig zu „Dienstleistungsregeln“, regulierten Verfahren für die Erbringung von Dienstleistungen, die Versuche sind, die Produktivitätsziele städtischer Dienstleistungsbürokratien zu kodifizieren.
Robert L. Lineberry, „Mandating Urban Equality: The Distribution of Municipal Public Services“; in Hahn & Levine (1980).
Allerdings ist die Finanzierung kommunaler Dienstleistungen sowie von Stadterneuerungs- und anderen Entwicklungsprojekten ein Dauerproblem, dem Städte durch Appelle an höhere Regierungen, Vereinbarungen mit dem privaten Sektor und Techniken wie Privatisierung (Verkauf von Dienstleistungen an den privaten Sektor) begegnen. Korporatisierung (Gründung quasi-privater kommunaler Unternehmen) und Finanzialisierung (Verpackung städtischer Vermögenswerte in handelbare Finanzinstrumente und Derivate).
Die Auswirkungen der Globalisierung und die Rolle multinationaler Konzerne in lokalen Regierungen weltweit haben zu einem Perspektivwechsel in der städtischen Governance geführt, weg von der „Theorie des städtischen Regimes“, in der eine Koalition lokaler Interessen funktional regiert, hin zu einer Theorie der Außenwirtschaft Kontrolle, die in der Wissenschaft häufig mit der Philosophie des Neoliberalismus in Verbindung gebracht wird.
Zu den Planungsinstrumenten, die über den ursprünglichen Entwurf der Stadt selbst hinausgehen, gehören öffentliche Kapitalinvestitionen in die Infrastruktur und Landnutzungskontrollen wie die Zoneneinteilung.
Den Städten stehen bei der Umsetzung ihrer Planungsziele auch kommunale Befugnisse zur Bebauung, zur Kontrolle der Unterteilung und zur Regelung von Bau-, Wohn- und Sanitärgrundsätzen zur Verfügung.“
Menschen, die relativ nahe beieinander leben, können in getrennten Gebieten leben, arbeiten und spielen und mit unterschiedlichen Menschen verkehren, wodurch ethnische oder Lebensstil-Enklaven oder, in Gebieten mit konzentrierter Armut, Ghettos gebildet werden.
Vororte im Westen und zunehmend geschlossene Wohnanlagen und andere Formen der „Privatopie“ auf der ganzen Welt ermöglichen es den lokalen Eliten, sich in sichere und exklusive Viertel abzusondern.
Dieses ausgestoßene Proletariat – heute vielleicht 1,5 Milliarden Menschen, 2,5 Milliarden im Jahr 2030 – ist die am schnellsten wachsende und neuartigste soziale Klasse auf dem Planeten.
Es ist ontologisch sowohl ähnlich als auch unähnlich zu der im Kommunistischen Manifest beschriebenen historischen Handlungsmacht.
Als Handelszentren sind Städte seit langem die Heimat des Einzelhandels und des Konsums über die Schnittstelle zum Einkaufen.
Ein dichterer Arbeitsmarkt ermöglicht eine bessere Abstimmung der Qualifikationen zwischen Unternehmen und Einzelpersonen.
Kulturelle Eliten leben in der Regel in Städten, sind durch gemeinsames Kulturkapital miteinander verbunden und spielen selbst eine gewisse Rolle in der Regierungsführung.
Greg Kerr und Jessica Oliver, „Rethinking Place Identities“, in Kavaratzis, Warnaby & Ashworth (2015).
Patriotische Touristen besuchen Agra, um das Taj Mahal zu besichtigen, oder New York City, um das World Trade Center zu besuchen.
Warum ziehen anonyme Menschen – die Armen, die Unterprivilegierten, die Unverbundenen – häufig das Leben unter miserablen Bedingungen in Mietskasernen der gesunden Ordnung und Ruhe kleiner Städte oder den Sanitäranlagen halbländlicher Siedlungen vor?
Diejenigen, die dort lebten, taten dies, um auf jedem erreichbaren Niveau teilzunehmen und sich zu messen.
Sport spielt auch eine wichtige Rolle bei der Markenbildung der Stadt und der Bildung lokaler Identität.
Noch wichtiger ist, dass es auch ein enormes langfristiges Potenzial sowohl für den Tourismus als auch für Investitionen gibt (Kasimati, 2003).
Der Krieg brachte die Konzentration der gesellschaftlichen Führung und der politischen Macht in den Händen einer waffentragenden Minderheit, unterstützt durch eine Priesterschaft, die heilige Kräfte ausübte und über geheimes, aber wertvolles wissenschaftliches und magisches Wissen verfügte.“
Während des Zweiten Weltkriegs erklärten nationale Regierungen gelegentlich bestimmte Städte für offen und übergaben sie praktisch einem vorrückenden Feind, um Schaden und Blutvergießen zu vermeiden.
Eine solche als Aufstandsbekämpfung bezeichnete Kriegsführung umfasst Techniken der Überwachung und psychologischen Kriegsführung sowie des Nahkampfs und erweitert funktional die moderne städtische Kriminalprävention, die bereits Konzepte wie den verteidigungsfähigen Raum nutzt.
Aufgrund der höheren Eintrittsbarrieren wurden diese Netzwerke als natürliche Monopole eingestuft, was bedeutet, dass die ökonomische Logik die Kontrolle jedes Netzwerks durch eine einzelne Organisation, ob öffentlich oder privat, begünstigt.
Kath Wellman und Frederik Pretorius, „Städtische Infrastruktur: Produktivität, Projektbewertung und Finanzierung“; in Wellman & Spiller (2012).
Sanitäranlagen, die für eine gute Gesundheit unter beengten Verhältnissen notwendig sind, erfordern Wasserversorgung und Abfallentsorgung sowie individuelle Hygiene.
Das moderne städtische Leben ist für den Betrieb elektrischer Maschinen (von Haushaltsgeräten über Industriemaschinen bis hin zu heute allgegenwärtigen elektronischen Systemen für Kommunikation, Wirtschaft und Regierung) sowie für Ampeln, Straßenlaternen und Innenbeleuchtung in hohem Maße auf die durch Elektrizität übertragene Energie angewiesen.
Tom Hart, „Transport und die Stadt“; in Paddison (2001).
In vielen amerikanischen Großstädten wird der öffentliche Nahverkehr immer noch mit der Bahn betrieben, wie beispielsweise das allseits beliebte New Yorker U-Bahn-System.
Vom Menschen verursachte Gebäude und Abfälle sowie der Anbau in Gärten schaffen physikalische und chemische Umgebungen, die in der Wildnis ihresgleichen suchen, und ermöglichen in einigen Fällen eine außergewöhnliche Artenvielfalt.
Aus einer Perspektive sind Städte aufgrund ihres Ressourcenbedarfs nicht ökologisch nachhaltig.
Moderne Städte sind dafür bekannt, durch Beton, Asphalt und andere künstliche Oberflächen, die sich im Sonnenlicht erwärmen und Regenwasser in unterirdische Kanäle leiten, ihr eigenes Mikroklima zu schaffen.
Luftpartikel erhöhen die Niederschlagsmenge um 5–10 %.
Innerhalb des städtischen Mikroklimas beispielsweise ertragen weniger bewachsene arme Viertel mehr Hitze (haben aber weniger Möglichkeiten, damit umzugehen).
Im Allgemeinen werden sie städtischer Freiraum (obwohl dieses Wort nicht immer Grünraum bedeutet), Grünraum, städtische Begrünung genannt.
Für die Studie wurden Daten von fast 20.000 Menschen im Vereinigten Königreich verwendet.
Personen, die nicht mindestens zwei Stunden bekamen – selbst wenn sie mehr als eine Stunde pro Woche leisteten – erhielten keine Leistungen.
In der Studie zählte die Zeit, die eine Person im eigenen Hof oder Garten verbrachte, nicht als Zeit in der Natur, aber die meisten Naturbesuche in der Studie fanden innerhalb von zwei Meilen von zu Hause statt. "
Saskia Sassen verwendete den Begriff „Global City“ in ihrem Werk The Global City: New York, London, Tokyo aus dem Jahr 1991, um sich auf die Macht, den Status und die Weltoffenheit einer Stadt zu beziehen, nicht auf ihre Größe.
3 (1982): 319 Globale Städte bilden den Schlussstein der globalen Hierarchie und üben durch ihren wirtschaftlichen und politischen Einfluss Befehl und Kontrolle aus.
Kritiker des Begriffs verweisen auf die unterschiedlichen Bereiche von Macht und Austausch.
Multinationale Konzerne und Banken haben ihren Hauptsitz in Weltstädten und wickeln einen Großteil ihrer Geschäfte in diesem Kontext ab.
Nancy Duxbury und Sharon Jeannotte, „Global Cultural Governance Policy“; Kapitel 21 in The Ashgate Research Companion to Planning and Culture; London: Ashgate, 2013.
Auf der Habitat-I-Konferenz im Jahr 1976 wurde die „Vancouver Declaration on Human Settlements“ verabschiedet, die das Stadtmanagement als einen grundlegenden Aspekt der Entwicklung identifiziert und verschiedene Grundsätze für die Erhaltung städtischer Lebensräume festlegt.
Im Januar 2002 wurde die UN-Kommission für menschliche Siedlungen zu einer Dachorganisation mit dem Namen „United Nations Human Settlements Programme“ oder UN-Habitat, einem Mitglied der UN-Entwicklungsgruppe.
Die Politik der Bank konzentrierte sich in der Regel auf die Stärkung der Immobilienmärkte durch Kredite und technische Hilfe.
Städte spielen in der traditionellen westlichen Kultur eine herausragende Rolle und erscheinen in der Bibel sowohl in böser als auch in heiliger Form, symbolisiert durch Babylon und Jerusalem.
Städte können als Extreme oder Gegensätze wahrgenommen werden: befreiend und unterdrückend, reich und arm, organisiert und chaotisch zugleich.
Diese und andere politische Ideologien beeinflussen Narrative und Themen im Diskurs über Städte stark.
Die klassische und mittelalterliche Literatur umfasst eine Gattung von Beschreibungen, die sich mit städtischen Merkmalen und der Geschichte befassen.
Andere frühe filmische Darstellungen von Städten im 20. Jahrhundert stellten sie im Allgemeinen als technologisch effiziente Räume mit reibungslos funktionierenden Systemen des Automobiltransports dar.
Ein Land ist eine eigenständige Gebietskörperschaft oder politische Einheit (d. h. eine Nation).
Es ist nicht von Natur aus souverän.
Das geografisch größte Land der Welt ist Russland, während China das bevölkerungsreichste ist, gefolgt von Indien, den Vereinigten Staaten, Indonesien, Pakistan und Brasilien.
In vielen europäischen Ländern werden die Wörter für Unterteilungen des Staatsgebiets verwendet, wie in den deutschen Bundesländern, sowie als weniger formelle Bezeichnung für einen souveränen Staat.
Über die Zahl der „Länder“ auf der Welt gibt es keine allgemeingültige Einigung, da mehrere Staaten ihren Souveränitätsstatus bestreiten.
Der Grad der Autonomie nicht-souveräner Länder ist sehr unterschiedlich.
Der Bericht klassifiziert die Länderentwicklung anhand des Bruttonationaleinkommens (BNE) pro Kopf.
Der Bericht von 2019 erkennt nur Industrieländer in Nordamerika, Europa sowie Asien und im Pazifik an.
Die Weltbank definiert ihre Regionen als Ostasien und Pazifik, Europa und Zentralasien, Lateinamerika und Karibik, Naher Osten und Nordafrika, Nordamerika, Südasien und Afrika südlich der Sahara.
Unter Exploration versteht man die Suche mit dem Ziel, Informationen oder Ressourcen zu entdecken, insbesondere im geografischen oder weltraumbezogenen Kontext, und nicht Forschung und Entwicklung, die sich normalerweise nicht auf Geowissenschaften oder Astronomie konzentriert.
Lediglich die von Kaiser Nero durchgeführte Aktion schien eine Vorbereitung für die Eroberung Äthiopiens oder Nubiens zu sein: Im Jahr 62 n. Chr. erkundeten zwei Legionäre die Quellen des Nils.
Die Römer organisierten auch mehrere Erkundungstouren nach Nordeuropa und erkundeten bis nach China in Asien.
100 n. Chr. – 166 n. Chr. Beginn der römisch-chinesischen Beziehungen.
Die wichtigste Erfindung ihrer Erforschung war das Auslegerkanu, das eine schnelle und stabile Plattform für den Transport von Gütern und Personen bot.
Studien aus dem Jahr 2011 in Wairau Bar in Neuseeland zeigen mit hoher Wahrscheinlichkeit, dass ein Ursprung die Insel Ruahine auf den Gesellschaftsinseln war.
Es gibt kulturelle und sprachliche Ähnlichkeiten zwischen den Cook-Insulanern und den neuseeländischen Maori.
Zwischen 1328 und 1333 segelte er entlang des Südchinesischen Meeres und besuchte viele Orte in Südostasien. Er gelangte bis nach Südasien, landete in Sri Lanka und Indien und reiste sogar nach Australien.
Portugal und Spanien dominierten die ersten Phasen der Erkundung, während andere europäische Nationen wie England, die Niederlande und Frankreich folgten.
Die extremen Bedingungen in der Tiefsee erfordern aufwändige Methoden und Technologien, um ihnen standzuhalten.
Unter einer administrativen Untergliederung wird hingegen die Aufteilung eines Staates im eigentlichen Sinne verstanden.
Die abhängigen Gebiete, die heute noch auf der Welt verbleiben, verfügen im Allgemeinen über ein sehr hohes Maß an politischer Autonomie.
Der Status der Cookinseln gilt im Sinne des Völkerrechts als gleichbedeutend mit Unabhängigkeit, und das Land übt die volle Souveränität über seine inneren und äußeren Angelegenheiten aus.
Gemäß den Bedingungen des Freiassoziierungsabkommens behält Neuseeland jedoch einen Teil der Verantwortung für die Außenbeziehungen und die Verteidigung Niues.
Diese Liste ist im Allgemeinen auf Einheiten beschränkt, die entweder einem internationalen Abkommen über ihren Status unterliegen, unbewohnt sind oder über ein einzigartiges Maß an Autonomie verfügen und in anderen Angelegenheiten als internationalen Angelegenheiten weitgehend selbstverwaltet sind.
Es handelt sich um unabhängig verwaltete Gerichtsbarkeiten, obwohl die britische Regierung allein für die Verteidigung und die internationale Vertretung verantwortlich ist und in letzter Instanz für die Gewährleistung einer guten Regierung verantwortlich ist.
Kein von der Krone abhängiges Land ist im britischen Parlament vertreten.
Neuseeland und seine Nebengebiete haben denselben Generalgouverneur und bilden ein einziges monarchisches Reich.
Der gemeinsam ausgehandelte Pakt zur Gründung eines Commonwealth der Nördlichen Marianen (CNMI) in politischer Union mit den Vereinigten Staaten wurde 1976 genehmigt.
Dies ist eine ständige Quelle von Unklarheiten und Verwirrung, wenn versucht wird, die politische Beziehung Puerto Ricos zu den Vereinigten Staaten zu definieren, zu verstehen und zu erklären.
Der Status seiner „konstituierenden Länder“ in der Karibik (Aruba, Curaçao und Sint Maarten) kann jedoch als mit Abhängigkeiten oder „assoziierten nicht unabhängigen Staaten“ vergleichbar angesehen werden.
Grenzen sind geografische Grenzen, die entweder durch geografische Merkmale wie Ozeane oder durch willkürliche Gruppierungen politischer Einheiten wie Regierungen, souveräne Staaten, föderierte Staaten und andere subnationale Einheiten festgelegt werden.
Die meisten Außengrenzen werden teilweise oder vollständig kontrolliert und dürfen nur an bestimmten Grenzkontrollpunkten und Grenzzonen legal überschritten werden.
In den meisten Ländern gibt es eine Art Grenzkontrolle, um die Ein- und Ausreise von Menschen, Tieren und Gütern zu regulieren oder zu begrenzen.
Um innerhalb der Grenzen eines Landes zu bleiben oder zu arbeiten, benötigen Ausländer (ausländische Personen) möglicherweise spezielle Einwanderungsdokumente oder -genehmigungen. Der Besitz solcher Dokumente garantiert jedoch nicht, dass der Person der Grenzübertritt gestattet wird.
Die meisten Länder verbieten den Transport illegaler Drogen oder gefährdeter Tiere über ihre Grenzen.
An Orten, an denen Schmuggel, Migration und Infiltration ein Problem darstellen, befestigen viele Länder ihre Grenzen mit Zäunen und Barrieren und führen formelle Grenzkontrollverfahren ein.
Dies ist in Ländern innerhalb des europäischen Schengen-Raums und in ländlichen Abschnitten der Grenze zwischen Kanada und den Vereinigten Staaten üblich.
Flüsse: Einige politische Grenzen wurden entlang natürlicher, durch Flüsse gebildeter Grenzen formalisiert.
In der hebräischen Bibel definierte Moses die Mitte des Flusses Arnon als Grenze zwischen Moab und den israelitischen Stämmen, die sich östlich des Jordan niederließen.
Beispiele sind der Tanganjikasee mit der Demokratischen Republik Kongo und Sambia an seinem Westufer sowie Tansania und Burundi im Osten; und die Großen Seen, die einen wesentlichen Teil der Grenze zwischen Kanada und den Vereinigten Staaten bilden.
Gebirgszüge: Viele Nationen haben ihre politischen Grenzen entlang von Gebirgszügen definiert, oft entlang einer Wasserscheide.
Ein Beispiel ist der Verteidigungswald, der im 11. Jahrhundert von der chinesischen Song-Dynastie angelegt wurde.
Beispielsweise ist die Grenze zwischen Ost- und Westdeutschland keine internationale Grenze mehr, aber aufgrund historischer Markierungen in der Landschaft immer noch erkennbar, und es handelt sich immer noch um eine kulturelle und wirtschaftliche Trennung in Deutschland.
Seegrenzen gibt es im Kontext von Hoheitsgewässern, Anschlusszonen und ausschließlichen Wirtschaftszonen; Die Terminologie umfasst jedoch nicht See- oder Flussgrenzen, die im Kontext von Landgrenzen betrachtet werden.
Der Luftraum erstreckt sich über 12 Seemeilen von der Küste eines Landes und ist für den Schutz seines eigenen Luftraums verantwortlich, es sei denn, er steht in Friedenszeiten unter dem Schutz der NATO.
Es besteht jedoch allgemeines Einvernehmen darüber, dass der vertikale Luftraum am Punkt der Kármán-Linie endet.
Allgemeine Grenzbestimmungen werden von nationalen und lokalen Regierungen erlassen und können je nach Land und aktuellen politischen oder wirtschaftlichen Bedingungen variieren.
Grenzübergreifend arbeiten – Das Potenzial grenzüberschreitender Aktivitäten nutzen, um die Existenzsicherung in den Trockengebieten am Horn von Afrika zu verbessern.
Der grenzüberschreitende Wirtschaftsverkehr von Menschen (abgesehen von Entführungen) kann Massenpendeln zwischen Arbeitsplätzen und Wohnsiedlungen beinhalten.
Es kann Bewegungen ermöglichen und stoppen, sowohl über als auch entlang von Grenzen.
Viele Grenzregionen engagieren sich auch für die Förderung der interkulturellen Kommunikation und des Dialogs sowie für grenzüberschreitende Wirtschaftsentwicklungsstrategien.
Seit seiner Konzeption Mitte der 80er Jahre hat diese künstlerische Praxis zur Entwicklung von Fragen rund um Heimat, Grenzen, Überwachung, Identität, Rasse, ethnische Zugehörigkeit und nationale Herkunft(en) beigetragen.
Grenzen können Sprache, Kultur, soziale und wirtschaftliche Klasse, Religion und nationale Identität umfassen, sind aber nicht darauf beschränkt.
Diese Künstler sind oft selbst „Grenzgänger“.
Im Allgemeinen ist ein ländliches Gebiet oder eine ländliche Gegend ein geografisches Gebiet, das außerhalb von Städten liegt.
Typische ländliche Gebiete weisen eine geringe Bevölkerungsdichte und kleine Siedlungen auf.
In überwiegend städtischen Regionen leben weniger als 15 Prozent der Bevölkerung in ländlichen Gemeinden.
Ländliche nördliche Regionen sind vorwiegend ländliche Volkszählungsbezirke, die in jeder Provinz entweder vollständig oder größtenteils über den folgenden Parallellinien liegen: Neufundland und Labrador, 50.; Quebec 54.; Ontario, 54.; Manitoba, 53.; Saskatchewan, Alberta und British Columbia, 54.
Das U.S. Census Bureau, der Economic Research Service des USDA und das Office of Management and Budget (OMB) haben sich zusammengeschlossen, um bei der Definition ländlicher Gebiete zu helfen.
Das Agrargesetz von 2002 (P.L. 107–171, Sec.
Im Handbuch „Definitions of Rural: A Handbook for Health Policy Makers and Researchers“ heißt es: „Es wird allgemein davon ausgegangen, dass Einwohner von Großstädten einfachen Zugang zu den relativ konzentrierten Gesundheitsdiensten in den zentralen Gebieten des Landkreises haben.“
Dies wurde zur Definition des Begriffs „ländlich“ in der Goldsmith-Modifikation. "
Die Regierung von Präsident Emmanuel Macron hat 2019 einen Aktionsplan zugunsten ländlicher Gebiete mit dem Namen „Agenda Rural“ auf den Weg gebracht.
In Schottland wird eine andere Definition des Begriffs „ländlich“ verwendet.
Die RBI definiert ländliche Gebiete als Gebiete mit einer Bevölkerung von weniger als 49.000 Einwohnern (Städte der Klassen 3 bis 6).
Ländliche Gebiete in Pakistan, die in der Nähe von Städten liegen, werden als Vorstadtgebiete oder Vorstädte betrachtet.
Vororte verfügen möglicherweise über eine eigene politische oder rechtliche Zuständigkeit, insbesondere in den Vereinigten Staaten. Dies ist jedoch nicht immer der Fall, insbesondere im Vereinigten Königreich, wo die meisten Vororte innerhalb der Verwaltungsgrenzen von Städten liegen.
In anderen Ländern, beispielsweise in Marokko, Frankreich und einem Großteil der Vereinigten Staaten, bleiben viele Vororte eigenständige Gemeinden oder werden lokal als Teil eines größeren Ballungsraums wie eines Landkreises, Bezirks oder Bezirks verwaltet.
Die Begriffe innerer Vorort und äußerer Vorort werden verwendet, um zwischen den Gebieten mit höherer Dichte in der Nähe des Stadtzentrums (die in den meisten anderen Ländern nicht als „Vororte“ bezeichnet würden) und den Vororten mit geringerer Dichte am Stadtrand zu unterscheiden das Stadtgebiet.
In Neuseeland sind die meisten Vororte nicht gesetzlich definiert, was zu Verwirrung darüber führen kann, wo sie beginnen und enden dürfen.
Das Wort „suburbani“ wurde erstmals vom römischen Staatsmann Cicero in Bezug auf die großen Villen und Anwesen verwendet, die die wohlhabenden Patrizier Roms am Stadtrand errichteten.
Mitte des 19. Jahrhunderts entstanden rund um London die ersten großen Vorstädte, da die Stadt (damals die größte der Welt) immer überfüllter und unhygienischer wurde.
Die Linie erreichte Harrow im Jahr 1880.
Die Marketingabteilung der Met prägte 1915 den Begriff „Metro-Land“, als aus dem „Guide to the Extension Line“ der „Metro-Land-Guide“ zum Preis von 1 Tag wurde.
Teilweise war dies eine Reaktion auf die schockierende mangelnde Fitness vieler Rekruten während des Ersten Weltkriegs, die auf die schlechten Lebensbedingungen zurückzuführen war; Eine Überzeugung, die in einem Wohnungsplakat aus dieser Zeit zusammengefasst wurde: „Man kann nicht erwarten, eine A1-Bevölkerung aus C3-Häusern herauszuholen“ – und bezieht sich dabei auf die militärischen Fitnessklassifizierungen dieser Zeit.
Der Bericht enthält auch Vorschriften zu den erforderlichen Mindeststandards für den weiteren Vorstadtbau. Dazu gehörten Regelungen zur maximalen Wohndichte und deren Anordnung und sogar Empfehlungen zur idealen Anzahl von Schlafzimmern und anderen Räumen pro Haus.
Innerhalb nur eines Jahrzehnts vergrößerten sich die Vororte dramatisch.
Levittown entwickelte sich zu einem wichtigen Prototyp für Massenwohnungen.
Der Einkauf verschiedener Waren und Dienstleistungen an einem zentralen Ort, ohne an mehrere Orte reisen zu müssen, trug dazu bei, dass Einkaufszentren ein Bestandteil dieser neu gestalteten Vororte blieben, in denen die Bevölkerung boomte.
Das Highway Act von 1956 trug dazu bei, den Bau von 64.000 Kilometern im ganzen Land zu finanzieren, indem 26 Milliarden US-Dollar zur Verfügung standen, was dazu beitrug, viele weitere Einkaufszentren problemlos an diese anzubinden.
Einige Vororte hatten sich rund um Großstädte entwickelt, in denen es einen Schienenverkehr zu den Arbeitsplätzen in der Innenstadt gab.
Das Produkt war ein großer Immobilienboom.
Bei 16 Millionen anspruchsberechtigten Veteranen bot sich plötzlich die Möglichkeit, ein Haus zu kaufen.
Entwickler kauften unbebautes Land etwas außerhalb der Stadt, errichteten Reihenhäuser nach einigen wenigen Entwürfen und stellten Straßen und Versorgungseinrichtungen zur Verfügung, oder örtliche Beamte kämpften um den Bau von Schulen.
Veteranen könnten eines mit einer viel geringeren Anzahlung bekommen.
Das Wachstum der Vororte wurde durch die Entwicklung von Bebauungsgesetzen, Redlining und zahlreichen Innovationen im Transportwesen erleichtert.
Afroamerikaner und andere farbige Menschen blieben größtenteils in verfallenden städtischen Armutsgebieten konzentriert.
Nach dem Zweiten Weltkrieg löste die Verfügbarkeit von FHA-Darlehen einen Immobilienboom in amerikanischen Vororten aus.
Das Wirtschaftswachstum in den Vereinigten Staaten förderte die Suburbanisierung amerikanischer Städte, die massive Investitionen in die neue Infrastruktur und Wohnungen erforderte.
Eine alternative Strategie ist die bewusste Gestaltung „neuer Städte“ und der Schutz von Grüngürteln rund um die Städte.
Bundeszuschüsse für die Vorstadtentwicklung beschleunigten diesen Prozess ebenso wie die Redlining-Praxis von Banken und anderen Kreditinstituten.
Virginia Beach ist heute die größte Stadt in ganz Virginia und hat längst die Bevölkerungszahl der benachbarten Hauptstadt Norfolk übertroffen.
Ein größerer Prozentsatz an Weißen (sowohl Nicht-Hispanoamerikaner als auch in einigen Gebieten Hispanoamerikaner) und ein geringerer Prozentsatz an Bürgern anderer ethnischer Gruppen als in städtischen Gebieten.
Im Vergleich zu ländlichen Gebieten weisen Vororte in der Regel eine höhere Bevölkerungsdichte, einen höheren Lebensstandard, komplexere Straßensysteme, mehr Franchise-Läden und Restaurants sowie weniger Ackerland und Wildtiere auf.
Allerdings lebte im Jahr 2001 fast die Hälfte dieser Großstadtbevölkerung in Vierteln mit geringer Bevölkerungsdichte, wobei nur jeder Fünfte in einem typischen „städtischen“ Viertel lebte.
In ganz Kanada gibt es umfassende Pläne zur Eindämmung der Zersiedelung.
Der Großteil des jüngsten Bevölkerungswachstums in Kanadas drei größten Metropolregionen (Großraum Toronto, Großraum Montréal und Großraum Vancouver) fand in nicht zum Kerngebiet gehörenden Gemeinden statt.
Dies ist auf die Annexion und den großen geografischen Fußabdruck innerhalb der Stadtgrenzen zurückzuführen.
Bei der Volkszählung 2016 hatte die Stadt Calgary eine Bevölkerung von 1.239.220, während die Metropolregion Calgary eine Bevölkerung von 1.392.609 hatte, was darauf hindeutet, dass die überwiegende Mehrheit der Menschen in der Calgary CMA innerhalb der Stadtgrenzen lebte.
Im Vereinigten Königreich versucht die Regierung, in Teilen Südostenglands eine Mindestdichte für neu genehmigte Wohnbauprojekte vorzuschreiben.
Vororte gibt es in Guadalajara, Mexiko-Stadt, Monterrey und den meisten größeren Städten.
Mit dem zunehmenden Wachstum der Vororte der Mittel- und Oberschicht sind auch die Gebiete der Unterschicht-Besetzer gewachsen, vor allem „Lost Cities“ in Mexiko, Campamentos in Chile, Barriadas in Peru, Villa Miserias in Argentinien, Asentamientos in Guatemala und Favelas von Brasilien.
In einem beispielhaften Fall in Südafrika wurden RDP-Wohnungen gebaut.
In bestimmten Gebieten wie Klang, Subang Jaya und Petaling Jaya bilden Vororte den Kern dieser Orte.
Im Vorstadtsystem erfordern die meisten Fahrten von einem Stadtteil zum anderen, dass Autos auf eine Sammelstraße fahren, egal wie kurz oder lang die Strecke ist.
Wenn es auf einer Sammelstraße zu einem Verkehrsunfall kommt oder wenn Straßenbauarbeiten den Verkehr behindern, kann das gesamte Straßensystem unbrauchbar werden, bis die Blockade beseitigt ist.
Dies ermutigt dazu, mit dem Auto zu fahren, selbst wenn die Distanz nur mehrere Hundert Yards oder Meter beträgt (aufgrund des Straßennetzes können sich diese auf mehrere Meilen oder Kilometer belaufen).
Zusammengenommen stellen diese beiden Gruppen von Steuerzahlern eine weitgehend ungenutzte potenzielle Einnahmequelle dar, die Städte möglicherweise aggressiver ins Visier nehmen, insbesondere wenn sie Probleme haben.
Französische Lieder wie La Zone von Fréhel (1933), Aux Quatre Coins de la Banlieue von Damia (1936), Ma Banlieue von Reda Caire (1937) oder Banlieue von Robert Lamoureux (1953) erinnern seit den 1930er Jahren explizit an die Vororte von Paris .
Das französische Kino interessierte sich jedoch bald für urbane Veränderungen in den Vororten, mit Filmen wie „Mon oncle“ von Jacques Tati (1958), „L'Amour existe“ von Maurice Pialat (1961) oder „Zwei oder drei Dinge, die ich über sie weiß“ von Jean-Luc Godard (1967).
Das Lied „Little Boxes“ von Malvina Reynolds aus dem Jahr 1962 verspottet die Entwicklung der Vorstadt und ihre wahrgenommenen bürgerlichen und konformistischen Werte, während das Lied „Subdivisions“ der kanadischen Band Rush aus dem Jahr 1982 ebenso wie „Rockin' the Suburbs“ von Ben Folds die Vorstadt thematisiert.
Over the Hedge ist ein syndizierter Comicstrip, der von Michael Fry und T. Lewis geschrieben und gezeichnet wurde.
Britische Fernsehserien wie „The Good Life“, „Butterflies“ und „The Fall and Rise of Reginald Perrin“ haben die Vorstädte als gepflegt, aber unerbittlich langweilig dargestellt und ihre Bewohner als entweder übermäßig konform oder dazu geneigt, verrückt zu werden.
Ein Dorf ist eine gruppierte menschliche Siedlung oder Gemeinde, größer als ein Weiler, aber kleiner als eine Stadt (obwohl das Wort häufig zur Beschreibung sowohl von Weilern als auch von kleineren Städten verwendet wird), mit einer Bevölkerung, die typischerweise zwischen einigen Hundert und einigen Tausend liegt.
Dies ermöglichte auch die Spezialisierung von Arbeit und Handwerk sowie die Entwicklung vieler Gewerbe.
Die Größe dieser Dörfer variiert erheblich.
Desa liegen im Allgemeinen in ländlichen Gebieten, während Kelurahan im Allgemeinen städtische Stadtteile sind.
Ein Desa oder Kelurahan ist die Unterteilung eines Kecamatan (Unterbezirks) und wiederum die Unterteilung eines Kabupaten (Bezirks) oder einer Kota (Stadt).
In Malaysia wird ein Kampung als Ortschaft mit 10.000 oder weniger Einwohnern definiert.
Alle Muslime im malaiischen oder indonesischen Dorf möchten, dass für sie gebetet wird und dass sie im Jenseits Gottes Segen erhalten.
Früher gab es auf dem Festland von Singapur viele Kampung-Dörfer, aber moderne Entwicklungen und rasche Urbanisierungsarbeiten haben dazu geführt, dass sie dem Erdboden gleichgemacht wurden. Kampong Lorong Buangkok ist das letzte erhaltene Dorf auf dem Festland des Landes.
Vietnams Dorf ist das typische Symbol der asiatischen Agrarproduktion.
In Slowenien wird das Wort Selo für sehr kleine Dörfer (weniger als 100 Einwohner) und in Dialekten verwendet; Das slowenische Wort „vas“ wird in ganz Slowenien verwendet.
Es könnte sich auf ein Sanskrit-Wort wie das afghanische Wort deh und das indonesische Wort desa beziehen.
Ungefähr 46 % aller Migranten haben ihren Wohnsitz von einer Stadt in eine andere gewechselt.
Die unterste Verwaltungseinheit des Russischen Reiches, ein Wolost oder sein sowjetischer oder moderner russischer Nachfolger, ein Selsowjet, hatte seinen Hauptsitz typischerweise in einem Selo und umfasste einige benachbarte Dörfer.
Während die Bauern in Zentralrussland in einem Dorf rund um das Gut des Gutsherrn lebten, lebte eine Kosakenfamilie oft auf ihrem eigenen Bauernhof namens Khutor.
Es gibt jedoch noch einen anderen kleineren Siedlungstyp, der im Ukrainischen als Selysche (селище) bezeichnet wird.
Sie stellen eine Art kleiner ländlicher Ort dar, der einst ein Khutir, eine Fischersiedlung oder eine Datscha gewesen sein könnte.
Im Zusammenhang mit urbanisierten Siedlungen werden Unklarheiten jedoch oft dadurch vermieden, dass man sie stattdessen mit der dreibuchstabigen Abkürzung smt bezeichnet.
Wirklich populär wurden sie während der Stolypin-Reform zu Beginn des 20. Jahrhunderts.
Größere Dörfer können je nach Region auch als Flecken oder Markt bezeichnet werden.
Beispielsweise liegen die Dörfer in Gebieten wie den Lincolnshire Wolds häufig entlang der Quelllinie auf halber Höhe der Hügel und sind ursprünglich als Quellliniensiedlungen mit den ursprünglichen offenen Feldsystemen rund um das Dorf entstanden.
Einige Dörfer sind verschwunden (z. B. verlassene mittelalterliche Dörfer), manchmal blieben eine Kirche oder ein Herrenhaus und manchmal nichts als Unebenheiten auf den Feldern zurück.
Andere Dörfer sind gewachsen und haben sich zusammengeschlossen und bilden oft Knotenpunkte innerhalb der allgemeinen Vorstadtmasse – wie Hampstead, London und Didsbury in Manchester.
Weit entfernt von der Hektik des modernen Lebens wird es als ruhig und harmonisch, wenn auch ein wenig nach innen gerichtet, dargestellt.
Diese (wie Murton, County Durham) entstanden aus Weilern, als der Untergang einer Zeche zu Beginn des 20. Jahrhunderts zu einem schnellen Bevölkerungswachstum führte und die Zechenbesitzer neue Wohnungen, Geschäfte, Pubs und Kirchen bauten.
Maltby wurde unter der Schirmherrschaft der Sheepbridge Coal and Iron Company erbaut und umfasste großzügige Freiflächen und Vorkehrungen für Gärten.
Das typische Dorf hatte eine Kneipe oder ein Gasthaus, Geschäfte und eine Schmiede.
Allerdings gibt es in einigen Zivilgemeinden weder einen funktionierenden Gemeinde-, Stadt- oder Stadtrat noch eine funktionierende Gemeindeversammlung.
In Schottland ist das Äquivalent ebenfalls ein Gemeinderat, der jedoch, obwohl es sich um gesetzliche Körperschaften handelt, über keine Exekutivbefugnisse verfügt.
Der Bezirk Danniyeh besteht aus sechsunddreißig kleinen Dörfern, darunter Almrah, Kfirchlan, Kfirhbab, Hakel al Azimah, Siir, Bakhoun, Miryata, Assoun, Sfiiri, Kharnoub, Katteen, Kfirhabou, Zghartegrein und Ein Qibil.
Dinniyeh verfügt über eine ausgezeichnete ökologische Umgebung mit Wäldern, Obstgärten und Hainen.
Dörfer im Süden Syriens (Hauran, Jabal al-Druze), im Nordosten (die syrische Insel) und im Einzugsgebiet des Orontes sind größtenteils von der Landwirtschaft abhängig, vor allem von Getreide, Gemüse und Obst.
Mittelmeerstädte in Syrien wie Tartus und Latakia haben ähnliche Dorftypen.
Jede Urbanisation ist ein „Pueblo“, es sei denn, sie wird per Dekret in die nächsthöhere Kategorie erhoben.
Dies ist jedoch eine allgemeine Regel; In vielen Bundesstaaten gibt es Dörfer, die um eine Größenordnung größer sind als die kleinsten Städte des Bundesstaates.
In einigen Fällen kann das Dorf eng mit der Stadt oder dem Township verbunden sein. In diesem Fall können die beiden eine konsolidierte Regierung haben.
Hempstead, das größte Dorf, hat 55.000 Einwohner; Damit ist es bevölkerungsreicher als einige Städte des Bundesstaates.
Das Dorf Arlington Heights, Illinois, hatte bei der Volkszählung 2010 75.101 Einwohner.
Dörfer können Land in mehreren Townships und sogar mehreren Landkreisen umfassen.
Das größte Dorf ist Menomonee Falls mit über 32.000 Einwohnern.
In Maryland kann ein Ort mit der Bezeichnung „Village of …“ entweder eine eingemeindete Stadt oder ein Sondersteuerbezirk sein.
Damals hatten traditionelle Herrscher die absolute Macht in ihren Verwaltungsgebieten.
Jedes Hausa-Dorf wurde von Magaji (Dorfvorsteher) regiert, der auf Stadtebene seinem Hakimi (Bürgermeister) verantwortlich war.
Sie haben Lehmhäuser mit Strohdächern, obwohl Zinkdächer, wie in den meisten Dörfern im Norden, immer häufiger anzutreffen sind.
Andere haben das Glück, Brunnen zu Fuß erreichen zu können.
Ein Atlas ist eine Sammlung von Karten; Dabei handelt es sich typischerweise um ein Bündel von Karten der Erde oder einer Region der Erde.
Dieser Titel liefert Mercators Definition des Wortes als Beschreibung der Schöpfung und Form des gesamten Universums, nicht einfach als eine Sammlung von Karten.
Ein Schreibtischatlas ist ähnlich einem Nachschlagewerk erstellt.
In der Kartographie verbindet eine Konturlinie (oft einfach „Kontur“ genannt) Punkte gleicher Höhe (Höhe) über einem bestimmten Niveau, beispielsweise dem mittleren Meeresspiegel.
Der Gradient der Funktion steht immer senkrecht zu den Höhenlinien.
Konturlinien sind gekrümmte, gerade oder eine Mischung aus beiden Linien auf einer Karte, die den Schnittpunkt einer realen oder hypothetischen Oberfläche mit einer oder mehreren horizontalen Ebenen beschreiben.
Im Jahr 1701 verwendete Edmond Halley solche Linien (Isogone) in einer Karte der magnetischen Variation.
Im Jahr 1791 verwendete eine Karte von Frankreich von J. L. Dupain-Triel Höhenlinien in Abständen von 20 Metern, Schraffuren, Punkthöhen und einen Vertikalschnitt.
Isobathen wurden auf Seekarten erst ab 1834 in Russland und ab 1838 in Großbritannien routinemäßig verwendet.
Noch 1944 bevorzugte John K. Wright das Isogramm, doch es erlangte nie eine breite Verbreitung.
Trotz der Versuche, einen einzigen Standard auszuwählen, haben alle diese Alternativen bis heute überlebt.
Wetterstationen sind selten genau an einer Höhenlinie positioniert (wenn dies der Fall ist, weist dies auf einen Messwert hin, der genau dem Wert der Höhenlinie entspricht).
In der Meteorologie werden die angezeigten Luftdrücke auf Meereshöhe reduziert, nicht auf die Oberflächendrücke an den Kartenstandorten.
Isallobare sind Linien, die Punkte gleicher Druckänderung während eines bestimmten Zeitintervalls verbinden.
Isallobare Gradienten sind wichtige Komponenten des Windes, da sie den geostrophischen Wind verstärken oder verringern.
Eine Isotherme bei 0 °C wird als Gefrierpunkt bezeichnet.
Anhand dieser Konturen kann ein Gefühl für das allgemeine Gelände ermittelt werden.
In der Kartographie ist das Höhenlinienintervall der Höhenunterschied zwischen benachbarten Höhenlinien.
Das Zusammentreffen zweier oder mehrerer Höhenlinien weist auf eine Klippe hin.
Normalerweise sind Konturintervalle auf der gesamten Karte einheitlich, es gibt jedoch Ausnahmen.
Ob das Überqueren einer Äquipotentiallinie einen Anstieg oder Abfall des Potentials darstellt, lässt sich aus den Beschriftungen auf den Ladungen ableiten.
Saure Niederschläge werden auf Karten mit Isoplaten angezeigt.
Höhenlinien werden auch in der Wirtschaftswissenschaft zur Darstellung nicht-geografischer Informationen verwendet.
Solche Isolinien eignen sich zur Darstellung von mehr als zwei Dimensionen (oder Mengen) in zweidimensionalen Diagrammen.
Bei der Interpretation von Radarbildern ist ein Isodop eine Linie mit gleicher Dopplergeschwindigkeit und ein Isoecho eine Linie mit gleichem Radarreflexionsvermögen.
Die Linienfarbe ist die Auswahl einer beliebigen Anzahl von Pigmenten, die zur Anzeige passen.
Der Linientyp bezieht sich darauf, ob die grundlegende Konturlinie durchgezogen, gestrichelt, gepunktet oder in einem anderen Muster unterbrochen ist, um den gewünschten Effekt zu erzielen.
Numerische Markierung ist die Art und Weise, die arithmetischen Werte von Höhenlinien anzugeben.
Wenn die Konturlinien nicht numerisch beschriftet sind und benachbarte Linien denselben Stil (mit derselben Stärke, Farbe und demselben Typ) haben, kann die Richtung des Farbverlaufs nicht allein anhand der Konturlinien bestimmt werden.
Eine ordnungsgemäß beschriftete Höhenlinienkarte hilft dem Leser, die Form des Geländes schnell zu erkennen.
Anschließend werden die Koordinaten anderer Orte vom nächstgelegenen Kontrollpunkt aus durch Vermessung gemessen.
Dieses Phänomen wird als Datumsverschiebung bezeichnet.
Ehrgeizigere Unternehmungen wie der Geodätische Struve-Bogen durch Osteuropa (1816–1855) und die Große Trigonometrische Vermessung Indiens (1802–1871) dauerten viel länger, führten jedoch zu genaueren Schätzungen der Form des Erdellipsoids.
Eine ungefähre Definition des Meeresspiegels ist das Datum WGS 84, ein Ellipsoid, während eine genauere Definition das Earth Gravitational Model 2008 (EGM2008) ist, das mindestens 2.159 sphärische Harmonische verwendet.
Bei uneingeschränkter Verwendung bezieht sich der Begriff Breitengrad auf die geodätische Breite.
Die Datumsverschiebung zwischen zwei bestimmten Datumsangaben kann von einem Ort zum anderen innerhalb eines Landes oder einer Region variieren und zwischen null und Hunderten von Metern (oder mehreren Kilometern für einige abgelegene Inseln) liegen.
In Sydney gibt es beispielsweise einen Unterschied von 200 Metern (700 Fuß) zwischen den in GDA (basierend auf dem globalen Standard WGS 84) und AGD (für die meisten lokalen Karten verwendet) konfigurierten GPS-Koordinaten, was für einige Anwendungen, wie z als Vermessungs- oder Standortstandort für Gerätetauchen.
Da Referenzdaten unterschiedliche Radien und unterschiedliche Mittelpunkte haben können, kann ein bestimmter Punkt auf der Erde je nach dem für die Messung verwendeten Datum erheblich unterschiedliche Koordinaten haben.
Die in Nordamerika am häufigsten verwendeten Referenzdaten sind NAD27, NAD83 und WGS 84.
Dieses als NAD 83 bezeichnete Datum basiert auf der Anpassung von 250.000 Punkten, einschließlich 600 Satelliten-Doppler-Stationen, die das System auf einen geozentrischen Ursprung beschränken.“
Es ist der vom US-Verteidigungsministerium (DoD) verwendete Referenzrahmen und wird von der National Geospatial-Intelligence Agency (NGA) (früher Defense Mapping Agency, dann National Imagery and Mapping Agency) definiert.
Es wurde ab dem 23. Januar 1987 als Referenzrahmen für die Übertragung von GPS-Ephemeriden (Umlaufbahnen) verwendet.
Am 28. Juni 1994 wurde es zum Referenzrahmen für Rundfunkumlaufbahnen.
WGS 84 (G873) wurde am 29. Januar 1997 als Referenzrahmen für Rundfunkumlaufbahnen übernommen.
WGS 84 ist das Standard-Standarddatum für Koordinaten, die in Freizeit- und kommerziellen GPS-Geräten gespeichert werden.
Beispielsweise nimmt der Längenunterschied zwischen einem Punkt am Äquator in Uganda auf der Afrikanischen Platte und einem Punkt am Äquator in Ecuador auf der Südamerikanischen Platte um etwa 0,0014 Bogensekunden pro Jahr zu.
Die meisten Kartierungen, beispielsweise innerhalb eines einzelnen Landes, umfassen keine Platten.
Ptolemaios schrieb ihm die vollständige Übernahme von Längen- und Breitengraden zu, anstatt den Breitengrad anhand der Länge des Mittsommertages zu messen.
Die mathematische Kartographie wurde in Europa wieder aufgenommen, nachdem Maximus Planudes kurz vor 1300 den Text des Ptolemäus wiedererlangt hatte. Der Text wurde um 1407 in Florenz von Jacobus Angelus ins Lateinische übersetzt.
Anschließend wählen sie die am besten geeignete Abbildung des sphärischen Koordinatensystems auf dieses Ellipsoid aus, das als terrestrisches Referenzsystem oder geodätisches Datum bezeichnet wird.
φ oder Phi) eines Punktes auf der Erdoberfläche ist der Winkel zwischen der Äquatorialebene und der geraden Linie, die durch diesen Punkt und durch (oder in der Nähe) des Erdmittelpunkts verläuft.
Alle Meridiane sind Hälften großer Ellipsen (oft Großkreise genannt), die am Nord- und Südpol zusammenlaufen.
Der antipodale Meridian von Greenwich beträgt sowohl 180°W als auch 180°E. Dies ist nicht mit der Internationalen Datumsgrenze zu verwechseln, die aus politischen und praktischen Gründen an mehreren Stellen davon abweicht, unter anderem zwischen dem äußersten Osten Russlands und den äußerst westlichen Aleuten-Inseln.
Koordinaten auf einer Karte werden normalerweise als Nord-N- und Ost-E-Versatz relativ zu einem bestimmten Ursprung angegeben.
In der Geographie ist der Breitengrad eine geografische Koordinate, die die Nord-Süd-Position eines Punktes auf der Erdoberfläche angibt.
Der Breitengrad wird zusammen mit dem Längengrad verwendet, um die genaue Position von Merkmalen auf der Erdoberfläche anzugeben.
Der zweite Schritt besteht darin, das Geoid durch eine mathematisch einfachere Referenzfläche anzunähern.
Linien konstanter Breiten- und Längengrade bilden zusammen ein Strichnetz auf der Referenzfläche.
Da es viele verschiedene Referenzellipsoide gibt, ist die genaue Breite eines Merkmals auf der Oberfläche nicht eindeutig: Dies wird in der ISO-Norm betont, die besagt, dass „ohne die vollständige Spezifikation des Koordinatenreferenzsystems Koordinaten (d. h. Breiten- und Längengrade) sind im besten Fall mehrdeutig und im schlimmsten Fall bedeutungslos.“
Die Ebene durch den Mittelpunkt der Erde und senkrecht zur Rotationsachse schneidet die Oberfläche auf einem Großkreis, der Äquator genannt wird.
Die zeitliche Variation wird im Artikel über die axiale Neigung ausführlicher besprochen.
Zur Sonnenwende im Juni ist die Situation umgekehrt, wenn die Sonne über dem Wendekreis des Krebses steht.
Da der Breitengrad in Bezug auf ein Ellipsoid definiert wird, ist die Position eines bestimmten Punktes auf jedem Ellipsoid unterschiedlich: Man kann den Breiten- und Längengrad eines geografischen Merkmals nicht genau angeben, ohne das verwendete Ellipsoid anzugeben.
Der geografische Breitengrad muss mit Vorsicht genutzt werden.
Die Auswertung des Meridianabstandsintegrals ist für viele Studien zur Geodäsie und Kartenprojektion von zentraler Bedeutung.
Es gibt zwei Vorgehensweisen.
Bei der Konvertierung von isometrisch oder konform in geodätisch ergeben zwei Iterationen von Newton-Raphson eine Genauigkeit mit doppelter Genauigkeit.
Die im Diagramm angezeigten Unterschiede werden in Bogenminuten angegeben.
Die Transformation zwischen geodätischen und kartesischen Koordinaten finden Sie unter Geografische Koordinatenkonvertierung.
Im Allgemeinen stimmt die wahre Vertikale an einem Punkt auf der Oberfläche weder mit der Normalen zum Referenzellipsoid noch mit der Normalen zum Geoid genau überein.
Der Längengrad ist eine geografische Koordinate, die die Ost-West-Position eines Punktes auf der Erdoberfläche oder der Oberfläche eines Himmelskörpers angibt.
Der Nullmeridian, der in der Nähe des Royal Observatory in Greenwich, England, verläuft, wird per Konvention als 0° Längengrad definiert.
Die Ortszeit (z. B. vom Sonnenstand aus) variiert mit dem Längengrad, wobei ein Unterschied von 15° Längengrad einem Unterschied von einer Stunde in der Ortszeit entspricht.
Das Prinzip ist einfach, aber in der Praxis dauerte es Jahrhunderte, eine zuverlässige Methode zur Bestimmung des Längengrads zu finden, und erforderte die Anstrengung einiger der größten wissenschaftlichen Köpfe.
Sein Nullmeridian verlief durch Alexandria.
Im Jahr 1910 veröffentlichte das Journal einen Artikel von Ulysses G. Weatherly (1865-1940), der die Vorherrschaft der Weißen und Rassentrennung zum Schutz der Rassenreinheit forderte.
In seiner Arbeit behauptete er, dass soziale Klasse, Kolonialismus und Kapitalismus die Vorstellungen über Rasse und Rassenkategorien prägten.
Im Jahr 1978 argumentierte William Julius Wilson (1935–), dass Rasse und Rassenklassifizierungssysteme an Bedeutung verloren und stattdessen die soziale Klasse das, was Soziologen zuvor als Rasse verstanden hatten, genauer beschrieb.
Eduardo Bonilla-Silva, Soziologieprofessor an der Duke University, bemerkt: „Ich behaupte, dass Rassismus mehr als alles andere eine Frage der Gruppenmacht ist; es geht darum, dass eine dominante Rassengruppe (Weiße) danach strebt, ihre systemischen Vorteile aufrechtzuerhalten, und dass Minderheiten kämpfen.“ den Rassenstatus quo zu untergraben.
Im klinischen Umfeld wird die Rasse manchmal bei der Diagnose und Behandlung von Erkrankungen berücksichtigt.
Unter biomedizinischen Forschern gibt es eine aktive Debatte über die Bedeutung und Wichtigkeit der Rasse in ihrer Forschung.
Mitglieder des letzteren Lagers stützen ihre Argumente oft auf das Potenzial, eine genombasierte personalisierte Medizin zu entwickeln.
Sie argumentieren, dass eine Überbetonung genetischer Beiträge zu gesundheitlichen Ungleichheiten verschiedene Risiken birgt, etwa die Verstärkung von Stereotypen, die Förderung von Rassismus oder die Ignorierung des Beitrags nichtgenetischer Faktoren zu gesundheitlichen Ungleichheiten.
„IC“ steht für „Identification Code“; diese Elemente werden auch als Phoenix-Klassifikationen bezeichnet.
In vielen Ländern, beispielsweise in Frankreich, ist es dem Staat gesetzlich verboten, Daten auf der Grundlage der Rasse zu speichern, was dazu führt, dass die Polizei häufig Fahndungsanzeigen an die Öffentlichkeit herausgibt, die Hinweise wie „dunkle Hautfarbe“ usw. enthalten.
Viele betrachten faktische Racial Profiling als Beispiel für institutionellen Rassismus in der Strafverfolgung.
Masseninhaftierung ist auch „das größere Netz von Gesetzen, Regeln, Richtlinien und Bräuchen, das die als Kriminellen bezeichneten Kriminellen sowohl innerhalb als auch außerhalb des Gefängnisses kontrolliert“.
Viele Forschungsergebnisse scheinen darin übereinzustimmen, dass der Einfluss der Rasse des Opfers auf die Entscheidung über die Festnahme von IPV möglicherweise eine rassistische Voreingenommenheit zugunsten weißer Opfer beinhalten könnte.
Einige Studien haben berichtet, dass Rassen mit bestimmten Methoden, wie beispielsweise der von Giles und Elliot entwickelten, mit einem hohen Maß an Genauigkeit identifiziert werden können.
Die Studie kam zu dem Schluss, dass „die Aufteilung der genetischen Vielfalt in der Hautfarbe untypisch ist und nicht für Klassifizierungszwecke herangezogen werden kann.“
Kulturanthropologie ist ein Zweig der Anthropologie, der sich auf die Erforschung kultureller Unterschiede zwischen Menschen konzentriert.
Bei der Auseinandersetzung mit dieser Frage teilten Ethnologen im 19. Jahrhundert zwei Denkschulen.
Einige derjenigen, die „unabhängige Erfindungen“ befürworteten, wie Lewis Henry Morgan, gingen außerdem davon aus, dass Ähnlichkeiten bedeuteten, dass verschiedene Gruppen dieselben Stadien der kulturellen Evolution durchlaufen hatten (siehe auch klassischer sozialer Evolutionismus).
Morgan glaubte wie andere Sozialevolutionäre des 19. Jahrhunderts, dass es einen mehr oder weniger geordneten Übergang vom Primitiven zum Zivilisierten gab.
Obwohl Ethnologen des 19. Jahrhunderts „Diffusion“ und „unabhängige Erfindung“ als sich gegenseitig ausschließende und konkurrierende Theorien betrachteten, kamen die meisten Ethnographen schnell zu einem Konsens darüber, dass beide Prozesse stattfinden und dass beide interkulturelle Ähnlichkeiten plausibel erklären können.
Boas formulierte die Idee erstmals im Jahr 1887: „... Zivilisation ist nichts Absolutes, sondern ... ist relativ, und ... unsere Ideen und Vorstellungen sind nur wahr, soweit unsere Zivilisation reicht.“
Der Kulturrelativismus beinhaltet spezifische erkenntnistheoretische und methodologische Ansprüche.
Der Kulturrelativismus war teilweise eine Reaktion auf den westlichen Ethnozentrismus.
Dieses Kulturverständnis stellt Anthropologen vor zwei Probleme: erstens, wie man den unbewussten Bindungen der eigenen Kultur entkommt, die unweigerlich unsere Wahrnehmung und Reaktionen auf die Welt beeinflussen, und zweitens, wie man einer unbekannten Kultur einen Sinn gibt.
Eine dieser Methoden ist die Ethnographie: Sie plädierte im Wesentlichen dafür, über einen längeren Zeitraum mit Menschen einer anderen Kultur zusammenzuleben, damit sie die lokale Sprache lernen und sich zumindest teilweise in diese Kultur integrieren konnten.
Sein Ansatz war empirisch, skeptisch gegenüber Überverallgemeinerungen und vermied Versuche, universelle Gesetze aufzustellen.
Er glaubte, dass jede Kultur in ihrer Besonderheit untersucht werden müsse, und argumentierte, dass interkulturelle Verallgemeinerungen, wie sie in den Naturwissenschaften gemacht würden, nicht möglich seien.
Zu seiner ersten Generation von Studenten gehörten Alfred Kroeber, Robert Lowie, Edward Sapir und Ruth Benedict, die jeweils sehr detaillierte Studien über die indigenen nordamerikanischen Kulturen verfassten.
Die Veröffentlichung von Alfred Kroebers Lehrbuch Anthropology (1923) markierte einen Wendepunkt in der amerikanischen Anthropologie.
Beeinflusst von psychoanalytischen Psychologen wie Sigmund Freud und Carl Jung versuchten diese Autoren zu verstehen, wie individuelle Persönlichkeiten durch die umfassenderen kulturellen und sozialen Kräfte, in denen sie aufwuchsen, geformt wurden.
Die von Karl Polanyi beeinflusste und von Marshall Sahlins und George Dalton praktizierte Wirtschaftsanthropologie forderte die neoklassische Standardökonomie heraus, kulturelle und soziale Faktoren zu berücksichtigen, und wandte die Marxsche Analyse in anthropologische Studien ein.
Zeitgemäß wurde ein Großteil der Anthropologie durch den algerischen Unabhängigkeitskrieg und die Opposition gegen den Vietnamkrieg politisiert; Der Marxismus wurde zu einem immer beliebteren theoretischen Ansatz in der Disziplin.
In den 1980er Jahren beschäftigten sich Bücher wie „Anthropology“ und „The Colonial Encounter“ mit den Verbindungen der Anthropologie zur kolonialen Ungleichheit, während die immense Popularität von Theoretikern wie Antonio Gramsci und Michel Foucault Fragen von Macht und Hegemonie ins Rampenlicht rückte.
Diese Interpretationen müssen dann auf ihre Urheber zurückgespiegelt werden und ihre Angemessenheit als Übersetzung muss in wiederholter Weise verfeinert werden, ein Prozess, der als hermeneutischer Zirkel bezeichnet wird.
David Schnieders kulturelle Analyse der amerikanischen Verwandtschaft hat sich als ebenso einflussreich erwiesen.
Die Methode hat ihren Ursprung in der Feldforschung von Sozialanthropologen, insbesondere Bronislaw Malinowski in Großbritannien, den Schülern von Franz Boas in den USA und in der späteren Stadtforschung der Chicago School of Sociology.
Walnut Creek, Kalifornien: AltaMira Press.
Um Verbindungen herzustellen, die letztendlich zu einem besseren Verständnis des kulturellen Kontexts einer Situation führen, muss ein Anthropologe offen dafür sein, Teil der Gruppe zu werden, und bereit sein, sinnvolle Beziehungen zu ihren Mitgliedern aufzubauen.
Bevor mit der teilnehmenden Beobachtung begonnen werden kann, muss ein Anthropologe sowohl einen Ort als auch einen Studienschwerpunkt auswählen.
Dies ermöglicht es dem Anthropologen, sich besser in der Gemeinschaft zu etablieren.
Der Großteil der teilnehmenden Beobachtungen basiert auf Gesprächen.
In einigen Fällen wenden sich Ethnographen auch der strukturierten Beobachtung zu, bei der die Beobachtungen eines Anthropologen von einer Reihe spezifischer Fragen geleitet werden, die er oder sie zu beantworten versucht.
Dies trägt dazu bei, die Untersuchungsmethode zu standardisieren, wenn ethnografische Daten über mehrere Gruppen hinweg verglichen werden oder zur Erfüllung eines bestimmten Zwecks benötigt werden, beispielsweise für die Forschung im Hinblick auf eine politische Entscheidung der Regierung.
Wer der Ethnograph ist, hat viel damit zu tun, was er oder sie letztendlich über eine Kultur schreiben wird, denn jeder Forscher wird von seiner oder ihrer eigenen Perspektive beeinflusst.
Allerdings waren diese Ansätze im Allgemeinen nicht erfolgreich, und moderne Ethnographen entscheiden sich oft dafür, ihre persönlichen Erfahrungen und mögliche Vorurteile in ihre Schriften einzubeziehen.
Eine Ethnographie ist eine Schrift über ein Volk an einem bestimmten Ort und zu einer bestimmten Zeit.
Eine typische Ethnographie umfasst auch Informationen über physische Geographie, Klima und Lebensraum.
Boas‘ Schüler wie Alfred L. Kroeber, Ruth Benedict und Margaret Mead stützten sich auf seine Auffassung von Kultur und Kulturrelativismus, um die Kulturanthropologie in den Vereinigten Staaten zu entwickeln.
Heutzutage befassen sich soziokulturelle Anthropologen mit all diesen Elementen.
Amerikanische „Kulturanthropologen“ konzentrierten sich auf die Art und Weise, wie Menschen ihre Sicht auf sich selbst und ihre Welt zum Ausdruck brachten, insbesondere in symbolischen Formen wie Kunst und Mythen.
Monogamie zum Beispiel wird häufig als universelles menschliches Merkmal angepriesen, doch vergleichende Studien zeigen, dass dies nicht der Fall ist.
Durch diese Methodik können größere Erkenntnisse bei der Untersuchung der Auswirkungen von Weltsystemen auf lokale und globale Gemeinschaften gewonnen werden.
Beispielsweise kann eine Ethnographie mit mehreren Standorten einem „Ding“, beispielsweise einer bestimmten Ware, folgen, während es durch die Netzwerke des globalen Kapitalismus transportiert wird.
Ein Beispiel für eine multizentrische Ethnographie ist die Arbeit von Nancy Scheper-Hughes über den internationalen Schwarzmarkt für den Handel mit menschlichen Organen.
Die Forschung im Bereich der Verwandtschaftsforschung erstreckt sich häufig über verschiedene anthropologische Teilbereiche, darunter medizinische, feministische und öffentliche Anthropologie.
Das ist die Matrix, in die menschliche Kinder in den allermeisten Fällen hineingeboren werden, und ihre ersten Worte sind oft Verwandtschaftsbegriffe.
Es gibt starke Unterschiede zwischen den Gemeinschaften in Bezug auf die Praxis und den Wert der Ehe, was viel Raum für anthropologische Feldforschung lässt.
Die in den meisten Kulturen übliche Ehepraxis ist jedoch die Monogamie, bei der eine Frau mit einem Mann verheiratet ist.
Ähnliche grundlegende Unterschiede gibt es auch beim Akt der Zeugung.
Der Wandel lässt sich bis in die 1960er Jahre zurückverfolgen, als Edmund Leach, Rodney Neeham, David Schneider und andere die Grundprinzipien der Verwandtschaft neu bewerteten.
Dieser Wandel wurde durch das Aufkommen des Feminismus der zweiten Welle in den frühen 1970er Jahren noch weiter vorangetrieben, der Ideen von ehelicher Unterdrückung, sexueller Autonomie und häuslicher Unterordnung einführte.
Zu dieser Zeit entstand der „Dritte-Welt-Feminismus“, eine Bewegung, die argumentierte, dass Verwandtschaftsstudien die Geschlechterverhältnisse in Entwicklungsländern nicht isoliert untersuchen könnten und auch rassische und wirtschaftliche Nuancen berücksichtigen müssten.
In Jamaika wird die Ehe als Institution häufig an die Stelle einer Reihe von Partnern gesetzt, da sich arme Frauen in einem Klima wirtschaftlicher Instabilität nicht auf regelmäßige finanzielle Beiträge verlassen können.
Mit dieser Technologie sind Fragen der Verwandtschaft über den Unterschied zwischen biologischer und genetischer Verwandtschaft aufgetaucht, da Leihmütter eine biologische Umgebung für den Embryo bereitstellen können, während die genetischen Bindungen bei einem Dritten bestehen bleiben.
Es gab auch Probleme mit dem Reproduktionstourismus und der Kommerzialisierung des Körpers, da Einzelpersonen durch hormonelle Stimulation und Eizellentnahme, bei denen es sich um potenziell schädliche Verfahren handelt, wirtschaftliche Sicherheit suchen.
Ein Kritikpunkt ist, dass der Rahmen der Verwandtschaftsstudien zu Beginn viel zu strukturiert und formelhaft war und auf einer dichten Sprache und strengen Regeln beruhte.
Ein Großteil dieser Entwicklung ist auf die Zunahme von Anthropologen zurückzuführen, die außerhalb der Wissenschaft arbeiten, und auf die zunehmende Bedeutung der Globalisierung sowohl in Institutionen als auch im Bereich der Anthropologie.
Die beiden Arten von Institutionen, die im Bereich der Anthropologie definiert werden, sind Gesamtinstitutionen und soziale Institutionen.
Die Anthropologie der Institutionen kann Gewerkschaften, Unternehmen von Kleinbetrieben bis hin zu Konzernen, Regierungen, medizinische Organisationen, Bildung, Gefängnisse und Finanzinstitute analysieren.
Institutionelle Anthropologen können die Beziehung zwischen Organisationen oder zwischen einer Organisation und anderen Teilen der Gesellschaft untersuchen.
Genauer gesagt können Anthropologen bestimmte Ereignisse innerhalb einer Institution analysieren, semiotische Untersuchungen durchführen oder die Mechanismen analysieren, durch die Wissen und Kultur organisiert und verbreitet werden.
Diese neue Ära würde viele neue technologische Entwicklungen mit sich bringen, beispielsweise die mechanische Aufzeichnung.
Current Anthropology 43 (Ergänzung): S5-17. Schieffelin, Bambi B. 2006.
Woolard stellt in ihrem Überblick über „Code Switching“ oder die systematische Praxis des Wechsels sprachlicher Varietäten innerhalb eines Gesprächs oder sogar einer einzelnen Äußerung fest, dass die zugrunde liegende Frage, die Anthropologen dieser Praxis stellen – Warum tun sie das? – eine vorherrschende sprachliche Ideologie widerspiegelt .
Andere Linguisten haben in den Bereichen Sprachkontakt, Sprachgefährdung und „Englisch als Weltsprache“ geforscht.
Die Arbeit von Joel Kuipers entwickelt dieses Thema anhand der Insel Sumba, Indonesien.
Tatsächlich ist er der Ansicht, dass die Idee des exemplarischen Zentrums eine der drei wichtigsten Erkenntnisse der Sprachanthropologie ist.
Daher werden diese Sprachen nach einigen Generationen möglicherweise nicht mehr gesprochen.
Um den Best Practices der Dokumentation zu folgen, sollten diese Aufzeichnungen klar mit Anmerkungen versehen und in einem Archiv sicher aufbewahrt werden.
Bei der Sprachrevitalisierung handelt es sich um die Praxis, eine Sprache wieder in den allgemeinen Gebrauch zu bringen.
Der Kurs zielt darauf ab, indigene und nicht-indigene Studenten über die Sprache und Kultur der Lenape aufzuklären.
Die Ermutigung derjenigen, die die Sprache bereits beherrschen, sie zu verwenden, die Erweiterung der Anwendungsbereiche und die Steigerung des Gesamtprestiges der Sprache sind alles Bestandteile der Rückgewinnung.
Sozialanthropologie ist die Untersuchung von Verhaltensmustern in menschlichen Gesellschaften und Kulturen.
Britische und amerikanische Anthropologen, darunter Gillian Tett und Karen Ho, die sich mit der Wall Street befassten, lieferten eine alternative Erklärung für die Finanzkrise von 2007–2010 zu den technischen Erklärungen, die in der wirtschaftlichen und politischen Theorie verwurzelt waren.
Diese Entwicklung wurde durch die Einführung des Kulturrelativismus durch Franz Boas verstärkt, der argumentierte, dass Kulturen auf unterschiedlichen Vorstellungen von der Welt basieren und daher nur im Hinblick auf ihre eigenen Standards und Werte richtig verstanden werden können.
Im Jahr 1906 wurde der kongolesische Zwerg Ota Benga von der amerikanischen Anthropologin Madison Grant in einen Käfig im Bronx Zoo gesteckt, der als „das fehlende Glied“ zwischen einem Orang-Utan und der „weißen Rasse“ bezeichnet wurde – Grant, ein renommierter Eugeniker, war auch der Autor von The Vorbeigehen der Großen Rasse (1916).
Die Anthropologie unterschied sich zunehmend von der Naturgeschichte und gegen Ende des 19. Jahrhunderts begann sich die Disziplin in ihrer modernen Form zu kristallisieren – 1935 war es beispielsweise T.K. Penniman, eine Geschichte der Disziplin mit dem Titel „Hundert Jahre Anthropologie“ zu schreiben.
Außereuropäische Gesellschaften wurden daher als evolutionäre „lebende Fossilien“ angesehen, die untersucht werden konnten, um die europäische Vergangenheit zu verstehen.
Wie Stocking anmerkt, beschäftigte sich Tylor jedoch hauptsächlich mit der Beschreibung und Kartierung der Verteilung bestimmter Elemente der Kultur und nicht mit der größeren Funktion, und er schien im Allgemeinen eher eine viktorianische Vorstellung von Fortschritt als die Idee des ungerichteten, multilinearen Fortschritts zu vertreten kultureller Wandel, der von späteren Anthropologen vorgeschlagen wurde.
Seine vergleichenden Studien, die in den zahlreichen Ausgaben von The Golden Bough am einflussreichsten waren, analysierten weltweit Ähnlichkeiten im religiösen Glauben und in der Symbolik.
Die Erkenntnisse der Expedition setzten neue Maßstäbe für die ethnografische Beschreibung.
Weitere intellektuelle Begründer sind W. H. R. Rivers und A. C. Haddon, deren Ausrichtung die zeitgenössischen Parapsychologien von Wilhelm Wundt und Adolf Bastian widerspiegelte, sowie Sir E. B. Tylor, der die Anthropologie im Anschluss an Auguste Comte als positivistische Wissenschaft definierte.
Auch A. R. Radcliffe-Brown veröffentlichte 1922 ein wegweisendes Werk.
Dies war insbesondere bei Radcliffe-Brown der Fall, der seine Agenda für „Sozialanthropologie“ verbreitete, indem er an Universitäten im gesamten britischen Empire und Commonwealth lehrte.
Er glaubte, dass in ethnografischen Daten verwendete indigene Begriffe zum Nutzen des Lesers in angloamerikanische Rechtsbegriffe übersetzt werden sollten.
Die Abteilungen für Sozialanthropologie an verschiedenen Universitäten konzentrieren sich tendenziell auf unterschiedliche Aspekte des Fachgebiets.
Ein Volk ist eine Vielzahl von Personen, die als Ganzes betrachtet werden.
Vier Bundesstaaten – Massachusetts, Virginia, Pennsylvania und Kentucky – bezeichnen sich in Bezug auf Fallunterschriften und Rechtsverfahren als Commonwealth.
In einigen Teilen der Welt hat sich die Ethnologie entlang unabhängiger Forschungs- und pädagogischer Lehrpfade entwickelt, wobei vor allem in den Vereinigten Staaten die Kulturanthropologie und in Großbritannien die Sozialanthropologie vorherrschend wurde.
Die Erforschung Amerikas durch europäische Entdecker im 15. Jahrhundert spielte eine wichtige Rolle bei der Formulierung neuer Vorstellungen vom Abendland (der westlichen Welt), beispielsweise der Vorstellung vom „Anderen“.
Der Fortschritt der Ethnologie, beispielsweise mit der strukturellen Anthropologie von Claude Lévi-Strauss, führte zur Kritik an Vorstellungen eines linearen Fortschritts oder des Pseudo-Oppositions zwischen „Gesellschaften mit Geschichte“ und „Gesellschaften ohne Geschichte“, die als zu abhängig von einem begrenzten Kontext beurteilt wurden Sicht der Geschichte als durch akkumulatives Wachstum konstituiertes.
Allerdings wurden die Behauptungen eines solchen kulturellen Universalismus von verschiedenen Sozialdenkern des 19. und 20. Jahrhunderts kritisiert, darunter Marx, Nietzsche, Foucault, Derrida, Althusser und Deleuze.
Eine ethnische Gruppe oder Ethnizität ist eine Gruppierung von Menschen, die sich aufgrund gemeinsamer Merkmale miteinander identifizieren, die sie von anderen Gruppen unterscheiden, wie etwa gemeinsame Traditionen, Abstammung, Sprache, Geschichte, Gesellschaft, Kultur, Nation, Religion usw soziale Behandlung in ihrem Wohngebiet.
Die Zugehörigkeit zu einer ethnischen Gruppe wird in der Regel durch ein gemeinsames kulturelles Erbe, eine gemeinsame Abstammung, einen Ursprungsmythos, eine Geschichte, ein Heimatland, eine Sprache oder einen Dialekt, symbolische Systeme wie Religion, Mythologie und Ritual, Küche, Kleidungsstil, Kunst oder körperliche Erscheinung definiert.
Durch Sprachwechsel, Akkulturation, Adoption und religiöse Konvertierung können Einzelpersonen oder Gruppen im Laufe der Zeit von einer ethnischen Gruppe zu einer anderen wechseln.
Ob durch Spaltung oder Verschmelzung, die Bildung einer eigenen ethnischen Identität wird als Ethnogenese bezeichnet.
Im frühneuzeitlichen Englisch und bis zur Mitte des 19. Jahrhunderts bedeutete „ethnisch“ heidnisch oder heidnisch (im Sinne unterschiedlicher „Nationen“, die noch nicht am christlichen Oikumene teilnahmen), wie in der Septuaginta ta ethne („die Nationen“) verwendet wurde „), um das hebräische Goyim mit „die Nationen, Nicht-Hebräer, Nicht-Juden“ zu übersetzen.
Im 19. Jahrhundert wurde der Begriff im Sinne einer Rückkehr zur ursprünglichen griechischen Bedeutung im Sinne von „eigentümlich für eine Rasse, ein Volk oder eine Nation“ verwendet.
ethnisch, a. und n.") Je nach Kontext kann der Begriff Nationalität entweder synonym mit ethnischer Zugehörigkeit oder synonym mit Staatsbürgerschaft (in einem souveränen Staat) verwendet werden.
Ob ethnische Zugehörigkeit als kulturelle Universalität gilt, hängt in gewissem Maße von der genauen Definition ab, die verwendet wird.
Laut Thomas Hylland Eriksen wurde die Erforschung der Ethnizität bis vor Kurzem von zwei unterschiedlichen Debatten dominiert.
Der instrumentalistische Ansatz hingegen behandelt Ethnizität in erster Linie als Ad-hoc-Element einer politischen Strategie, das als Ressource für Interessengruppen zur Erreichung sekundärer Ziele wie beispielsweise der Steigerung von Wohlstand, Macht oder Status genutzt wird.
Konstruktivisten betrachten nationale und ethnische Identitäten als das Produkt historischer Kräfte, die oft neu sind, selbst wenn die Identitäten als alt dargestellt werden.
Dies steht im Zusammenhang mit Debatten über Multikulturalismus in Ländern wie den Vereinigten Staaten und Kanada, in denen große Einwanderer aus vielen verschiedenen Kulturen leben, sowie mit dem Postkolonialismus in der Karibik und Südasien.
Drittens resultierte die Gruppenbildung aus dem Drang, Macht und Status zu monopolisieren.
Barth ging weiter als Weber, indem er die konstruierte Natur der ethnischen Zugehörigkeit betonte.
Er wollte sich von anthropologischen Vorstellungen von Kulturen als begrenzten Einheiten und Ethnizität als primordialistischen Bindungen trennen und sie durch einen Fokus auf die Schnittstelle zwischen Gruppen ersetzen.
Er stimmt mit Joan Vincents Beobachtung überein, dass (in Cohens Paraphrase) „Ethnizität ... in Grenzbegriffen in Bezug auf die spezifischen Bedürfnisse der politischen Mobilisierung eingegrenzt oder erweitert werden kann.“
Ethnische Gruppen wurden eher als soziale als als biologische Einheiten definiert.
Beispiele für verschiedene Ansätze sind Primordialismus, Essentialismus, Perennialismus, Konstruktivismus, Modernismus und Instrumentalismus.
Der „essentialistische Primordialismus“ vertritt außerdem die Ansicht, dass ethnische Zugehörigkeit eine apriorische Tatsache der menschlichen Existenz sei, dass ethnische Zugehörigkeit jeder menschlichen sozialen Interaktion vorausgeht und dass sie durch diese unverändert bleibt.
Der „Verwandtschaftsprimordialismus“ geht davon aus, dass ethnische Gemeinschaften Erweiterungen von Verwandtschaftseinheiten sind und im Wesentlichen durch Verwandtschafts- oder Clanbindungen entstanden sind, wobei die Auswahl kultureller Zeichen (Sprache, Religion, Traditionen) genau darauf ausgerichtet ist, diese biologische Affinität zu zeigen.
„Geertz‘ Primordialismus“, insbesondere vertreten durch den Anthropologen Clifford Geertz, argumentiert, dass Menschen im Allgemeinen urmenschlichen „Gegebenheiten“ wie Blutsbande, Sprache, Territorium und kulturellen Unterschieden eine überwältigende Macht zuschreiben.
Smith (1999) unterscheidet zwei Varianten: den „kontinuierlichen Perennialismus“, der behauptet, dass bestimmte Nationen über sehr lange Zeiträume existierten, und den „rekurrenten Perennialismus“, der sich auf die Entstehung, Auflösung und Wiedererscheinung von Nationen als wiederkehrenden Aspekt der Menschheitsgeschichte konzentriert.
Diese Ansicht besagt, dass das Konzept der ethnischen Zugehörigkeit ein Instrument ist, mit dem politische Gruppen Ressourcen wie Reichtum, Macht, Territorium oder Status im Interesse ihrer jeweiligen Gruppe manipulieren.
Der „instrumentalistische Perennialismus“ betrachtet Ethnizität in erster Linie als ein vielseitiges Instrument, das verschiedene ethnische Gruppen und Grenzen im Laufe der Zeit identifiziert, erklärt Ethnizität jedoch als einen Mechanismus der sozialen Schichtung, was bedeutet, dass Ethnizität die Grundlage für eine hierarchische Anordnung von Individuen ist.
Laut Donald Noel wird eine ethnische Schichtung nur dann entstehen, wenn bestimmte ethnische Gruppen miteinander in Kontakt gebracht werden und nur wenn diese Gruppen durch ein hohes Maß an Ethnozentrismus, Konkurrenz und unterschiedlicher Macht gekennzeichnet sind.
Um mit Noels Theorie fortzufahren, muss für die Entstehung einer ethnischen Schichtung ein gewisses Maß an unterschiedlicher Macht vorhanden sein.
Die verschiedenen ethnischen Gruppen müssen um ein gemeinsames Ziel konkurrieren, etwa um Macht oder Einfluss, oder um ein materielles Interesse, etwa Reichtum oder Territorium.
Es geht davon aus, dass ethnische Gruppen nur Produkte menschlicher sozialer Interaktion sind und nur insoweit aufrechterhalten werden, als sie als gültige soziale Konstrukte in Gesellschaften aufrechterhalten werden.
Sie sind der Ansicht, dass ethnische Homogenität zuvor nicht als idealer oder notwendiger Faktor für die Bildung großer Gesellschaften angesehen wurde.
Mitglieder einer ethnischen Gruppe behaupten im Großen und Ganzen, dass es im Laufe der Zeit kulturelle Kontinuitäten gegeben habe, obwohl Historiker und Kulturanthropologen dokumentiert haben, dass viele der Werte, Praktiken und Normen, die eine Kontinuität mit der Vergangenheit implizieren, relativ neue Erfindungen sind.
Es basiert auf dem Begriff „Kultur“.
Diese Ansicht entstand als Rechtfertigung für die Versklavung von Afroamerikanern und den Völkermord an amerikanischen Ureinwohnern in einer Gesellschaft, die offiziell auf Freiheit für alle gegründet war.
Viele der führenden Wissenschaftler der damaligen Zeit griffen die Idee der Rassenunterschiede auf und stellten fest, dass weiße Europäer überlegen seien.
Anstatt den marginalisierten Status farbiger Menschen in den Vereinigten Staaten auf ihre inhärente biologische Minderwertigkeit zurückzuführen, führte er ihn auf ihre Unfähigkeit zurück, sich in die amerikanische Kultur zu integrieren.
Sie argumentieren in „Racial Formation in the United States“, dass die Ethnizitätstheorie ausschließlich auf den Einwanderungsmustern der weißen Bevölkerung basierte und die einzigartigen Erfahrungen von Nicht-Weißen in den Vereinigten Staaten berücksichtigte.
Die Assimilation, bei der die besonderen Qualitäten einer einheimischen Kultur aufgegeben werden, um sich in die Kultur des Gastlandes einzufügen, funktionierte bei manchen Gruppen als Reaktion auf Rassismus und Diskriminierung nicht, bei anderen hingegen schon.
Sie gipfelten in der Entstehung von „Nationalstaaten“, in denen die mutmaßlichen Grenzen der Nation mit den Staatsgrenzen zusammenfielen (oder idealerweise zusammenfielen).
Zu Nationalstaaten gehören jedoch immer auch Bevölkerungsgruppen, die aus dem einen oder anderen Grund vom nationalen Leben ausgeschlossen wurden.
Multiethnische Staaten können das Ergebnis zweier gegensätzlicher Ereignisse sein: entweder der jüngsten Schaffung von Staatsgrenzen im Widerspruch zu traditionellen Stammesgebieten oder der jüngsten Einwanderung ethnischer Minderheiten in einen ehemaligen Nationalstaat.
Staaten wie das Vereinigte Königreich, Frankreich und die Schweiz umfassten seit ihrer Gründung unterschiedliche ethnische Gruppen und erlebten ebenfalls eine erhebliche Einwanderung, was zu sogenannten „multikulturellen“ Gesellschaften führte, insbesondere in Großstädten.
Obwohl diese Kategorien üblicherweise als dem öffentlichen, politischen Bereich zugehörig diskutiert werden, werden sie weitgehend auch im privaten, familiären Bereich beibehalten.
Vor Weber (1864–1920) wurden Rasse und ethnische Zugehörigkeit in erster Linie als zwei Aspekte derselben Sache angesehen.
Nach dieser Auffassung sollte der Staat die ethnische, nationale oder rassische Identität nicht anerkennen, sondern vielmehr die politische und rechtliche Gleichheit aller Menschen durchsetzen.
Im 19. Jahrhundert entwickelte sich die politische Ideologie des ethnischen Nationalismus, als der Rassenbegriff zunächst von deutschen Theoretikern wie Johann Gottfried von Herder mit dem Nationalismus verknüpft wurde.
Jeder vertrat die panethnische Idee, dass diese Regierungen nur Ländereien erwarben, die schon immer von ethnischen Deutschen bewohnt worden waren.
Die Kolonialisierung Asiens wurde im 20. Jahrhundert weitgehend beendet, als es auf dem gesamten Kontinent zu nationalen Bestrebungen nach Unabhängigkeit und Selbstbestimmung kam.
Eine Reihe europäischer Länder, darunter Frankreich und die Schweiz, erheben keine Informationen über die ethnische Zugehörigkeit ihrer Wohnbevölkerung.
Während der europäischen Kolonialisierung kamen Europäer nach Nordamerika.
Die digitale Ethnographie bietet viel mehr Möglichkeiten, verschiedene Kulturen und Gesellschaften zu betrachten.
Die relationale Ethnographie artikuliert eher Forschungsfelder als Orte oder Prozesse statt verarbeiteter Menschen.
Ziel ist es, Daten so zu sammeln, dass der Forscher den Daten möglichst wenig persönliche Voreingenommenheit verleiht.
Interviews werden oft auf Tonband aufgezeichnet und später transkribiert, sodass das Interview ungestört von Notizen durchgeführt werden kann, alle Informationen jedoch später für eine vollständige Analyse verfügbar sind.
Trotz dieser Reflexivitätsversuche kann kein Forscher völlig unvoreingenommen sein.
Diese Informanten werden in der Regel gebeten, andere Informanten zu identifizieren, die die Gemeinschaft repräsentieren, häufig mithilfe von Schneeball- oder Kettenstichproben.
2010) untersuchen die ontologischen und epistemologischen Voraussetzungen, die der Ethnographie zugrunde liegen.
Forscher der Kritischen Theorie befassen sich mit „Machtfragen innerhalb der von Forschern erforschten Beziehungen und den Verbindungen zwischen Wissen und Macht“.
Ein Bild kann aus der Perspektive eines bestimmten Individuums in der physischen Welt enthalten sein, hauptsächlich basierend auf den vergangenen Erfahrungen dieses Individuums.
Die Idee eines Bildes beruht auf der Vorstellungskraft und wird von Kindern nachweislich auf sehr spontane und natürliche Weise genutzt.
Heutzutage legen Kultur- und Sozialanthropologen großen Wert auf ethnografische Forschung.
Ethnographien werden manchmal auch „Fallstudien“ genannt.
Bei der Feldforschung geht es in der Regel darum, ein Jahr oder länger in einer anderen Gesellschaft zu verbringen, mit den Menschen vor Ort zusammenzuleben und etwas über ihre Lebensweise zu lernen.
Benedicts Erfahrungen mit dem Südwest-Zuni-Pueblo gelten als Grundlage ihrer prägenden Feldforschung.
Eine typische Ethnographie versucht ganzheitlich zu sein und folgt in der Regel einer Gliederung, die eine kurze Geschichte der betreffenden Kultur, eine Analyse der physischen Geographie oder des von den untersuchten Menschen bewohnten Geländes, einschließlich des Klimas, und oft auch das, was biologische Anthropologen als Lebensraum bezeichnen, umfasst.
Verwandtschaft und soziale Struktur (einschließlich Alterseinstufung, Peer-Gruppen, Geschlecht, freiwillige Vereinigungen, Clans, Gruppierungen usw., sofern vorhanden) werden typischerweise einbezogen.
Riten, Rituale und andere Zeugnisse der Religion erregen seit langem großes Interesse und sind manchmal von zentraler Bedeutung für Ethnographien, insbesondere wenn sie in der Öffentlichkeit durchgeführt werden, wo Anthropologen sie sehen können.
Wenn beispielsweise das Zwinkern innerhalb einer Gruppe von Menschen eine kommunikative Geste war, versuchte er zunächst herauszufinden, was ein Zwinkern bedeuten könnte (es könnte mehrere Bedeutungen haben).
Geertz folgte zwar immer noch einer Art traditioneller ethnografischer Gliederung, ging jedoch über diese Gliederung hinaus und sprach von „Netzen“ statt von „Umrissen“ der Kultur.
Die Schreibkultur trug dazu bei, Veränderungen sowohl in der Anthropologie als auch in der Ethnographie herbeizuführen, die oft als „postmodern“, „reflexiv“, „literarisch“, „dekonstruktiv“ oder „poststrukturell“ beschrieben werden, indem der Text dazu beitrug, die verschiedenen epistemischen und politische Zwangslagen, die viele Praktiker als eine Belastung für ethnografische Darstellungen und Praktiken ansahen.
In Bezug auf diesen letzten Punkt wurde Writing Culture zu einem Schwerpunkt für die Untersuchung, wie Ethnographen verschiedene Kulturen und Gesellschaften beschreiben können, ohne die Subjektivität der untersuchten Individuen und Gruppen zu leugnen, und dies gleichzeitig tun, ohne den Anspruch auf absolutes Wissen und objektive Autorität zu erheben.
Da der Zweck der Ethnographie darin besteht, die geteilten und erlernten Werte-, Verhaltens-, Glaubens- und Sprachmuster einer Kultur teilenden Gruppe zu beschreiben und zu interpretieren, stellen Harris (1968) und Agar (1980) fest, dass Ethnographie sowohl ein Prozess als auch ein Prozess ist ein Ergebnis der Forschung.
Die Soziologin Sam Ladner argumentiert in ihrem Buch, dass das Verständnis der Verbraucher und ihrer Wünsche einen Wechsel des „Standpunkts“ erfordert, den nur die Ethnographie ermöglicht.
Durch die Beurteilung der Benutzererfahrung in einer „natürlichen“ Umgebung liefert die Ethnologie Einblicke in die praktischen Anwendungen eines Produkts oder einer Dienstleistung.
Ein Beweis dafür ist die Konferenz „Ethnographic Praxis in Industry“ (EPIC).
Die Monographie „The New Language of Qualitative Method“ von Jaber F. Gubrium und James A. Holstein (1997) diskutiert Formen der Ethnographie im Hinblick auf ihre „Methodengespräche“.
Im Wesentlichen behauptet Fine, dass Forscher in der Regel nicht so ethisch sind, wie sie behaupten oder annehmen – und dass „jeder Job Methoden beinhaltet, Dinge zu tun, von denen andere nichts wissen sollten“.
Er vertritt die Auffassung, dass „Illusionen“ unerlässlich seien, um den beruflichen Ruf aufrechtzuerhalten und möglicherweise noch schlimmere Folgen zu vermeiden.
Der Ethikkodex stellt fest, dass Anthropologen Teil eines größeren wissenschaftlichen und politischen Netzwerks sowie der menschlichen und natürlichen Umwelt sind, über die respektvoll berichtet werden muss.
Forscher nehmen Beinahe-Fiktionen und verwandeln sie in Tatsachenbehauptungen.
In Wirklichkeit wird ein Ethnograph aufgrund mangelnder Allwissenheit immer einen Aspekt übersehen.
Indigene Völker, auch Ureinwohner, Ureinwohner, Ureinwohner oder autochthone Völker genannt, sind kulturell unterschiedliche ethnische Gruppen, die an einem Ort beheimatet sind, der von einer anderen ethnischen Gruppe kolonisiert und besiedelt wurde.
Völker werden üblicherweise als „einheimisch“ bezeichnet, wenn sie Traditionen oder andere Aspekte einer frühen Kultur pflegen, die mit einer bestimmten Region verbunden ist.
Indigene Völker sind weiterhin Bedrohungen ihrer Souveränität, ihres wirtschaftlichen Wohlergehens, ihrer Sprachen, ihrer Wissensweisen und ihres Zugangs zu den Ressourcen ausgesetzt, von denen ihre Kulturen abhängen.
Schätzungen der gesamten Weltbevölkerung indigener Völker liegen üblicherweise zwischen 250 und 600 Millionen.
Als Bezug auf eine Gruppe von Menschen wurde der Begriff „indigen“ erstmals von Europäern verwendet, die ihn zur Unterscheidung der indigenen Völker Amerikas von versklavten Afrikanern verwendeten.
In den 1970er Jahren wurde der Begriff verwendet, um die Erfahrungen, Probleme und Kämpfe von Gruppen kolonisierter Menschen über internationale Grenzen hinweg miteinander zu verbinden.
Diese Situation kann auch dann bestehen bleiben, wenn die indigene Bevölkerung zahlenmäßig größer ist als die der anderen Einwohner der Region oder des Staates; Der entscheidende Begriff hier ist die Trennung von Entscheidungs- und Regulierungsprozessen, die einen gewissen, zumindest titelmäßigen, Einfluss auf Aspekte ihrer Gemeinschaft und Landrechte haben.
In einem 2009 vom Sekretariat des Ständigen Forums für indigene Fragen veröffentlichten Bericht der Vereinten Nationen heißt es: „Seit Jahrhunderten, seit der Zeit ihrer Kolonisierung, Eroberung oder Besetzung, haben indigene Völker Geschichten über Widerstand, Kontakte oder Zusammenarbeit mit Staaten dokumentiert und damit ihre Überzeugung unter Beweis gestellt.“ und Entschlossenheit, mit ihrer unterschiedlichen souveränen Identität zu überleben.
Diese Menschen wurden von antiken Schriftstellern entweder als Vorfahren der Griechen oder als eine frühere Gruppe von Menschen angesehen, die vor den Griechen in Griechenland lebten.
Die Kreuzzüge (1096-1271) basierten auf dem Ehrgeiz eines heiligen Krieges gegen diejenigen, die die Kirche als Ungläubige ansah.
Der Rat bestätigte jedoch, dass Eroberungen „legal“ stattfinden könnten, wenn Nichtchristen sich weigern würden, sich an die Christianisierung und das europäische Naturrecht zu halten.
Im 14. und 15. Jahrhundert gerieten die indigenen Völker der heutigen Kanarischen Inseln, bekannt als Guanchen (die seit der Zeit vor Christus auf den Inseln lebten), in den Mittelpunkt der Aufmerksamkeit der Kolonisatoren.
Im Jahr 1402 begannen die Spanier mit der Invasion und Kolonisierung der Inseln.
Die Eindringlinge brachten Zerstörung und Krankheiten über das Guanche-Volk, dessen Identität und Kultur dadurch verschwanden.
Wie Robert J. Miller, Jacinta Ruru, Larissa Behrendt und Tracey Lindberg feststellten, entwickelte sich die Doktrin im Laufe der Zeit, „um die Herrschaft nichtchristlicher, nichteuropäischer Völker und die Beschlagnahmung ihres Landes und ihrer Rechte zu rechtfertigen“.
Der spanische König Ferdinand und Königin Isabella heuerten den 1492 entsandten Christoph Kolumbus an, um neue Länder zu kolonisieren und unter die spanische Krone zu bringen.
Alexander gewährte Spanien alle Ländereien, die es entdeckte, solange sie „nicht zuvor von irgendeinem christlichen Besitzer besessen“ worden waren.
Viele Konquistadoren befürchteten offenbar, dass indigene Völker, wenn man ihnen die Möglichkeit dazu gäbe, tatsächlich das Christentum annehmen würden, was eine Invasion ihres Landes und den Diebstahl ihres Hab und Guts rechtlich nicht zulassen würde.
Als katholische Länder im Jahr 1493 arbeiteten sowohl England als auch Frankreich daran, die Entdeckungslehre „neu zu interpretieren“, um ihren eigenen kolonialen Interessen zu dienen.
Landansprüche wurden durch symbolische „Entdeckungsrituale“ geltend gemacht, die durchgeführt wurden, um den Rechtsanspruch der kolonisierenden Nation auf das Land zu veranschaulichen.
Im Jahr 1774 versuchte Kapitän James Cook, die spanischen Landansprüche auf Tahiti zu entkräften, indem er die Besitzmarken entfernte und dann englische Besitzmarken einführte.
Dieses Konzept formalisierte die Idee, dass Ländereien, die nicht in einer Weise genutzt wurden, die von den europäischen Rechtssystemen genehmigt wurde, für die europäische Kolonisierung offen waren.
Als die „Regeln“ der Kolonisierung in einer zwischen den europäischen Kolonialmächten vereinbarten Rechtsdoktrin verankert wurden, weiteten sich die Methoden zur Geltendmachung von Ansprüchen auf indigenes Land weiterhin rasch aus.
Aufgrund der Schwierigkeiten bei der Identifizierung sowie der Abweichungen und Unzulänglichkeiten der verfügbaren Volkszählungsdaten sind genaue Schätzungen für die Gesamtbevölkerung der indigenen Völker der Welt sehr schwierig zu erstellen.
Dazu gehören mindestens 5.000 verschiedene Völker in über 72 Ländern.
Einige wurden auch von anderen Bevölkerungsgruppen assimiliert oder haben viele andere Veränderungen erfahren.
Die äußerst vielfältigen und zahlreichen ethnischen Gruppen, aus denen sich die meisten modernen, unabhängigen afrikanischen Staaten zusammensetzen, beherbergen verschiedene Völker, deren Situation, Kultur und Lebensstil als Hirten oder Jäger und Sammler im Allgemeinen marginalisiert und von den vorherrschenden politischen und wirtschaftlichen Strukturen der Nation getrennt sind.
Die Auswirkungen der historischen und anhaltenden europäischen Kolonisierung Amerikas auf indigene Gemeinschaften waren im Allgemeinen recht schwerwiegend, wobei viele Behörden einen erheblichen Bevölkerungsrückgang schätzen, der hauptsächlich auf Krankheiten, Landraub und Gewalt zurückzuführen ist.
In den südlichen Bundesstaaten Oaxaca (65,73 %) und Yucatán (65,40 %) ist die Mehrheit der Bevölkerung indigen, wie 2015 berichtet wurde.
Die Bezeichnungen „Indianer“ und „Eskimo“ werden in Kanada nicht mehr verwendet.
Am bemerkenswertesten war der Wechsel von Aboriginal Affairs and Northern Development Canada (AANDC) zu Indigenous and Northern Affairs Canada (INAC) im Jahr 2015, das sich dann im Jahr 2017 in Indigenous Services Canada und Crown-Indigenous Relations and Northern Development Canada aufspaltete.
Die Völker der First Nations unterzeichneten zwischen 1871 und 1921 in weiten Teilen des heutigen Kanadas, mit Ausnahme von Teilen von British Columbia, elf nummerierte Verträge.
Das autonome Gebiet Grönland innerhalb des Königreichs Dänemark ist auch die Heimat einer anerkannten indigenen Bevölkerung und einer Mehrheitsbevölkerung von Inuit (etwa 85 %), die das Gebiet im 13. Jahrhundert besiedelten und die indigenen Dorseter und grönländischen Nordmänner verdrängten.
In spanisch- oder portugiesischsprachigen Ländern findet man die Verwendung von Begriffen wie „Indianer“, „indigene Völker“, „Indianer“, „Ureinwohner“ und in Peru „Ureinwohnergemeinschaften“, insbesondere bei amazonischen Gesellschaften wie den Urarina und Matsés.
Indigene Völker gibt es auf dem gesamten Territorium Brasiliens, die meisten von ihnen leben jedoch in Indianerreservaten im Norden und Mittelwesten des Landes.
Aufgrund des Völkermords an den Armeniern im Jahr 1915 leben derzeit mehr Armenier außerhalb ihrer angestammten Heimat.
Das Argument kam in den 1990er Jahren in den israelisch-palästinensischen Konflikt, als Palästinenser als bereits existierende Bevölkerung, die durch jüdische Siedlungen vertrieben wurde und derzeit eine Minderheit im Staat Israel darstellt, den indigenen Status beanspruchten.
In Russland ist die Definition von „indigenen Völkern“ umstritten, da sie sich weitgehend auf eine Bevölkerungszahl (weniger als 50.000 Menschen) bezieht und die Selbstidentifikation sowie die Herkunft aus indigenen Bevölkerungsgruppen vernachlässigt, die das Land oder die Region bei der Invasion, Kolonisierung oder Staatsgründung bewohnten Grenzen, besondere soziale, wirtschaftliche und kulturelle Institutionen.
Die Tibeter sind in Tibet beheimatet.
In Hongkong werden die Ureinwohner der New Territories in der Chinesisch-Britischen Gemeinsamen Erklärung als Menschen definiert, die in männlicher Linie von einer Person abstammen, die 1898 vor der Konvention zur Erweiterung des Hongkong-Territoriums lebte.
Die Cham sind die Ureinwohner des ehemaligen Bundesstaates Champa, der während der Nam Tiến in den Cham-Vietnamesischen Kriegen von Vietnam erobert wurde.
Die Khmer Krom sind die Ureinwohner des Mekong-Deltas und Saigons, die Vietnam vom kambodschanischen König Chey Chettha II. im Austausch gegen eine vietnamesische Prinzessin erworben hat.
Dieses Problem teilen viele andere Länder in der ASEAN-Region.
Im Sulu-Archipel gibt es viele verschiedene Arten indigener Völker.
Diese Gruppen werden oft zusammen als indigene Australier bezeichnet.
Im Laufe des 20. Jahrhunderts erlangten mehrere dieser ehemaligen Kolonien ihre Unabhängigkeit und es bildeten sich Nationalstaaten unter lokaler Kontrolle.
Auf den Inseln Palau in Mikronesien wurden kürzlich Überreste von mindestens 25 Miniaturmenschen gefunden, die vor 1.000 bis 3.000 Jahren lebten.
Laut der Volkszählung von 2013 machen neuseeländische Māori 14,9 % der neuseeländischen Bevölkerung aus, wobei weniger als die Hälfte (46,5 %) aller Māori-Bewohner sich ausschließlich als Māori identifizieren.
Viele nationale Führer der Māori unterzeichneten einen Vertrag mit den Briten, den Vertrag von Waitangi (1840), der in manchen Kreisen als Bildung der modernen geopolitischen Einheit Neuseelands angesehen wird.
Zu diesen Themen gehören die Erhaltung der Kultur und Sprache, Landrechte, Eigentum und Ausbeutung natürlicher Ressourcen, politische Entschlossenheit und Autonomie, Umweltzerstörung und -eingriff, Armut, Gesundheit und Diskriminierung.
Die Situation kann noch verwirrender sein, wenn es in einer bestimmten Region eine komplizierte oder umstrittene Migrations- und Bevölkerungsgeschichte gibt, die zu Streitigkeiten über den Vorrang und das Eigentum an Land und Ressourcen führen kann.
Trotz der Vielfalt der indigenen Völker lässt sich feststellen, dass sie gemeinsame Probleme und Probleme im Umgang mit der vorherrschenden oder eindringenden Gesellschaft haben.
Bemerkenswerte Ausnahmen sind die Sacha- und Komi-Völker (zwei nördliche indigene Völker Russlands), die jetzt ihre eigenen autonomen Republiken innerhalb des russischen Staates kontrollieren, und die kanadischen Inuit, die einen Großteil des Territoriums von Nunavut (gegründet 1999) ausmachen.
Diese Ablehnung führte letztendlich zur Anerkennung, dass es bereits ein vom Volk der Meriam praktiziertes Rechtssystem gab.
Abgerufen am 11. Oktober 2011.
Sowohl Hindus als auch Chams haben unter der gegenwärtigen vietnamesischen Regierung religiöse und ethnische Verfolgung und Einschränkungen ihres Glaubens erlebt. Der vietnamesische Staat beschlagnahmte Cham-Eigentum und verbot Cham, ihre religiösen Überzeugungen zu befolgen.
Im Jahr 2012 stürmte die vietnamesische Polizei im Dorf Chau Giang eine Cham-Moschee, stahl den Stromgenerator und vergewaltigte auch Cham-Mädchen.
Im Jahr 2012 erklärte Indonesien: „Die indonesische Regierung unterstützt die Förderung und den Schutz indigener Völker weltweit … Indonesien erkennt jedoch die Anwendung des Konzepts der indigenen Völker … im Land nicht an.“
Die Vietnamesen konzentrierten sich ursprünglich auf das Delta des Roten Flusses, führten aber während der Nam-Tien-Zeit Eroberungen durch und eroberten neue Gebiete wie Champa, das Mekong-Delta (aus Kambodscha) und das zentrale Hochland.
Das enorme Ausmaß der Überschwemmung vietnamesischer Kinh-Kolonisten in das zentrale Hochland hat die Bevölkerungsstruktur der Region erheblich verändert.
Und keine Eliminierung einer Kultur durch eine andere.“
Indigene Völker wurden als primitiv, wild oder unzivilisiert bezeichnet.
Einige Philosophen, wie Thomas Hobbes (1588-1679), betrachteten die Ureinwohner lediglich als „Wilde“.
Abgerufen aus dem Internetarchiv vom 13. Dezember 2013.
Die UN-Erklärung über die Rechte indigener Völker, die 2007 von der Generalversammlung angenommen wurde, begründet das Recht indigener Völker auf Selbstbestimmung und beinhaltet mehrere Rechte in Bezug auf die Bewirtschaftung natürlicher Ressourcen.
Ölbohrungen könnten die jahrtausendealte Kultur der Gwich'in zerstören.
Entwicklungsprojekte wie der Bau von Staudämmen, Pipelines und der Rohstoffabbau haben eine große Zahl indigener Völker vertrieben, oft ohne Entschädigung.
Diese Frauen werden auch wirtschaftlich von Männern abhängig, wenn sie ihren Lebensunterhalt verlieren.
Beispielsweise lehnen die Munduruku im Amazonas-Regenwald mit Hilfe von Greenpeace den Bau des Tapajós-Staudamms ab.
Es werden zwei Hauptszenarien vorgeschlagen: eine frühe Ausbreitung nach Zentralafrika und ein einziger Ursprung der von dort ausgehenden Ausbreitung, oder eine frühe Aufteilung in eine Ausbreitungswelle nach Osten und eine nach Süden, wobei sich eine Welle über das Kongobecken in Richtung Ostafrika bewegt. und ein weiterer zog nach Süden entlang der afrikanischen Küste und des Kongo-Flusssystems in Richtung Angola.
Die von den relativ wenigen modernen Bantu-Pastoralistengruppen verwendete Terminologie für Rinder lässt darauf schließen, dass der Erwerb des Viehs möglicherweise von zentralsudanesischen, kuliakischen und kuschitischsprachigen Nachbarn erfolgte.
Unweit des Mutirikiwi-Flusses bauten die Monomatapa-Könige den Groß-Simbabwe-Komplex, eine Zivilisation, die auf das Kalanga-Volk zurückgeht.
Die Swahili-Kultur, die aus diesem Austausch hervorgegangen ist, weist viele arabische und islamische Einflüsse auf, die in der traditionellen Bantu-Kultur nicht zu finden sind, ebenso wie die vielen afroarabischen Mitglieder des Bantu-Swahili-Volkes.
Nach dem Zweiten Weltkrieg übernahmen die Regierungen der National Party diesen Sprachgebrauch offiziell, während die wachsende afrikanische nationalistische Bewegung und ihre liberalen Verbündeten sich stattdessen dem Begriff „afrikanisch“ zuwandten, so dass „Bantu“ mit der Politik der Apartheid identifiziert wurde.
Wiederum diskreditierte die Assoziation mit der Apartheid den Begriff, und die südafrikanische Regierung wechselte zu dem politisch attraktiven, aber historisch irreführenden Begriff „ethnische Heimatländer“.
In Swati ist der Stamm -ntfu und das Substantiv buntfu.
Nicht alle Basken sprechen Baskisch.
modernes baskisches esan) und das Suffix -(k)ara („Art (etwas zu tun)“).
Er verzeichnet den Namen der baskischen Sprache als enusquera.
Obwohl sie aufgrund der Isolation in mancher Hinsicht genetisch unterschiedlich sind, sind die Basken in Bezug auf ihre Y-DNA- und mtDNA-Sequenzen sowie in Bezug auf einige andere genetische Loci immer noch sehr typisch europäisch.
Untersuchungen der Y-DNA-Haplogruppen ergaben jedoch, dass die überwiegende Mehrheit der modernen Basken in ihren direkten männlichen Abstammungslinien eine gemeinsame Abstammung mit anderen Westeuropäern hat, nämlich ein deutliches Überwiegen der indogermanischen Haplogruppe R1b-DF27 (70 %).
Trotz seiner hohen Häufigkeit im Baskenland ist die interne Y-STR-Diversität von R1b-DF27 dort geringer und führt zu neueren Altersschätzungen“, was darauf hindeutet, dass es von anderswo in die Region gebracht wurde.
Die Sammlung der dort beprobten mtDNA- und Y-DNA-Haplogruppen unterschied sich erheblich im Vergleich zu ihren modernen Häufigkeiten.
Vielmehr wurde vor etwa 4500 Jahren fast das gesamte Y-DNA-Erbe aus iberischen Beimischungen mesolithischer Jäger und Sammler und neolithischer Bauern durch die R1b-Linie indogermanischer Hirten aus der Steppe ersetzt, und die baskische genetische Besonderheit ist das Ergebnis jahrhundertelanger Unterwanderung Populationsgröße, genetische Drift und Endogamie.
Mattias Jakobsson von der Universität Uppsala in Schweden analysierte das genetische Material von acht menschlichen Skeletten aus der Steinzeit, die in der Höhle El Portalón in Atapuerca, Nordspanien, gefunden wurden.
Die Ergebnisse wurden in Proceedings of the National Academy of Sciences of the United States veröffentlicht.
Es wurde festgestellt, dass diese gemischte Gruppe auch Vorfahren anderer heutiger iberischer Völker war, aber während die Basken nach dieser Zeit Jahrtausende lang relativ isoliert blieben, führten spätere Migrationen nach Iberien zu deutlichen und zusätzlichen Beimischungen in allen anderen iberischen Gruppen.
Es gibt genügend Beweise, die die Hypothese stützen, dass sie zu dieser Zeit und später alte Varianten der baskischen Sprache sprachen (siehe: Aquitanische Sprache).
Das Königreich Pamplona, ein zentrales baskisches Reich, das später als Navarra bekannt wurde, durchlief einen Prozess der Feudalisierung und war dem Einfluss seiner viel größeren aragonesischen, kastilischen und französischen Nachbarn unterworfen.
Durch den Navarrese-Bürgerkrieg geschwächt, fiel der Großteil des Reiches schließlich vor dem Angriff der spanischen Armeen (1512–1524).
Dennoch genossen die Basken bis zur Französischen Revolution (1790) und den Karlistenkriegen (1839, 1876) ein hohes Maß an Selbstverwaltung, als die Basken den Thronfolger Carlos V. und seine Nachkommen unterstützten.
Die Baskische Autonome Gemeinschaft (BAC) ist im Baskischen als Baskische Autonome Gemeinschaft (BAAC) und in den baskischen Provinzen Álava, Biskaya und Gipuzkoa als Baskische Autonome Gemeinschaft (BAC) bekannt.
Manchmal wird es von Schriftstellern und Behörden nur unter Berücksichtigung dieser drei westlichen Provinzen einfach als „das Baskenland“ (oder Baskenland) bezeichnet, manchmal aber auch nur als praktische Abkürzung, wenn dies im Kontext nicht zu Verwirrung führt.
Insbesondere im allgemeinen Sprachgebrauch bezieht sich der französische Begriff Pays Basque („Baskenland“), sofern keine weitere Einschränkung vorliegt, entweder auf das gesamte Baskenland („Euskal Herria“ auf Baskisch) oder nicht selten auf den Norden (oder „Französisch“) ") Speziell Baskenland.
Beachten Sie, dass sich Navarra in historischen Kontexten möglicherweise auf ein größeres Gebiet bezieht und dass die heutige nördliche baskische Provinz Unter-Navarra auch als (Teil von) Nafarroa bezeichnet werden kann, während der Begriff „Hoch-Navarra“ (Nafarroa Garaia auf Baskisch, Alta Navarra auf Spanisch) wird auch als Bezeichnung für das Gebiet der heutigen Autonomen Gemeinschaft verwendet.
Spanischkenntnisse sind gemäß der spanischen Verfassung obligatorisch (Artikel Nr.
Nachdem die Baskischkenntnisse während der Franco-Diktatur viele Jahre lang aufgrund der offiziellen Verfolgung zurückgegangen waren, nehmen die Kenntnisse des Baskischen aufgrund günstiger offizieller Sprachenpolitik und der Unterstützung der Bevölkerung wieder zu.
Nur Spanisch ist eine offizielle Sprache von Navarra, und die baskische Sprache ist nur in der nördlichen Region der Provinz, wo sich die meisten baskischsprachigen Navarresen konzentrieren, zweioffiziell.
Ein Großteil dieser Bevölkerung lebt im oder in der Nähe des Stadtgürtels Bayonne-Anglet-Biarritz (BAB) an der Küste (auf Baskisch sind dies Baiona, Angelu und Miarritze).
Millionen baskischer Nachkommen (siehe Baskisch-Amerikaner und Baskisch-Kanadier) leben in Nordamerika (den Vereinigten Staaten; Kanada, hauptsächlich in den Provinzen Neufundland und Quebec), Lateinamerika (in allen 23 Ländern), Südafrika und Australien.
Schätzungen zufolge leben in Chile 2,5 bis 5 Millionen baskische Nachkommen. Die Basken hatten einen großen, wenn nicht den stärksten Einfluss auf die kulturelle und wirtschaftliche Entwicklung des Landes.
Es bestand größtenteils aus dem Gebiet, das heute die Bundesstaaten Chihuahua und Durango umfasst.
In Guatemala leben die meisten Basken seit sechs Generationen im Departement Sacatepequez, Antigua Guatemala und Jalapa, während einige nach Guatemala-Stadt ausgewandert sind.
Bambuco, eine kolumbianische Volksmusik, hat baskische Wurzeln.
Elko, Nevada, sponsert ein jährliches baskisches Festival, das den Tanz, die Küche und die Kultur der baskischen Völker spanischer, französischer und mexikanischer Nationalität feiert, die seit dem späten 19. Jahrhundert in Nevada angekommen sind.
In dieser Region befinden sich einige der größten Ranches Nordamerikas, die im Rahmen dieser kolonialen Landzuteilungen gegründet wurden.
In Chino, Kalifornien, gibt es eine Geschichte der baskischen Kultur.
Sie sind größtenteils Nachkommen von Siedlern aus Spanien und Mexiko.
Dieses mit der Landessprache verbundene Gefühl der baskischen Identität existiert nicht nur isoliert.
Wie in vielen europäischen Staaten schließt sich eine regionale Identität, sei sie sprachlich oder anderweitig, nicht gegenseitig mit der umfassenderen nationalen Identität aus.
Ich habe Freunde, die sich politisch engagieren, aber das ist nichts für mich.
Es gibt äußerst wenige einsprachige Baskischsprecher: Im Wesentlichen sind alle Baskischsprecher auf beiden Seiten der Grenze zweisprachig.
Es wird angenommen, dass die baskische Sprache im Gegensatz zu anderen europäischen Sprachen, die fast alle zur großen indogermanischen Sprachfamilie gehören, ein genetisches Sprachisolat ist.
Heimat ist in diesem Zusammenhang gleichbedeutend mit familiären Wurzeln.
Wie in anderen Kulturen hing das Schicksal anderer Familienmitglieder vom Vermögen einer Familie ab: Wohlhabende baskische Familien neigten dazu, für alle Kinder auf die eine oder andere Weise zu sorgen, während weniger wohlhabende Familien möglicherweise nur über ein Vermögen verfügten, das sie für ein Kind bereitstellen konnten.
Meistens nach Beginn der Industrialisierung führte dieses System zur Auswanderung vieler ländlicher Basken nach Spanien, Frankreich oder Amerika.
Einige Wissenschaftler und Kommentatoren haben versucht, diese Punkte in Einklang zu bringen, indem sie annahmen, dass die patrilineare Verwandtschaft eine Neuerung darstellt.
Sie gingen aus dem Franco-Regime mit einer wiederbelebten Sprache und Kultur hervor.
Die Region war eine Quelle von Missionaren wie Francis Xavier und Michel Garicoïts.
Lasuén war der Nachfolger des Franziskaners Padre Junípero Serra und gründete 9 der 21 bestehenden kalifornischen Missionen entlang der Küste.
Als Heinrich III. von Navarra zum Katholizismus konvertierte, um König von Frankreich zu werden, verschwand der Protestantismus praktisch aus der baskischen Gemeinschaft.
Einer einzigen Meinungsumfrage zufolge bekennen sich heutzutage nur noch etwas mehr als 50 % der Basken zu einem gewissen Glauben an Gott, während der Rest entweder Agnostiker oder Atheist ist.
Der einen zufolge gelangte das Christentum im 4. und 5. Jahrhundert ins Baskenland, der anderen zufolge erfolgte dies jedoch erst im 12. und 13. Jahrhundert.
In diesem Sinne kam das Christentum „früh“.
Einer Überlieferung zufolge reiste sie alle sieben Jahre zwischen einer Höhle auf dem Berg Anboto und einer auf einem anderen Berg (die Geschichten variieren); Das Wetter war nass, wenn sie in Anboto war, trocken, wenn sie in Aloña, Supelegor oder Gorbea war.
Es heißt, dass sie die Stürme verursachten, als sie sich in den hohen Höhlen der heiligen Gipfel versammelten.
Legenden sprechen auch von vielen und zahlreichen Geistern, wie Jentilak (entspricht Riesen), Lamiak (entspricht Nymphen), Mairuak (Erbauer der Cromlechs oder Steinkreise, wörtlich Mauren), Iratxoak (Kobolde), Sorginak (Hexen, Priesterin von Mari). ), und so weiter.
Es gibt einen Betrüger namens San Martin Txiki („St. Martin der Kleine“).
Die Jentilak („Riesen“) hingegen sind ein legendäres Volk, was das Verschwinden eines Volkes der Steinzeitkultur erklärt, das einst im Hochland lebte und keine Kenntnisse über Eisen hatte.
Seit mehr als einem Jahrhundert diskutieren Wissenschaftler ausführlich über den hohen Status baskischer Frauen in den Gesetzbüchern sowie über ihre Positionen als Richterinnen, Erbinnen und Schiedsrichterinnen in der vorrömischen Zeit, im Mittelalter und in der Neuzeit.
Navarra verfügt über ein eigenes Autonomiestatut, eine umstrittene Regelung, die während des Übergangs Spaniens zur Demokratie geschaffen wurde (das Amejoramiento, eine „Aufwertung“ seines früheren Status während der Diktatur).
Fragen der politischen, sprachlichen und kulturellen Zugehörigkeit und Identität sind in Navarra äußerst komplex.
Die Mehrheit der Schulen im Zuständigkeitsbereich des baskischen Bildungssystems verwenden Baskisch als primäre Unterrichtssprache.
Im Gegensatz dazu ist der Wunsch nach mehr Autonomie oder Unabhängigkeit besonders unter linken baskischen Nationalisten verbreitet.
Sie betrachten sich als kulturell und vor allem sprachlich verschieden von ihren umliegenden Nachbarn.
Miguel de Unamuno war ein bekannter Schriftsteller und Philosoph des späten 19. und 20. Jahrhunderts.
Er gründete auch den chilenischen Gewerkschaftsverband, um eine Gewerkschaftsbewegung zu fördern, die auf den Soziallehren der katholischen Kirche basiert.
Die historische Präsenz der San in Botswana zeigt sich besonders deutlich in der Tsodilo Hills-Region im Norden Botswanas.
Von den 1950er bis 1990er Jahren wechselten die San-Gemeinden aufgrund staatlich angeordneter Modernisierungsprogramme zur Landwirtschaft.
Bestimmte San-Gruppen sind eine von 14 bekannten, noch existierenden „Vorfahren-Bevölkerungsgruppen“; das heißt, „Gruppen von Bevölkerungsgruppen mit gemeinsamer genetischer Abstammung, die ethnische Zugehörigkeit und Ähnlichkeiten sowohl in ihrer Kultur als auch in den Eigenschaften ihrer Sprachen teilen“.
Vertreter der San-Völker erklärten im Jahr 2003, dass sie, soweit möglich, die Verwendung solcher individueller Gruppennamen der Verwendung des Sammelbegriffs San vorziehen würden.
Ich benutzte weiterhin Bushman und wurde mehrmals von den Gerechten öffentlich korrigiert.
Stattdessen bestand der Vertreter des San Council darauf, dass ihnen oder der San-Gemeinschaft durch die Art und Weise, in der (Die Burger) das Wort „Boesman“ veröffentlichte, kein Schaden oder Schaden zugefügt wurde.“
Die San-Verwandtschaft ist mit der Eskimo-Verwandtschaft vergleichbar, mit den gleichen Begriffen wie in europäischen Kulturen, verwendet aber auch eine Namensregel und eine Altersregel.
Kinder haben außer dem Spielen keine sozialen Pflichten und Freizeit ist für San jeden Alters sehr wichtig.
Sie treffen wichtige Familien- und Gruppenentscheidungen und beanspruchen das Eigentum an Wasserlöchern und Nahrungsgebieten.
Dürren können viele Monate andauern und Wasserstellen können austrocknen.
In dieses Loch wird ein langer, hohler Grashalm eingeführt.
Der frühe Frühling ist die härteste Jahreszeit: eine heiße Trockenperiode, die auf den kühlen, trockenen Winter folgt.
Frauen sammeln Obst, Beeren, Knollen, Buschzwiebeln und anderes Pflanzenmaterial für den Verzehr der Band.
Je nach Standort fressen die San 18 bis 104 Arten, darunter Heuschrecken, Käfer, Raupen, Motten, Schmetterlinge und Termiten.
Diese Haplogruppen sind spezifische Untergruppen der Haplogruppen A und B, den beiden frühesten Zweigen des menschlichen Y-Chromosomenbaums.
Die am stärksten divergierende (älteste) mitochondriale Haplogruppe, L0d, wurde mit ihrer höchsten Häufigkeit in den südafrikanischen San-Gruppen identifiziert.
Die San sind besonders vom Eindringen von Mehrheitsvölkern und nicht-indigenen Bauern in Land betroffen, das traditionell von der San-Bevölkerung bewohnt wird.
Der Landverlust trägt wesentlich zu den Problemen bei, mit denen die indigene Bevölkerung Botswanas konfrontiert ist, insbesondere die Vertreibung der San aus dem Central Kalahari Game Reserve.
Dadurch würden den San Lizenzgebühren für den Nutzen ihres indigenen Wissens zuerkannt.
Van der Post wuchs in Südafrika auf und hatte ein Leben lang eine respektvolle Faszination für die einheimischen afrikanischen Kulturen.
Angetrieben von einer lebenslangen Faszination für diesen „verschwundenen Stamm“ veröffentlichte Van der Post 1958 ein Buch über diese Expedition mit dem Titel „The Lost World of the Kalahari“.
Sein früher Film The Hunters aus dem Jahr 1957 zeigt eine Giraffenjagd.
Seine Schwester Elizabeth Marshall Thomas schrieb mehrere Bücher und zahlreiche Artikel über die San, teilweise basierend auf ihren Erfahrungen mit diesen Menschen, als ihre Kultur noch intakt war.
Dies wurde von Lawrence Van Gelder für die New York Times rezensiert, der sagte, dass der Film „einen Akt der Bewahrung und ein Requiem darstellt“.
Die BBC-Serie „The Life of Mammals“ (2003) enthält Videoaufnahmen eines indigenen San der Kalahari-Wüste, der unter rauen Wüstenbedingungen eine Dauerjagd auf einen Kudu unternimmt.
Aufgrund ihrer Ähnlichkeit könnten die San-Werke die Gründe für antike Höhlenmalereien veranschaulichen.
Regisseur des Films war Jamie Uys, der ein Jahrzehnt später mit „The Gods Must Be Crazy“ in die USA zurückkehrte, was sich als internationaler Hit erwies.
James A. Micheners The Covenant (1980) ist ein historisches Romanwerk mit Schwerpunkt auf Südafrika.
In Norman Rushs Roman Mating aus dem Jahr 1991 geht es um ein Lager von Basarwa in der Nähe der (imaginären) Stadt in Botswana, in der die Haupthandlung spielt.
Im Jahr 2007 veröffentlichte David Gilman „The Devil's Breath“.
Der Verlobte des Protagonisten von The No.
Die Germanen waren eine historische Völkergruppe, die in Mitteleuropa und Skandinavien lebte.
In Diskussionen über die Römerzeit werden die germanischen Völker manchmal als Germanen oder Altgermanen bezeichnet, obwohl viele Gelehrte den zweiten Begriff für problematisch halten, da er eine Identität mit modernen Germanen suggeriert.
Im Gegensatz dazu beschrieben römische Autoren erstmals germanische Völker in der Nähe des Rheins, als das Römische Reich seine Vorherrschaft in dieser Region etablierte.
Die römischen Bemühungen, das große Gebiet zwischen Rhein und Elbe zu integrieren, endeten um 16 n. Chr. nach der großen römischen Niederlage in der Schlacht im Teutoburger Wald im Jahr 9 n. Chr.
Im 3. Jahrhundert beherrschten die germanischsprachigen Goten die pontische Steppe außerhalb Germaniens und unternahmen eine Reihe von Seeexpeditionen auf dem Balkan und in Anatolien bis nach Zypern.
Die Archäologie zeigt stattdessen eine komplexe Gesellschaft und Wirtschaft in ganz Germanien.
Traditionell wurde angenommen, dass die germanischen Völker über ein Gesetz verfügten, das von den Konzepten der Fehde und der Blutsvergütung dominiert wurde.
Die alten germanischsprachigen Völker teilten wahrscheinlich eine gemeinsame poetische Tradition, alliterative Verse, und spätere germanische Völker teilten auch Legenden, die ihren Ursprung in der Völkerwanderungszeit hatten.
Sogar die Sprache, aus der es stammt, ist umstritten, mit Vorschlägen germanischen, keltischen, lateinischen und illyrischen Ursprungs.
Unabhängig von seiner Herkunftssprache wurde der Name über keltische Sprecher an die Römer weitergegeben.
Bis zur Spätantike wurden nur Völker in der Nähe des Rheins, insbesondere die Franken und manchmal auch die Alemannen, von lateinischen oder griechischen Schriftstellern als Germanen bezeichnet.
Während römische Autoren keltisch sprechende Völker nicht konsequent ausschlossen oder die germanischen Völker als Namen eines Volkes behandelten, verstand diese neue Definition, indem sie die germanische Sprache als Hauptkriterium verwendete, die Germanen als ein Volk oder eine Nation mit einer stabilen Gruppenidentität mit der Sprache verknüpft.
Einige Wissenschaftler, die sich mit dem Frühmittelalter befassen, betonen heute die Frage, ob sich die germanischen Völker als ethnische Einheit verstanden, während andere darauf hinweisen, dass die Existenz germanischer Sprachen eine historische Tatsache sei, die zur Identifizierung germanischer Völker herangezogen werden könne, unabhängig davon, ob sie sich selbst als ethnische Einheit sahen bezeichnen sich selbst als „germanisch“.
Aus diesen Gründen argumentiert Goffart, dass der Begriff „germanisch“ gänzlich zugunsten von „Barbar“ vermieden werden sollte, außer im sprachlichen Sinne, und Historiker wie Walter Pohl haben ebenfalls gefordert, den Begriff zu vermeiden oder mit sorgfältiger Erklärung zu verwenden.
In Caesars Bericht war das klarste charakteristische Merkmal des germanischen Volkes, dass es östlich des Rheins, gegenüber Gallien auf der Westseite, lebte, eine Beobachtung, die er in seinen Schriften mit historischen Abschweifungen machte.
Tacitus war sich zeitweise nicht sicher, ob ein Volk germanisch war oder nicht, und brachte seine Unsicherheit über die Bastarnae zum Ausdruck, die seiner Meinung nach wie Sarmaten aussahen, aber wie die Germanen sprachen, über die Osi und die Cotini und über die Aesti, die wie Suebi waren, aber sprachen eine andere Sprache.
Als südliche Grenze diente die Obere Donau.
Es ist unklar, ob diese Germanen eine germanische Sprache sprachen, möglicherweise waren sie stattdessen keltische Sprecher.
Tacitus erwähnt weiterhin germanische Stämme am Westufer des Rheins in der Zeit des frühen Reiches, wie die Tungri, Nemetes, Ubii und die Bataver.
Davon inspiriert werden diese drei Gruppen manchmal auch in der älteren modernen Sprachterminologie verwendet und versuchen, die Unterteilungen späterer germanischer Sprachen zu beschreiben.)
Zu den Herminonen oder Hermines im Inneren gehörten laut Plinius die Sueben, die Hermunduri, die Chatten und die Cherusker.
Andererseits schrieb Tacitus in derselben Passage, dass einige glauben, dass es andere Gruppen gibt, die genauso alt sind wie diese drei, darunter „die Marsi, Gambrivii, Suevi, Vandilii“.
Strabo, der sich hauptsächlich auf die Germanen zwischen Elbe und Rhein konzentrierte und die Söhne des Mannus nicht erwähnte, trennte auch die Namen der Germanen, die nicht Sueben sind, in zwei andere Gruppen, was ebenfalls drei Hauptunterteilungen impliziert: „kleinere germanische Stämme, als Cherusci, Chatti, Gamabrivi, Chattuarii und neben dem Ozean die Sicambri, Chaubi, Bructeri, Cimbri, Cauci, Caulci, Campsiani“.
Während der vorgermanischen Sprachperiode (2500–500 v. Chr.) wurde die Protosprache mit ziemlicher Sicherheit von sprachlichen Substraten beeinflusst, die noch immer in der germanischen Phonologie und im Lexikon erkennbar sind.
Es gibt auch einen großen Einfluss auf den Wortschatz aus den keltischen Sprachen, aber das meiste davon scheint viel später zu erfolgen, wobei die meisten Lehnwörter entweder vor oder während der im Grimmschen Gesetz beschriebenen Lautverschiebung auftauchen.
Obwohl das Protogermanische mithilfe der Vergleichsmethode ohne Dialekte rekonstruiert wird, ist es fast sicher, dass es nie eine einheitliche Protosprache war.
Die frühesten bezeugten Runeninschriften (Vimose-Kamm, Øvre Stabu-Speerspitze), die ursprünglich im modernen Dänemark konzentriert und mit dem Elder-Futhark-System geschrieben wurden, werden auf die zweite Hälfte des 2. Jahrhunderts n. Chr. datiert.
Die Verschmelzung unbetonter protogermanischer Vokale, die in Runeninschriften aus dem 4. und 5. Jahrhundert n. Chr. belegt ist, lässt jedoch auch darauf schließen, dass das Urnordische kein direkter Vorgänger westgermanischer Dialekte gewesen sein kann.
Bereits im späten 3. Jahrhundert n. Chr. kam es innerhalb des „restlichen“ nordwestlichen Dialektkontinuums zu sprachlichen Divergenzen wie dem westgermanischen Verlust des Endkonsonanten -z.
Die Einbeziehung der burgundischen und vandalischen Sprachen in die ostgermanische Gruppe ist zwar plausibel, aber aufgrund ihrer spärlichen Bezeugung noch ungewiss.
Eine Gesellschaft ist eine Gruppe von Individuen, die an ständiger sozialer Interaktion beteiligt sind, oder eine große soziale Gruppe, die sich dasselbe räumliche oder soziale Territorium teilt und typischerweise derselben politischen Autorität und denselben vorherrschenden kulturellen Erwartungen unterliegt.
Gesellschaften konstruieren Verhaltensmuster, indem sie bestimmte Handlungen oder Äußerungen als akzeptabel oder inakzeptabel erachten.
Soweit eine Gesellschaft kollaborativ ist, kann sie ihren Mitgliedern ermöglichen, auf eine Weise zu profitieren, die auf individueller Basis sonst schwierig wäre; Somit können sowohl individuelle als auch soziale (gemeinsame) Vorteile unterschieden werden, oder in vielen Fällen auch Überschneidungen festgestellt werden.
Dies wiederum leitet sich vom lateinischen Wort societas ab, das wiederum vom Substantiv socius („Kamerad, Freund, Verbündeter“; Adjektivform socialis) abgeleitet wurde, das eine Bindung oder Interaktion zwischen freundschaftlichen oder zumindest bürgerlichen Parteien beschreibt.
In den 1630er Jahren wurde es in Bezug auf „durch Nachbarschaft und Verkehr verbundene Menschen, die sich des Zusammenlebens in einer geordneten Gemeinschaft bewusst waren“ verwendet.
Abhängig vom kulturellen, geografischen und historischen Umfeld, mit dem sich diese Gesellschaften auseinandersetzen müssen, können diese Strukturen über unterschiedliche Grade politischer Macht verfügen.
Stammesgesellschaften, in denen es einige begrenzte Instanzen von sozialem Rang und Prestige gibt.
Diese kulturelle Entwicklung hat tiefgreifende Auswirkungen auf die Gemeinschaftsmuster.
Städte wurden zu Stadtstaaten und Nationalstaaten.
Umgekehrt können Mitglieder einer Gesellschaft auch Mitglieder der Gesellschaft meiden oder zum Sündenbock machen, die gegen ihre Normen verstoßen.
Einige Gesellschaften verleihen einer Einzelperson oder einer Gruppe von Menschen einen Status, wenn diese Einzelperson oder Gruppe eine bewunderte oder gewünschte Handlung ausführt.
Obwohl Menschen im Laufe der Geschichte viele Arten von Gesellschaften gegründet haben, neigen Anthropologen dazu, verschiedene Gesellschaften danach zu klassifizieren, inwieweit verschiedene Gruppen innerhalb einer Gesellschaft ungleichen Zugang zu Vorteilen wie Ressourcen, Prestige oder Macht haben.
Allerdings lebten einige Jäger- und Sammlergesellschaften in Gebieten mit reichlichen Ressourcen (wie die Tlingit-Menschen) in größeren Gruppen und bildeten komplexe hierarchische soziale Strukturen wie das Häuptlingstum.
Der Status innerhalb des Stammes ist relativ gleich und Entscheidungen werden durch allgemeine Zustimmung getroffen.
Es gibt keine politischen Ämter mit wirklicher Macht, und ein Chef ist lediglich eine einflussreiche Person, eine Art Berater; Daher sind Stammeszusammenschlüsse für kollektives Handeln nicht staatlich.
Da ihre Nahrungsmittelversorgung weitaus zuverlässiger ist, können Hirtengesellschaften größere Bevölkerungsgruppen ernähren.
Manche Menschen werden zum Beispiel Handwerker und stellen unter anderem Werkzeuge, Waffen und Schmuck her.
Diese Familien gewinnen oft durch ihren gestiegenen Reichtum an Macht.
Die wilde Vegetation wird abgeholzt und verbrannt, die Asche wird als Dünger verwendet.
Möglicherweise kehren sie einige Jahre später in das ursprüngliche Land zurück und beginnen den Prozess erneut.
Die Größe der Bevölkerung eines Dorfes hängt von der Menge an Land ab, das für die Landwirtschaft zur Verfügung steht. Daher können Dörfer zwischen 30 und 2000 Einwohnern umfassen.
Soziologen verwenden den Ausdruck Agrarrevolution, um sich auf die technologischen Veränderungen zu beziehen, die bereits vor 8.500 Jahren stattfanden und zum Anbau von Nutzpflanzen und zur Aufzucht von Nutztieren führten.
In Agrargesellschaften kam es zu einem höheren Grad sozialer Schichtung.
Als sich die Lebensmittelgeschäfte jedoch verbesserten und Frauen bei der Versorgung der Familie mit Nahrungsmitteln eine geringere Rolle spielten, wurden sie zunehmend den Männern untergeordnet.
Es entstand auch ein System von Herrschern mit hohem sozialen Status.
Die Erkundung Amerikas durch Europa war ein Anstoß für die Entwicklung des Kapitalismus.
Dies führte zu weiteren drastischen Effizienzsteigerungen.
Dieser größere Überschuss führte dazu, dass alle zuvor in der Domestikationsrevolution diskutierten Veränderungen noch deutlicher hervortraten.
Allerdings wurde die Ungleichheit noch größer als zuvor.
Geografisch umfasst es mindestens die Länder Westeuropa, Nordamerika, Australien und Neuseeland.
Ein Interessengebiet der Europäischen Union ist die Informationsgesellschaft.
Einige akademische, berufliche und wissenschaftliche Vereinigungen bezeichnen sich selbst als Gesellschaften (z. B. die American Mathematical Society, die American Society of Civil Engineers oder die Royal Society).
Eine Gemeinschaft ist eine soziale Einheit (eine Gruppe von Lebewesen) mit Gemeinsamkeiten wie Normen, Religion, Werten, Bräuchen oder Identität.
In diesem Sinne ist es gleichbedeutend mit dem Konzept einer antiken Siedlung – sei es ein Weiler, ein Dorf, eine Stadt oder eine Großstadt.
Die meisten Rekonstruktionen sozialer Gemeinschaften durch Archäologen basieren auf dem Prinzip, dass soziale Interaktion in der Vergangenheit durch physische Distanz bedingt war.
Keine Gruppe ist ausschließlich das eine oder das andere.
Die Sozialisation wird vor allem durch die Familie beeinflusst, durch die Kinder zunächst Gemeinschaftsnormen erlernen.
Fachleute für Gemeindeentwicklung müssen verstehen, wie sie mit Einzelpersonen arbeiten und wie sie die Positionen von Gemeinden im Kontext größerer sozialer Institutionen beeinflussen können.
An der Schnittstelle zwischen Community-Entwicklung und Community-Aufbau gibt es eine Reihe von Programmen und Organisationen mit Community-Entwicklungstools.
Leere: Geht über die Versuche hinaus, das Chaosstadium zu reparieren, zu heilen und zu bekehren, wenn alle Menschen in der Lage werden, ihre eigene Verwundung und Zerbrochenheit anzuerkennen, die allen Menschen gemeinsam ist.
Die drei grundlegenden Arten der Gemeinschaftsorganisation sind Basisorganisation, Koalitionsbildung und „institutionelle Gemeinschaftsorganisation“ (auch „breitenbasierte Gemeinschaftsorganisation“ genannt; ein Beispiel hierfür ist glaubensbasierte Gemeinschaftsorganisation oder kongregationsbasierte Gemeinschaft). Organisieren).
Abgerufen am: 22. Juni 2008.
Community Organizing kann sich auf mehr als nur die Lösung spezifischer Probleme konzentrieren.
Solche Gruppen erleichtern und fördern die Entscheidungsfindung im Konsens, wobei der Schwerpunkt eher auf der allgemeinen Gesundheit der Gemeinschaft als auf einer bestimmten Interessengruppe liegt.
Identitätsbasierte Gemeinschaften: reichen von lokalen Cliquen, Subkulturen, ethnischen Gruppen, religiösen, multikulturellen oder pluralistischen Zivilisationen bis hin zu den globalen Gemeinschaftskulturen von heute.
Die Beziehungen zwischen Mitgliedern einer virtuellen Community konzentrieren sich in der Regel auf den Informationsaustausch zu bestimmten Themen.
Geisteswissenschaftler sind „Geisteswissenschaftler“ oder Humanisten.
Die Geisteswissenschaften untersuchen im Allgemeinen lokale Traditionen anhand ihrer Geschichte, Literatur, Musik und Kunst, wobei der Schwerpunkt auf dem Verständnis bestimmter Personen, Ereignisse oder Epochen liegt.
Die Anthropologie passt (wie einige Bereiche der Geschichte) nicht ohne weiteres in eine dieser Kategorien, und verschiedene Zweige der Anthropologie stützen sich auf einen oder mehrere dieser Bereiche.
Das Wort Anthropos (άνθρωπος) leitet sich vom griechischen Wort für „Mensch“ oder „Person“ ab.
Das bedeutet, dass Anthropologen zwar im Allgemeinen nur auf ein Teilgebiet spezialisiert sind, aber stets die biologischen, sprachlichen, historischen und kulturellen Aspekte jedes Problems im Auge behalten.
Das Streben nach Ganzheitlichkeit führt die meisten Anthropologen dazu, ein Volk im Detail zu untersuchen und dabei neben der direkten Beobachtung zeitgenössischer Bräuche auch biogene, archäologische und sprachliche Daten zu nutzen.
Die Archäologie kann sowohl als Sozialwissenschaft als auch als Zweig der Geisteswissenschaften betrachtet werden.
Ein Großteil der Philosophie des 20. und 21. Jahrhunderts widmete sich der Analyse der Sprache und der Frage, ob, wie Wittgenstein behauptete, viele unserer philosophischen Verwirrungen auf das von uns verwendete Vokabular zurückzuführen sind; Die Literaturtheorie hat die rhetorischen, assoziativen und ordnenden Merkmale der Sprache untersucht; und historische Linguisten haben die Entwicklung von Sprachen im Laufe der Zeit untersucht.
Es wurde definiert als ein „System von Regeln“, als ein „interpretierendes Konzept“, um Gerechtigkeit zu erreichen, als eine „Autorität“, um die Interessen der Menschen zu vermitteln, und sogar als „das Kommando eines Souveräns, gestützt durch die Androhung einer Sanktion“. .
Gesetze sind Politik, weil Politiker sie schaffen.
Wie Immanuel Kant bemerkte: „Die antike griechische Philosophie war in drei Wissenschaften unterteilt: Physik, Ethik und Logik.“
Shintoismus, Daoismus und andere Volks- oder Naturreligionen haben keine ethischen Kodizes.
Glaubenssysteme implizieren ein logisches Modell, das Religionen aufgrund ihrer inneren Widersprüche, fehlender Beweise und Unwahrheiten nicht aufweisen. .
Sie sind notwendig, um die missliche Lage der Menschheit zu verstehen.
Die nicht gegründeten Religionen sind Hinduismus, Shintoismus und einheimische oder Volksreligionen.
Wenn traditionelle Religionen es versäumen, auf neue Anliegen einzugehen, werden neue Religionen entstehen.
Die darstellenden Künste werden auch von Arbeitnehmern in verwandten Bereichen wie Songwriting und Bühnenkunst unterstützt.
Das nennt man Performance-Kunst.
Tanz wird auch verwendet, um Methoden der nonverbalen Kommunikation (siehe Körpersprache) zwischen Menschen oder Tieren (Bienentanz, Paarungstanz) und Bewegung in unbelebten Objekten (im Wind tanzende Blätter) zu beschreiben.
In der byzantinischen und gotischen Kunst des Mittelalters bestand die Dominanz der Kirche auf dem Ausdruck biblischer und nicht materieller Wahrheiten.
Charakteristisch für diesen Stil ist, dass das Lokalkolorit oft durch einen Umriss definiert wird (ein zeitgenössisches Äquivalent ist der Cartoon).
Im Allgemeinen geht es darum, Markierungen auf einer Oberfläche zu machen, indem man mit einem Werkzeug Druck ausübt oder ein Werkzeug über eine Oberfläche bewegt.
Im künstlerischen Sinne bedeutet es jedoch die Verwendung dieser Tätigkeit in Kombination mit Zeichnung, Komposition und anderen ästhetischen Überlegungen, um die ausdrucksstarke und konzeptionelle Absicht des Praktizierenden zu manifestieren.
Schwarz wird im Westen mit Trauer in Verbindung gebracht, anderswo jedoch möglicherweise Weiß.
Das Wort „Rot“ kann beispielsweise ein breites Spektrum an Variationen des reinen Rots des Spektrums abdecken.
Dies begann mit dem Kubismus und ist keine Malerei im engeren Sinne.
Folglich verbringen viele die ersten Jahre nach ihrem Abschluss damit, zu entscheiden, was sie als nächstes tun wollen, was zu geringeren Einkommen zu Beginn ihrer Karriere führt; währenddessen erleben Absolventen berufsorientierter Studiengänge einen schnelleren Einstieg in den Arbeitsmarkt.
Die empirischen Belege zeigen jedoch auch, dass Absolventen der Geisteswissenschaften immer noch ein deutlich höheres Einkommen erzielen als Arbeitnehmer ohne postsekundäre Ausbildung und eine mit der Arbeitszufriedenheit ihrer Kollegen aus anderen Fachbereichen vergleichbare Arbeitszufriedenheit aufweisen.
Der Anteil der Geisteswissenschaften an der Art der verliehenen Abschlüsse scheint jedoch rückläufig zu sein.
Bundesmittel machen einen viel geringeren Anteil der Mittel für Geisteswissenschaften aus als andere Bereiche wie MINT oder Medizin.
Sie behaupteten, dieses Verständnis binde gleichgesinnte Menschen mit ähnlichem kulturellen Hintergrund zusammen und vermittle ein Gefühl der kulturellen Kontinuität mit der philosophischen Vergangenheit.
Abgesehen von ihrer gesellschaftlichen Anwendung ist die narrative Imagination ein wichtiges Werkzeug bei der (Re-)Produktion verstandener Bedeutung in Geschichte, Kultur und Literatur.
Der Poststrukturalismus hat einen Ansatz zur humanistischen Forschung problematisiert, der auf Fragen der Bedeutung, Intentionalität und Autorschaft basiert.
Darüber hinaus kann kritisches Denken, obwohl es wohl ein Ergebnis humanistischer Ausbildung ist, auch in anderen Kontexten erworben werden.
Dieses Vergnügen steht im Gegensatz zur zunehmenden Privatisierung der Freizeit und der unmittelbaren Befriedigung, die für die westliche Kultur charakteristisch ist. Es entspricht damit den Anforderungen von Jürgen Habermas an die Missachtung des gesellschaftlichen Status und die rationale Problematisierung bislang unhinterfragter Bereiche, die für ein im bürgerlichen öffentlichen Raum stattfindendes Unterfangen notwendig sind.
Trotz vieler geisteswissenschaftlicher Argumente gegen die Geisteswissenschaften haben einige innerhalb der exakten Wissenschaften ihre Rückkehr gefordert.
Es ist gut, die Geschichte der Philosophie zu kennen.“
Kommunikation (aus dem Lateinischen communicare, was „teilen“ oder „in Beziehung stehen“ bedeutet) ist „eine offensichtliche Antwort auf die schmerzhaften Trennungen zwischen sich selbst und anderen, privat und öffentlich, sowie innerem Denken und äußerem Wort.“
Nachrichtenzusammensetzung (weitere interne oder technische Ausarbeitung dessen, was genau ausgedrückt werden soll).
Lärmquellen wie Naturkräfte und in einigen Fällen menschliche Aktivitäten (sowohl absichtlich als auch versehentlich) beginnen, die Qualität der Signale zu beeinflussen, die sich vom Sender zu einem oder mehreren Empfängern ausbreiten.
Interpretation und Sinngebung der vermuteten Originalbotschaft.
Beispiele für Vorsatz sind freiwillige, absichtliche Bewegungen wie Händeschütteln oder Augenzwinkern, aber auch unwillkürliche Bewegungen wie Schwitzen.
Ebenso umfassen geschriebene Texte nonverbale Elemente wie den Handschriftstil, die räumliche Anordnung von Wörtern und die Verwendung von Emoticons zur Vermittlung von Emotionen.
Einige der Funktionen der nonverbalen Kommunikation beim Menschen bestehen darin, die denotative Botschaft zu ergänzen und zu veranschaulichen, zu verstärken und hervorzuheben, zu ersetzen und zu ersetzen, zu kontrollieren und zu regulieren sowie ihr zu widersprechen.
Für eine vollständige Kommunikation müssen bei der persönlichen Interaktion alle nonverbalen Kanäle wie Körper, Gesicht, Stimme, Aussehen, Berührung, Entfernung, Timing und andere Umweltkräfte aktiviert werden.
„Nonverbale Verhaltensweisen können ein universelles Sprachsystem bilden.“
Das Erlernen von Sprachen findet normalerweise in der Kindheit des Menschen am intensivsten statt.
Konstruierte Sprachen wie Esperanto, Programmiersprachen und verschiedene mathematische Formalismen sind nicht unbedingt auf die Eigenschaften beschränkt, die menschliche Sprachen gemeinsam haben.
Die Eigenschaften der Sprache werden durch Regeln geregelt.
Entgegen der landläufigen Meinung gelten die Gebärdensprachen der Welt (z. B. die amerikanische Gebärdensprache) als verbale Kommunikation, da ihr Gebärdenvokabular, ihre Grammatik und andere sprachliche Strukturen allen erforderlichen Klassifizierungen als gesprochene Sprachen entsprechen.
Kommunikation ist somit ein Prozess, bei dem Bedeutung zugewiesen und vermittelt wird, um ein gemeinsames Verständnis zu schaffen.
Ein Kanal, an den Signale zur Übertragung angepasst werden.
Ein Ziel, an dem die Nachricht ankommt.
Kein Zuschuss für abweichende Zwecke.
Keine Berücksichtigung situativer Zusammenhänge.
Diese Handlungen können in einer der verschiedenen Kommunikationsarten viele Formen annehmen.
Syntaktik (formale Eigenschaften von Zeichen und Symbolen).
Angesichts dieser Schwächen schlug Barnlund (2008) ein transaktionales Kommunikationsmodell vor.
Diese zweite Kommunikationshaltung, die als konstitutives Modell oder konstruktivistische Sichtweise bezeichnet wird, konzentriert sich auf die Art und Weise, wie ein Individuum kommuniziert, als bestimmenden Faktor für die Art und Weise, wie die Nachricht interpretiert wird.
Die persönlichen Filter des Senders und die persönlichen Filter des Empfängers können je nach regionalen Traditionen, Kulturen oder Geschlecht variieren; Dies kann die beabsichtigte Bedeutung des Nachrichteninhalts verändern.
Obwohl das Modell so etwas wie Codebücher impliziert, sind sie nirgends im Modell vertreten, was zu vielen konzeptionellen Schwierigkeiten führt.
Unternehmen mit begrenzten Ressourcen entscheiden sich möglicherweise dafür, nur einige dieser Aktivitäten durchzuführen, während größere Organisationen möglicherweise das gesamte Kommunikationsspektrum nutzen.
Die Informationsumgebung ist die Gesamtheit von Einzelpersonen, Organisationen und Systemen, die Informationen sammeln, verarbeiten, verbreiten oder darauf reagieren.
In der verbalen zwischenmenschlichen Kommunikation werden zwei Arten von Nachrichten gesendet: eine Inhaltsnachricht und eine relationale Nachricht.
Dies ist die Untersuchung, wie Einzelpersonen erklären, was unterschiedliche Ereignisse und Verhaltensweisen verursacht.
Eine offene und ehrliche Kommunikation schafft eine Atmosphäre, die es den Familienmitgliedern ermöglicht, ihre Unterschiede sowie ihre Liebe und Bewunderung füreinander zum Ausdruck zu bringen.
Forscher entwickeln Theorien, um Kommunikationsverhalten zu verstehen.
Dazu gehört auch das Fehlen einer „wissensgerechten“ Kommunikation, die auftritt, wenn eine Person mehrdeutige oder komplexe juristische Wörter, medizinische Fachsprache oder Beschreibungen einer Situation oder Umgebung verwendet, die der Empfänger nicht versteht.
Ebenso können schlechte oder veraltete Geräte, insbesondere das Versäumnis des Managements, neue Technologien einzuführen, Probleme verursachen.
Beispiele könnten eine Organisationsstruktur sein, die unklar ist und es daher verwirrend macht, zu wissen, mit wem man kommunizieren soll.
Es ist besser, solche Wörter zu vermeiden und nach Möglichkeit Alternativen zu verwenden.
Kommunikationsforschung hat jedoch gezeigt, dass Verwirrung der Forschung Legitimität verleihen kann, wenn die Überzeugung scheitert.
Dies geschieht, wenn der Sender einen Gedanken oder ein Wort ausdrückt, der Empfänger ihm jedoch eine andere Bedeutung gibt.
Dies hat wiederum zu einer bemerkenswerten Veränderung in der Art und Weise geführt, wie jüngere Generationen kommunizieren und ihre eigene Selbstwirksamkeit wahrnehmen, um mit anderen zu kommunizieren und Kontakte zu knüpfen.
Angst vor Kritik – Dies ist ein wesentlicher Faktor, der eine gute Kommunikation verhindert.
Dies stärkt nicht nur Ihr Selbstvertrauen, sondern verbessert auch Ihre Sprache und Ihren Wortschatz.
Auch bestimmte Einstellungen können die Kommunikation erschweren.
Unter Begriffsklärung versteht man den Versuch, Rauschen und Fehlinterpretationen zu reduzieren, wenn der semantische Wert oder die Bedeutung eines Zeichens Rauschen unterliegen kann oder mehrere Bedeutungen vorhanden sind, was die Sinnfindung erschwert.
Beispielsweise haben Wörter, Farben und Symbole in verschiedenen Kulturen unterschiedliche Bedeutungen.
Das Verständnis kultureller Aspekte der Kommunikation bedeutet, dass man über Kenntnisse verschiedener Kulturen verfügt, um effektiv mit interkulturellen Menschen kommunizieren zu können.
Dazu gehören auch Kehlkopfgeräusche, die alle stark von kulturellen Unterschieden über die Grenzen hinweg beeinflusst werden.
Dieses Konzept unterscheidet sich von Kultur zu Kultur, da der zulässige Raum in verschiedenen Ländern unterschiedlich ist.
Einige Probleme, die dieses Konzept erklären, sind Pausen, Stille und Reaktionsverzögerungen während einer Interaktion.
In verschiedenen Ländern werden dieselben Gesten und Körperhaltungen verwendet, um unterschiedliche Botschaften zu vermitteln.
Pflanzenwurzeln kommunizieren mit Rhizombakterien, Pilzen und Insekten im Boden.
Parallel dazu produzieren sie andere flüchtige Stoffe, um Parasiten anzulocken, die diese Pflanzenfresser angreifen.
Die Biochemikalien veranlassen den Pilzorganismus zu einer spezifischen Reaktion. Wenn dieselben chemischen Moleküle jedoch nicht Teil biotischer Botschaften sind, lösen sie keine Reaktion des Pilzorganismus aus.
Durch Quorum Sensing können Bakterien die Dichte von Zellen erkennen und die Genexpression entsprechend regulieren.
Informationen sind im Allgemeinen verarbeitete, organisierte und strukturierte Daten.
Informationen werden mit Daten verknüpft.
Informationen können zeitlich, über Datenspeicherung und räumlich, über Kommunikation und Telekommunikation übermittelt werden.
Informationen können zur Übertragung und Interpretation in verschiedene Formen kodiert werden (Informationen können beispielsweise in eine Folge von Zeichen kodiert oder über ein Signal übertragen werden).
Die Unsicherheit ist umgekehrt proportional zur Eintrittswahrscheinlichkeit.
Darüber hinaus enthielt das Lateinische selbst bereits das Wort īnfōrmātiō mit der Bedeutung „Konzept“ oder „Idee“. Es ist jedoch nicht klar, inwieweit dies die Entwicklung des Wortes „Information“ im Englischen beeinflusst haben könnte.
Im modernen Griechisch wird das Wort Πληροφορία immer noch täglich verwendet und hat dieselbe Bedeutung wie das Wort Information im Englischen.
Das Fachgebiet wurde grundlegend durch die Arbeiten von Harry Nyquist und Ralph Hartley in den 1920er Jahren sowie Claude Shannon in den 1940er Jahren begründet.
Die Entropie quantifiziert den Grad der Unsicherheit, der mit dem Wert einer Zufallsvariablen oder dem Ergebnis eines Zufallsprozesses verbunden ist.
Wichtige Teilgebiete der Informationstheorie sind Quellenkodierung, algorithmische Komplexitätstheorie, algorithmische Informationstheorie und informationstheoretische Sicherheit.
In seinem Buch „Sensory Ecology“ nennt der Biophysiker David B. Dusenbery diese kausalen Inputs.
In der Praxis werden Informationen meist durch schwache Reize übertragen, die von speziellen Sinnessystemen erfasst und durch Energiezufuhr verstärkt werden müssen, bevor sie für den Organismus oder das System funktionsfähig sein können.
Die Nukleotidsequenz ist ein Muster, das die Entstehung und Entwicklung eines Organismus beeinflusst, ohne dass dafür ein Bewusstsein erforderlich ist.
Mit anderen Worten kann man sagen, dass Information in diesem Sinne etwas ist, das möglicherweise als Repräsentation wahrgenommen wird, obwohl es nicht zu diesem Zweck erstellt oder präsentiert wird.
Ob die Antwort Erkenntnisse liefert, hängt von der informierten Person ab.
Dies entspricht einem Informationsäquivalent von fast 61 CD-ROMs pro Person im Jahr 2007.
Das Sound Records Management stellt sicher, dass die Integrität der Aufzeichnungen so lange gewahrt bleibt, wie sie benötigt wird.
Beynon-Davies erklärt den vielschichtigen Informationsbegriff anhand von Zeichen und Signalzeichensystemen.
Die Pragmatik befasst sich mit dem Zweck der Kommunikation.
Mit anderen Worten: Pragmatik verbindet Sprache mit Handeln.
Semantik ist die Untersuchung der Bedeutung von Zeichen – der Verbindung zwischen Zeichen und Verhalten.
Der Bereich Syntax untersucht die Form der Kommunikation im Hinblick auf die Logik und Grammatik von Zeichensystemen.
Er führt das Konzept der lexikografischen Informationskosten ein und bezieht sich auf den Aufwand, den ein Wörterbuchbenutzer unternehmen muss, um Daten zunächst zu finden und dann zu verstehen, damit er Informationen generieren kann.
In einer Kommunikationssituation werden Absichten durch Nachrichten ausgedrückt, die Sammlungen miteinander verbundener Zeichen aus einer Sprache umfassen, die von den an der Kommunikation beteiligten Akteuren gegenseitig verstanden wird.
Die Informationsvisualisierung (kurz InfoVis) basiert auf der Berechnung und digitalen Darstellung von Daten und unterstützt Benutzer bei der Mustererkennung und Anomalieerkennung.
Der Begriff wird allgemein in der Soziologie und den anderen Sozialwissenschaften sowie in der Philosophie und Bioethik verwendet.
In sich entwickelnden Gesellschaften basiert es möglicherweise hauptsächlich auf Verwandtschaft und gemeinsamen Werten, während sich in weiter entwickelten Gesellschaften verschiedene Theorien darüber ansammeln, was zu einem Gefühl der Solidarität bzw. des sozialen Zusammenhalts beiträgt.
Durkheim führte die Begriffe mechanische und organische Solidarität als Teil seiner Theorie der Entwicklung von Gesellschaften in „Die Arbeitsteilung in der Gesellschaft“ (1893) ein.
Collins Dictionary of Sociology, S. 405-6.
Definition: Es handelt sich um sozialen Zusammenhalt, der auf der Abhängigkeit der Individuen voneinander in fortgeschritteneren Gesellschaften beruht.
Frühe antike Philosophen wie Sokrates und Aristoteles diskutieren Solidarität als einen tugendethischen Rahmen, denn um ein gutes Leben zu führen, muss man Handlungen und Verhaltensweisen ausführen, die in Solidarität mit der Gemeinschaft stehen.
Die moderne Praxis der Bioethik ist maßgeblich von Immanuel Kants Konzept des Kategorischen Imperativs beeinflusst.
Auslandsstudien gab es praktisch nicht.
Die ersteren wurden zu Befürwortern der Regionalstudien, die letzteren zu Befürwortern der Modernisierungstheorie.
Von 1953 bis 1966 spendete es 270 Millionen US-Dollar an 34 Universitäten für Landes- und Sprachstudien.
Weitere große und wichtige Programme folgten Fords.
Andere bestanden jedoch darauf, dass die Regionalstudien nach ihrer Etablierung auf dem Universitätsgelände eine viel umfassendere und tiefere intellektuelle Agenda umfassten als die von den Regierungsbehörden vorgesehene und daher nicht auf Amerika ausgerichtet seien.
Andere interdisziplinäre Forschungsbereiche wie Frauenstudien, Geschlechterstudien, Behindertenstudien, LGBT-Studien und ethnische Studien (einschließlich Afroamerikanistik, Asien-Amerikanistik, Latino-Studien, Chicano-Studien und Native American Studies) sind nicht Teil der Gebietsstudien, werden aber manchmal einbezogen im Gespräch dabei.
Demografie (vom Präfix demo aus dem Altgriechischen δῆμος (dēmos), was „das Volk“ bedeutet, und -graphy von γράφω (graphō), was „Schreiben, Beschreiben oder Messen“ bedeutet) ist die statistische Untersuchung von Bevölkerungsgruppen, insbesondere von Menschen.
Patientendemografien bilden den Kern der Daten für jede medizinische Einrichtung, z. B. Patienten- und Notfallkontaktinformationen sowie Daten zu Patientenakten.
Der Begriff Demographie bezieht sich auf die Gesamtstudie der Bevölkerung.
Im Mittelalter widmeten christliche Denker viel Zeit der Widerlegung der klassischen Vorstellungen zur Demographie.
Eine der frühesten demografischen Studien der Neuzeit war „Natural and Political Observations Made upon the Bills of Mortality“ (1662) von John Graunt, das eine primitive Form einer Sterbetafel enthält.
Seine Arbeit beeinflusste Thomas Robert Malthus, der Ende des 18. Jahrhunderts schrieb und befürchtete, dass das Bevölkerungswachstum, wenn es nicht kontrolliert würde, tendenziell das Wachstum der Nahrungsmittelproduktion übertreffen würde, was zu immer größerer Hungersnot und Armut führen würde (siehe Malthusianische Katastrophe).
Eine Volkszählung ist die andere gängige direkte Methode zur Erhebung demografischer Daten.
Nach einer Volkszählung werden Analysen durchgeführt, um abzuschätzen, wie viele Über- oder Unterzählungen stattgefunden haben.
Zu anderen indirekten Methoden in der zeitgenössischen Demografie gehört die Befragung von Menschen nach Geschwistern, Eltern und Kindern.
Dazu gehören Modelle der Sterblichkeit (einschließlich Sterbetafel, Gompertz-Modelle, Gefahrenmodelle, Cox-Proportional-Hazards-Modelle, Sterbetafeln mit mehreren Dekrementen, relationale Logits von Brass), Fruchtbarkeit (Hernes-Modell, Coale-Trussell-Modelle, Paritätsprogressionsverhältnisse) und Ehe (Singulieren). Mittelwert bei Heirat, Page-Modell), Behinderung (Sullivan-Methode, Multistate-Sterbetafeln), Bevölkerungsprognosen (Lee-Carter-Modell, Leslie-Matrix) und Bevölkerungsdynamik (Keyfitz).
Die altersspezifischen Fruchtbarkeitsraten, die jährliche Zahl der Lebendgeburten pro 1.000 Frauen in bestimmten Altersgruppen (in der Regel 15–19 Jahre, 20–24 Jahre usw.)
Die Lebenserwartung (oder Lebenserwartung), die Anzahl der Jahre, die eine Person in einem bestimmten Alter bei der gegenwärtigen Sterblichkeitsrate voraussichtlich leben wird.
Eine stationäre Bevölkerung, die sowohl stabil als auch in ihrer Größe unveränderlich ist (der Unterschied zwischen der rohen Geburtenrate und der rohen Sterberate beträgt Null).
Beachten Sie, dass die oben definierte rohe Sterblichkeitsrate bei Anwendung auf die gesamte Bevölkerung einen irreführenden Eindruck erwecken kann.
Bei Personen, die ihre ethnische Selbstbezeichnung ändern oder deren ethnische Klassifizierung in den Regierungsstatistiken sich im Laufe der Zeit ändert, kann man davon ausgehen, dass sie von einer Bevölkerungsunterkategorie in eine andere migrieren oder wechseln.
Die Abbildung in diesem Abschnitt zeigt die neuesten (2004) UN-Prognosen der Weltbevölkerung bis zum Jahr 2150 (rot u003d hoch, orange u003d mittel, grün u003d niedrig).
Unter Mortalität versteht man die Untersuchung der Ursachen, Folgen und Messung von Prozessen, die den Tod von Mitgliedern der Bevölkerung beeinflussen.
Migrationsforscher bezeichnen Bewegungen nicht als „Migrationen“, es sei denn, sie sind einigermaßen dauerhaft.
Demografie wird heute an vielen Universitäten auf der ganzen Welt umfassend gelehrt und zieht Studierende mit einer Grundausbildung in Sozialwissenschaften, Statistik oder Gesundheitswissenschaften an.
In dieser Hinsicht kann man die Informationswissenschaft als eine Antwort auf den technologischen Determinismus sehen, den Glauben, dass sich die Technologie „nach ihren eigenen Gesetzen entwickelt, dass sie ihr eigenes Potenzial ausschöpft, das nur durch die verfügbaren materiellen Ressourcen und die Kreativität ihrer Entwickler begrenzt ist“.
Es befasst sich mit dem Wissensbestand in Bezug auf die Entstehung, Sammlung, Organisation, Speicherung, den Abruf, die Interpretation, die Übertragung, die Umwandlung und die Nutzung von Informationen.
Dies gilt insbesondere im Zusammenhang mit dem Konzept, das A. I. Michailow und andere sowjetische Autoren Mitte der 1960er Jahre entwickelt haben.
In akademischen Informatikprogrammen tauchen Definitionen auf, die von der Art der Werkzeuge abhängen, die zur Ableitung sinnvoller Informationen aus Daten verwendet werden.
Es kann verwendet werden, um über die Entitäten innerhalb dieser Domäne nachzudenken und kann zur Beschreibung der Domäne verwendet werden.
Traditionell beschäftigen sie sich bei ihrer Arbeit mit gedruckten Materialien, doch diese Fähigkeiten werden zunehmend auch bei elektronischen, visuellen, Audio- und digitalen Materialien eingesetzt.
Institutionell entstand die Informationswissenschaft im 19. Jahrhundert zusammen mit vielen anderen sozialwissenschaftlichen Disziplinen.
Im Jahr 1731 gründete Benjamin Franklin die Library Company of Philadelphia, die erste Bibliothek im Besitz einer Gruppe öffentlicher Bürger, die schnell über den Bereich der Bücher hinaus expandierte und zu einem Zentrum wissenschaftlicher Experimente wurde und öffentliche Ausstellungen wissenschaftlicher Experimente veranstaltete.
Im Jahr 1801 erfand Joseph Marie Jacquard ein Lochkartensystem zur Steuerung des Webstuhls in Frankreich.
1843 entwickelte Richard Hoe die Rotationspresse und 1844 sendete Samuel Morse die erste öffentliche Telegrafennachricht.
Im Jahr 1860 fand an der Technischen Hochschule Karlsruhe ein Kongress statt, bei dem die Machbarkeit der Einführung einer systematischen und rationalen Nomenklatur für die Chemie erörtert wurde.
Im folgenden Jahr begann die Royal Society in London mit der Veröffentlichung ihres Catalogue of Papers.
Viele Historiker der Informationswissenschaft bezeichnen Paul Otlet und Henri La Fontaine als die Väter der Informationswissenschaft mit der Gründung des International Institute of Bibliography (IIB) im Jahr 1895.
Dokumentaristen betonten die utilitaristische Integration von Technologie und Technik für bestimmte soziale Ziele.
Otlet und Lafontaine gründeten zahlreiche Organisationen, die sich der Standardisierung, Bibliographie, internationalen Verbänden und damit der internationalen Zusammenarbeit widmeten.
Diese Sammlung umfasste standardisierte Papierblätter und Karten, die in maßgeschneiderten Schränken nach einem hierarchischen Index (der weltweit Informationen aus verschiedenen Quellen zusammensuchte) und einem kommerziellen Informationsabrufdienst (der schriftliche Anfragen beantwortete, indem relevante Informationen von Karteikarten kopiert wurden) abgelegt wurden.
Darüber hinaus begannen die traditionellen Grenzen zwischen den Disziplinen zu verschwinden und viele Wissenschaftler der Informationswissenschaft schlossen sich anderen Programmen an.
In den 1980er Jahren entstanden auch zahlreiche Interessengruppen, um auf die Veränderungen zu reagieren.
Zhang, B., Semenov, A., Vos, M. und Veijlainen, J. (2014).
Das Teilen über soziale Medien ist so einflussreich geworden, dass Verlage „nett spielen“ müssen, wenn sie erfolgreich sein wollen.
Aus diesem Grund wurde das Potenzial dieser Netzwerke erkannt. "
Wie sieht es mit der Zuweisung von Berechtigungen und der Beschränkung des Zugriffs auf nicht autorisierte Benutzer aus?
Es handelt sich um eine aufstrebende Disziplin und Praxisgemeinschaft, die sich darauf konzentriert, Design- und Architekturprinzipien in der digitalen Landschaft zusammenzuführen.
Automatisierte Informationsabrufsysteme werden eingesetzt, um die sogenannte „Informationsüberflutung“ zu reduzieren.
Ein Informationsabrufprozess beginnt, wenn ein Benutzer eine Abfrage in das System eingibt.
Stattdessen können mehrere Objekte mit der Abfrage übereinstimmen, möglicherweise mit unterschiedlichem Relevanzgrad.
Je nach Anwendung kann es sich bei den Datenobjekten beispielsweise um Textdokumente, Bilder, Audiodateien, Mindmaps oder Videos handeln.
Die Suche nach Informationen hängt mit dem Informationsabruf (Information Retrieval, IR) zusammen, unterscheidet sich jedoch von diesem.
Mithilfe der Logik wird eine formale Semantik dafür bereitgestellt, wie Argumentationsfunktionen auf die Symbole im KR-System angewendet werden sollten.
Man glaubte auch allgemein, dass Naturkatastrophen wie Hungersnöte und Überschwemmungen göttliche Vergeltungsmaßnahmen seien, die Zeichen des Unmuts des Himmels über den Herrscher seien. Deshalb kam es nach größeren Katastrophen oft zu Aufständen, da die Menschen diese Katastrophen als Zeichen dafür ansahen, dass das Mandat des Himmels erfüllt worden sei zurückgezogen.
Das Konzept ähnelt in mancher Hinsicht dem europäischen Konzept des göttlichen Rechts der Könige; Anders als das europäische Konzept verleiht es jedoch kein unbedingtes Herrschaftsrecht.
Das Mandat des Himmels wurde von Philosophen und Gelehrten in China oft als Mittel zur Eindämmung des Machtmissbrauchs durch den Herrscher in einem System angeführt, das nur wenige andere Kontrollen hatte.
Bemerkenswert ist, dass die Dynastie eine beträchtliche Zeit währte, in der 31 Könige über einen längeren Zeitraum von 17 Generationen herrschten.
Im Laufe der Zeit führte der Missbrauch der anderen sozialen Klassen durch die Herrscher jedoch zu sozialer Unruhe und Instabilität.
Sie schufen das Mandat des Himmels, um ihr Recht zu erklären, die Herrschaft zu übernehmen, und gingen davon aus, dass die einzige Möglichkeit, das Mandat zu erfüllen, darin bestehe, in den Augen des Himmels gut zu regieren.
Um jedoch einige Bürger zu besänftigen, erlaubten sie einigen Shang-Begünstigten, ihre kleinen Königreiche weiterhin in Übereinstimmung mit den Zhou-Regeln und -Vorschriften zu regieren.
Sie leisteten auch hervorragende Leistungen im Schiffbau, was sie zusammen mit ihrer Entdeckung der Himmelsnavigation zu hervorragenden Seefahrern machte.
Die meisten dieser Werke sind Kommentare zum Fortschritt und zur politischen Bewegung der Dynastie.
In ihren Werken betonten sie vor allem die Bedeutung der herrschenden Klasse, des Respekts und ihrer Beziehung zur Unterschicht.
Innerhalb dieser Bezirke gab es Administratoren, die von der Regierung ernannt wurden. Im Gegenzug mussten sie ihre Loyalität gegenüber der wichtigsten internen Regierung aufrechterhalten.
Als schließlich die Macht der Zhou-Dynastie nachließ, wurde sie vom Staat Qin ausgelöscht, der glaubte, die Zhou seien schwach und ihre Herrschaft ungerecht geworden.
Während dieser Reformation wurden Verwaltungsänderungen vorgenommen und ein System des Legalismus entwickelt, das besagte, dass das Gesetz über jedem Einzelnen, einschließlich der Herrscher, steht.
Die Gründung der Han-Dynastie markierte einen großen Zeitraum in der Geschichte Chinas, der von bedeutenden Veränderungen in der politischen Struktur des Landes geprägt war.
Ein Hauptzweck bestand darin, eine Rechtfertigung für die Übertragung des Mandats des Himmels durch diese fünf Dynastien und damit auf die Song-Dynastie zu finden.
Sie besaßen auch wesentlich mehr Territorium als alle anderen chinesischen Staaten, die im Süden angrenzend existierten.
Das brutale Verhalten von Zhu Wen und dem späteren Liang war eine Quelle erheblicher Verlegenheit, und daher bestand der Druck, sie aus dem Mandat auszuschließen.
Allerdings war Kublai Khan der einzige gleichgültige Herrscher, als er das Mandat des Himmels über die Yuan-Dynastie beanspruchte, da er über ein beträchtliches Militär verfügte und Teil des Khitan-Volkes war, wie viele andere mit demselben Hintergrund, da sie nicht dieselben Traditionen hatten und Kultur als ihre chinesischen Gegner.
Es war von Anfang bis Ende reine Politik und ein Versuch des Kaisers, eine wohlwollende Haltung gegenüber dem Himmel aufrechtzuerhalten.
Das Recht auf Rebellion ist in keinem offiziellen Gesetz verankert.
Da der Gewinner derjenige ist, der bestimmt, wer das Mandat des Himmels erhalten und wer es verloren hat, betrachten einige chinesische Gelehrte es als eine Art Siegerjustiz, die am besten durch das beliebte chinesische Sprichwort „Der Gewinner wird König, der Verlierer“ charakterisiert wird Gesetzloser“ (Chinesisch: „成者爲王，敗者爲寇“).
Auch das Königreich Silla soll das Himmelsmandat übernommen haben, die frühesten Aufzeichnungen stammen jedoch aus der Joseon-Dynastie, die das Himmelsmandat zu einer dauerhaften Staatsideologie machte.
Die späteren und stärker zentralisierten vietnamesischen Dynastien übernahmen den Konfuzianismus als Staatsideologie, was zur Schaffung eines vietnamesischen Tributsystems in Südostasien führte, das dem chinesischen sinozentrischen System in Ostasien nachempfunden war.
In späteren Zeiten wurde diese Notwendigkeit überflüssig, da das japanische Kaiserhaus behauptete, in einer ununterbrochenen Linie von der japanischen Sonnengöttin Amaterasu abzustammen.
Selbst nach der Meiji-Restauration im Jahr 1868, als der Kaiser wieder in die Mitte der politischen Bürokratie gerückt wurde, hatte der Thron selbst nur sehr wenig Macht gegenüber der Meiji-Oligarchie.
Medienwissenschaft ist eine Disziplin und Studienrichtung, die sich mit den Inhalten, der Geschichte und den Wirkungen verschiedener Medien beschäftigt; insbesondere die Massenmedien.
Die Medienwissenschaft wurde in Australien erstmals in den frühen 1960er Jahren als Studienfach an viktorianischen Universitäten und Mitte der 1960er Jahre an weiterführenden Schulen entwickelt.
An weiterführenden Schulen wurde ab Mitte der 1960er Jahre erstmals ein Kurs für frühe Filmstudien als Teil des Lehrplans der viktorianischen Mittelstufe unterrichtet.
Seitdem ist es ein wichtiger Bestandteil des VCE geworden und bleibt es auch weiterhin.
Medienwissenschaften werden im Bundesstaat New South Wales offenbar nicht im Sekundarbereich unterrichtet.
Harold Innis und Marshall McLuhan sind berühmte kanadische Wissenschaftler für ihre Beiträge zu den Bereichen Medienökologie und politische Ökonomie im 20. Jahrhundert.
Die Carleton University und die University of Western Ontario schufen voraussichtlich 1945 und 1946 journalistische Programme oder Schulen.
Heutzutage bieten die meisten Universitäten Bachelor-Abschlüsse in Medien- und Kommunikationswissenschaften an, und viele kanadische Wissenschaftler leisten einen aktiven Beitrag auf diesem Gebiet, darunter: Brian Massumi (Philosophie, Kulturwissenschaften), Kim Sawchuk (Kulturwissenschaften, Feministin, Altersstudien), Carrie Rentschler ( feministische Theorie) und François Cooren (Organisationskommunikation).
Ein Medium ist alles, was unsere Interaktion mit der Welt oder anderen Menschen vermittelt.
McLuhan sagt, dass „die Technik der Fragmentierung, die das Wesen der Maschinentechnologie ist“, die Umstrukturierung menschlicher Arbeit und Assoziation geprägt hat und „das Wesen der Automatisierungstechnik das Gegenteil ist“.
Das Merkmal aller Medien bedeutet, dass der „Inhalt“ eines Mediums immer ein anderes Medium ist.
Wenn das elektrische Licht für Fußball am Freitagabend oder zur Beleuchtung Ihres Schreibtisches verwendet wird, könnten Sie argumentieren, dass der Inhalt des elektrischen Lichts diese Aktivitäten sind.
Erst wenn elektrisches Licht verwendet wird, um einen Markennamen zu buchstabieren, wird es als Medium erkannt.
Die Wirkung des Mediums wird dadurch verstärkt, dass ihm ein anderer medialer „Inhalt“ gegeben wird.
Heiße Medien haben eine geringe Beteiligung und coole Medien eine hohe Beteiligung.
Die Communication University of China, früher bekannt als Beijing Broadcasting Institute, stammt aus dem Jahr 1954.
Bourdieus Analyse ist, dass das Fernsehen weitaus weniger Autonomie oder Freiheit bietet, als wir denken.
Auch im Bereich der Filmwissenschaft waren sowohl Frankfurt als auch Berlin dominant bei der Entwicklung neuer Perspektiven auf Bewegtbildmedien.
Eine der frühen Veröffentlichungen in dieser neuen Richtung ist der von Helmut Kreuzer herausgegebene Band „Literaturwissenschaft – Medienwissenschaft“, der die Vorträge des Düsseldorfer Germanistentags 1976 zusammenfasst.
Das 2005 vom Medienwissenschaftler Lutz Hachmeister gegründete Deutsche Institut für Medien- und Kommunikationspolitik ist eine der wenigen unabhängigen Forschungseinrichtungen, die sich Fragen der Medien- und Kommunikationspolitik widmet.
Medienwissenschaften gehören derzeit zu den beliebtesten Studiengängen an Universitäten in Deutschland, wobei viele Bewerber fälschlicherweise davon ausgehen, dass ein Studium automatisch zu einer Karriere im Fernsehen oder anderen Medien führt.
Es bietet ein fünfjähriges integriertes Programm und ein zweijähriges Programm für elektronische Medien an.
Während sich die Kommunikationswissenschaften auf die Art und Weise konzentrieren, wie Menschen kommunizieren, sei es vermittelt oder unvermittelt, tendiert die Medienwissenschaft dazu, die Kommunikation auf lediglich vermittelte Kommunikation einzugrenzen.
Kommunikationswissenschaften (oder eine Ableitung davon) können an der Erasmus-Universität Rotterdam, der Radboud-Universität, der Universität Tilburg, der Universität Amsterdam, der Universität Groningen, der Universität Twente, der Roosevelt Academy, der Universität Utrecht, der VU-Universität Amsterdam und dem Universitäts- und Forschungszentrum Wageningen studiert werden .
Die University of the Punjab Lahore ist die älteste Fakultät.
Mittlerweile wird in ganz Großbritannien Medienwissenschaft gelehrt.
Der Schwerpunkt solcher Programme schließt jedoch manchmal bestimmte Medien aus – Filme, Buchveröffentlichungen, Videospiele usw.
Dies ist zum Teil der Übernahme von Professor Siva Vaidhyanathan, einem Kulturhistoriker und Medienwissenschaftler, sowie der ersten Verklin Media Policy and Ethics Conference zu verdanken, die vom CEO von Canoe Ventures und UVA-Alumnus David Verklin gestiftet wurde.
Ein Hauptfach Medienwissenschaft in Radford bedeutet immer noch, dass sich jemand auf Journalismus, Rundfunk, Werbung oder Webproduktion konzentriert.)
Bergson stellte einer offenen Gesellschaft das gegenüber, was er eine geschlossene Gesellschaft nannte, ein geschlossenes System aus Recht, Moral oder Religion.
Soros, George, „Das Zeitalter der Fehlbarkeit“, Public Affairs (2006).
Der Totalitarismus zwang das Wissen dazu, politisch zu werden, was kritisches Denken unmöglich machte und zur Zerstörung des Wissens in totalitären Ländern führte.
In der geschlossenen Gesellschaft führt der Anspruch auf bestimmtes Wissen und die ultimative Wahrheit zum Versuch, eine Version der Realität aufzuzwingen.
Da die Wahrnehmung der Realität durch die Wähler leicht manipuliert werden kann, führt ein demokratischer politischer Diskurs nicht unbedingt zu einem besseren Verständnis der Realität.
Popper identifizierte die offene Gesellschaft jedoch weder mit Demokratie noch mit Kapitalismus oder einer Laissez-faire-Wirtschaft, sondern eher mit einer kritischen Geisteshaltung des Einzelnen gegenüber gemeinschaftlichen Gruppendenken welcher Art auch immer.
Regulierungskollegien sind juristische Personen, deren Aufgabe es ist, dem öffentlichen Interesse zu dienen, indem sie die Ausübung eines Berufs regulieren.
Beispielsweise darf in Ontario kein Arbeitnehmer in einem Pflichtgewerbe arbeiten, ohne Mitglied im Ontario College of Trades zu sein.
Für Weber ist Soziologie das Studium der Gesellschaft und des Verhaltens und muss daher auf den Kern der Interaktion blicken.
Der Begriff ist praktischer und umfassender als Florian Znanieckis „soziale Phänomene“, da das Individuum, das soziales Handeln ausführt, nicht passiv, sondern aktiv und reaktiv ist.
Dies gilt auch als alternatives Mittel, wenn die Nebenfolgen beendet sind.
Wenn sich der Student dafür entscheidet, im College keine guten Leistungen zu erbringen, weiß er, dass es schwierig sein wird, Jura zu studieren und letztendlich das Ziel zu erreichen, Anwalt zu werden.
Value Relation gliedert sich in die Untergruppen Gebote und Forderungen.
Diese Forderungen haben mehrere Probleme aufgeworfen, selbst der Rechtsformalismus wurde auf die Probe gestellt.
In dem Maße, in dem es viele religiöse Unternehmen gibt, die miteinander konkurrieren, tendieren sie dazu, sich zu spezialisieren und auf die besonderen Bedürfnisse einiger Segmente religiöser Verbraucher einzugehen.
Es ist bekannt, dass strenge Kirchen in den heutigen Vereinigten Staaten stark sind und wachsen, während liberale Kirchen im Niedergang begriffen sind.
Affektive Handlungen (auch als emotionale Handlungen bekannt): Handlungen, die aufgrund der eigenen Emotionen ergriffen werden, um persönliche Gefühle auszudrücken.
Bei unkontrollierter Reaktion gibt es keine Zurückhaltung und es mangelt an Diskretion.
Wenn Wünsche nicht erfüllt werden, entsteht innere Unruhe.
Ein häufiges Beispiel sind Verhaltens- und Rational-Choice-Annahmen.
Diese sechs Konzepte wurden von Aristoteles identifiziert und sind immer noch Gegenstand mehrerer Vorträge.
Mikrologische Wirtschaftstheorien betrachten Handlungen einer Gruppe von Individuen.
Dadurch wird die Wettbewerbsfähigkeit der Anbieter erhöht und somit Ordnung in der Wirtschaft geschaffen.
Obwohl die Rational-Choice-Theorie zunehmend von Ökonomen kolonisiert wird, unterscheidet sie sich von mikroökonomischen Konzepten.
Traditionelle Handlungen: Handlungen, die aus Tradition ausgeführt werden, weil sie für bestimmte Situationen immer auf eine bestimmte Art und Weise ausgeführt werden.
Ein Brauch ist eine Praxis, die auf Vertrautheit beruht.
Eine Gewohnheit ist eine Reihe von Schritten, die nach und nach und manchmal ohne bewusstes Bewusstsein erlernt werden.
Die Idee von Cooleys Spiegelselbst besteht darin, dass sich unser Selbstbewusstsein entwickelt, wenn wir andere beobachten und darüber nachdenken und was sie über unsere Handlungen denken.
Sozialkapital ist „das Beziehungsnetzwerk zwischen Menschen, die in einer bestimmten Gesellschaft leben und arbeiten und es dieser Gesellschaft ermöglichen, effektiv zu funktionieren“.
In der ersten Hälfte des 19. Jahrhunderts machte de Tocqueville Beobachtungen über das amerikanische Leben, die das soziale Kapital zu skizzieren und zu definieren schienen.
Die Gemeinschaft als Ganzes wird von der Zusammenarbeit aller ihrer Teile profitieren, während der Einzelne in seinen Vereinigungen die Vorteile der Hilfe, der Sympathie und der Kameradschaft seiner Nachbarn finden wird.
Mit den Worten von Stein (1960:1): „Der Preis für die Aufrechterhaltung einer Gesellschaft, die kulturelle Differenzierung und Experimente fördert, ist zweifellos die Akzeptanz eines gewissen Maßes an Desorganisation sowohl auf individueller als auch auf sozialer Ebene.“
Alle diese Überlegungen trugen in den folgenden Jahrzehnten erheblich zur Entwicklung des Sozialkapitalkonzepts bei.
Robert D. Putnam (1993) schlug vor, dass Sozialkapital die Zusammenarbeit und sich gegenseitig unterstützende Beziehungen in Gemeinschaften und Nationen erleichtern würde und daher ein wertvolles Mittel zur Bekämpfung vieler sozialer Störungen moderner Gesellschaften, beispielsweise Kriminalität, wäre.
Nan Lins Konzept des Sozialkapitals verfolgt einen individualistischeren Ansatz: „Investition in soziale Beziehungen mit erwarteten Renditen auf dem Markt.“
Der Begriff Kapital wird in Analogie zu anderen Formen des Wirtschaftskapitals verwendet, da Sozialkapital ähnliche (wenn auch weniger messbare) Vorteile haben soll.
Robison, Schmid und Siles (2002) überprüften verschiedene Definitionen von Sozialkapital und kamen zu dem Schluss, dass viele die formalen Anforderungen einer Definition nicht erfüllten.
Sie schlagen vor, Sozialkapital als Sympathie zu definieren: Der Gegenstand der Sympathie eines anderen hat Sozialkapital; Wer Mitgefühl für andere hat, stellt soziales Kapital zur Verfügung.
Sozialkapital unterscheidet sich auch von der ökonomischen Theorie des Sozialkapitalismus.
Es „schafft einen Mehrwert für die Menschen, die verbunden sind, und auch für die Umstehenden.“
Laut Robert D. Putnam bezieht sich Sozialkapital auf „Verbindungen zwischen Individuen – soziale Netzwerke und die daraus resultierenden Normen der Gegenseitigkeit und Vertrauenswürdigkeit“.
Dies zeigt sich in einem geringeren Vertrauen in die Regierung und einem geringeren Maß an Bürgerbeteiligung.
Putnam weist außerdem darauf hin, dass eine Hauptursache für den Rückgang des Sozialkapitals der Eintritt von Frauen in die Arbeitswelt ist, was mit zeitlichen Einschränkungen zusammenhängen könnte, die das Engagement bürgerschaftlicher Organisationen wie Eltern-Lehrer-Vereinigungen behindern.
Fukuyama weist darauf hin, dass soziales Kapital zwar entwicklungsfördernd ist, aber auch Kosten für Nichtgruppenmitglieder verursacht, die unbeabsichtigte Folgen für das allgemeine Wohlergehen haben.
Diese Dimension konzentriert sich auf die Vorteile, die sich aus der Konfiguration des individuellen oder kollektiven Netzwerks eines Akteurs ergeben.
Dies lässt sich am besten durch das Vertrauen anderer und deren Kooperation sowie die Identifikation einer Person innerhalb eines Netzwerks charakterisieren.
Untersuchungen von Sheri Berman und Dylan Riley sowie den Ökonomen Shanker Satyanath, Nico Voigtländer und Hans-Joachim Voth haben Bürgervereinigungen mit dem Aufstieg faschistischer Bewegungen in Verbindung gebracht.
Die negativen Folgen von Sozialkapital werden häufiger mit Bindung als mit Überbrückung in Verbindung gebracht.
Bindendes und überbrückendes Sozialkapital kann im Gleichgewicht produktiv zusammenarbeiten, aber auch gegeneinander wirken.
Die Stärkung der Inselbindungen kann zu vielfältigen Auswirkungen wie ethnischer Marginalisierung oder sozialer Isolation führen.
Aus Frust über das Versagen der nationalen Regierung und der politischen Parteien stürzten sich die Deutschen in ihre Vereine, Freiwilligenverbände und Berufsverbände und trugen so dazu bei, die Weimarer Republik zu untergraben und Hitlers Aufstieg zur Macht zu erleichtern.
In der Weimarer Republik waren sie sehr introvertiert.
Robert Putnam weist in seinen späteren Arbeiten auch darauf hin, dass das Sozialkapital und das damit verbundene Wachstum des öffentlichen Vertrauens durch Einwanderung und die zunehmende Rassenvielfalt in Gemeinschaften gehemmt werden.
Mangelnde Homogenität führte dazu, dass sich die Menschen selbst aus ihren engsten Gruppen und Beziehungen zurückzogen und so eine atomisierte Gesellschaft statt einer zusammenhängenden Gemeinschaft entstand.
Auf Humankapital, eine private Ressource, konnte über das zugegriffen werden, was die vorherige Generation durch Sozialkapital angesammelt hatte.
Auch wenn Coleman in seiner Diskussion nie wirklich auf Pierre Bourdieu eingeht, deckt sich dies mit Bourdieus Argumentation in „Reproduktion in Bildung, Gesellschaft und Kultur“.
Somit ist es die soziale Plattform selbst, die einen mit der sozialen Realität ausstattet, an die man sich gewöhnt.
Um dies zu veranschaulichen, gehen wir davon aus, dass ein Individuum seinen Platz in der Gesellschaft verbessern möchte.
Ist die Zivilgesellschaft eine adäquate Theorie?
Typische Beispiele sind, dass kriminelle Banden verbindendes soziales Kapital schaffen, während Chöre und Bowlingclubs (daher der Titel, da Putnam ihren Niedergang beklagte) überbrückendes soziales Kapital schaffen.
Aldrich wendet die Ideen des Sozialkapitals auch auf die Grundprinzipien der Katastrophenwiederherstellung an und erörtert Faktoren, die die Wiederherstellung entweder unterstützen oder behindern, wie etwa das Ausmaß des Schadens, die Bevölkerungsdichte, die Qualität der Regierung und der Hilfe.
Menschen, die ihr Leben auf diese Weise leben, haben das Gefühl, dass dies gesellschaftliche Normen sind und können ihr Leben ohne Sorgen um ihre Kreditwürdigkeit und Kinder führen und bei Bedarf Almosen erhalten.
Alle Formen von „Kapital“ waren für Marx nur im Besitz von Kapitalisten, und er betonte die Grundlage der Arbeit in der kapitalistischen Gesellschaft als eine Klasse, die aus Individuen besteht, die gezwungen sind, ihre Arbeitskraft zu verkaufen, weil ihnen in jedem Sinne ausreichend Kapital fehlt Wort, anders zu tun.
Als Beispiel hierfür nennt Portes die Schenkung eines Stipendiums an einen Angehörigen derselben ethnischen Gruppe.
Es werden Bonding- und Bridging-Subskalen vorgeschlagen, die in über 300 wissenschaftlichen Artikeln übernommen wurden.
Es gibt jedoch keine einheitliche quantitative Methode zur Bestimmung des Grads der Kohäsion, sondern vielmehr eine Sammlung von Modellen sozialer Netzwerke, die Forscher im Laufe der Jahrzehnte zur Operationalisierung des Sozialkapitals verwendet haben.
Gruppen mit höherer Mitgliederzahl (z. B. politische Parteien) tragen mehr zum Kapital bei als Gruppen mit niedrigerer Mitgliederzahl, obwohl viele Gruppen mit niedriger Mitgliederzahl (z. B. Gemeinschaften) insgesamt immer noch von Bedeutung sind.
Auch die Beziehung einer Gruppe zum Rest der Gesellschaft wirkt sich auf das Sozialkapital aus, allerdings auf andere Weise.
In der Erkenntnis, dass man die Sympathie anderer möglicherweise nicht beeinflussen kann, können Personen, die Zugehörigkeit anstreben, Maßnahmen ergreifen, um ihre eigene Sympathie für andere und die Organisationen oder Institutionen, die sie vertreten, zu steigern.
Nach Ansicht von Autoren wie Walzer (1992), Alessandrini (2002), Newtown, Stolle & Rochon, Foley & Edwards (1997) und Walters ist es die Zivilgesellschaft, oder genauer gesagt, der dritte Sektor, dass Einzelpersonen dazu in der Lage sind Beziehungsnetzwerke aufbauen und pflegen.
Laut Lyons‘ Third Sector (2001) ist nicht nur dokumentiert, dass die Zivilgesellschaft Quellen für Sozialkapital produziert, Sozialkapital erscheint auch in keiner Form unter den Faktoren, die das Wachstum des Dritten Sektors ermöglichen oder stimulieren.
Ziel ist es, diejenigen, die von den Belohnungen des Wirtschaftssystems ausgegrenzt sind, wieder in die „Gemeinschaft“ zu integrieren.
Alessandrini stimmt zu und sagt, dass „insbesondere in Australien der Neoliberalismus in ökonomischen Rationalismus umgestaltet wurde und von mehreren Theoretikern und Kommentatoren aufgrund der Nutzung des Sozialkapitals als Gefahr für die Gesellschaft insgesamt identifiziert wurde.“
Im Bereich der internationalen Entwicklung haben Ben Fine (2001) und John Harriss (2001) die unangemessene Einführung von Sozialkapital als angebliches Allheilmittel (z. B. Förderung zivilgesellschaftlicher Organisationen und NGOs als Entwicklungsakteure) für die entstandenen Ungleichheiten heftig kritisiert durch neoliberale Wirtschaftsentwicklung.
Allerdings führte ein höheres Maß an Sozialkapital zu einer stärkeren Unterstützung der Demokratie.
Eine sorgfältige Bewertung dieser grundlegenden Faktoren lässt häufig darauf schließen, dass Frauen nicht in gleichem Umfang wie Männer wählen.
Sozialkapital bietet eine Fülle von Ressourcen und Netzwerken, die politisches Engagement erleichtern.
Frauen organisieren sich eher weniger hierarchisch und konzentrieren sich auf die Schaffung eines Konsenses.
Beispielsweise kann eine an Krebs erkrankte Person Informationen, Geld oder moralische Unterstützung erhalten, die sie benötigt, um die Behandlung zu überstehen und sich zu erholen.
Darüber hinaus kann soziales Kapital in der Nachbarschaft auch dazu beitragen, gesundheitliche Ungleichheiten bei Kindern und Jugendlichen abzufedern.
Die Beziehungen und Netzwerke, die eine ethnische Minderheit in einem geografischen Gebiet pflegt, in dem ein hoher Prozentsatz der Einwohner derselben ethnischen Gruppe angehört, können zu besseren Gesundheitsergebnissen führen, als aufgrund anderer individueller und nachbarschaftlicher Merkmale zu erwarten wäre.
Ergebnisse einer Umfrage unter 13- bis 18-jährigen Schülern in Schweden zeigten beispielsweise, dass geringes Sozialkapital und geringes soziales Vertrauen mit einer höheren Rate an psychosomatischen Symptomen, Muskel-Skelett-Schmerzen und Depressionen verbunden sind.
In einer Studie korrelierte die Informationsnutzung des Internets positiv mit der Produktion von Sozialkapital einer Person, während die Nutzung des Internets im sozialen und Freizeitbereich negativ korrelierte (höhere Nutzungsgrade korrelierten mit einem geringeren Sozialkapital).
Dies bedeutet, dass Einzelpersonen auf der Grundlage ermittelter Interessen und Hintergründe selektiv mit anderen in Kontakt treten können.
Dieses Argument wird fortgesetzt, obwohl die überwiegende Zahl der Beweise einen positiven Zusammenhang zwischen Sozialkapital und dem Internet zeigt.
Aktuelle Untersuchungen aus dem Jahr 2006 zeigen außerdem, dass Internetnutzer häufig über größere Netzwerke verfügen als diejenigen, die unregelmäßig oder gar nicht auf das Internet zugreifen.
Andere Untersuchungen zeigen, dass jüngere Menschen das Internet als ergänzendes Kommunikationsmedium nutzen, anstatt zuzulassen, dass die Internetkommunikation den persönlichen Kontakt ersetzt.
Sie kritisieren Coleman, der nur die Anzahl der in der Familie anwesenden Eltern heranzog und den unsichtbaren Effekt diskreterer Dimensionen wie Stiefeltern und verschiedener Arten von Einelternfamilien vernachlässigte.
Morgan und Sorensen (1999) kritisieren Coleman direkt, weil ihm ein expliziter Mechanismus fehlt, um zu erklären, warum Schüler katholischer Schulen bei standardisierten Leistungstests besser abschneiden als Schüler öffentlicher Schulen.
Es wurde festgestellt, dass soziales Kapital zwar positive Auswirkungen auf die Aufrechterhaltung einer umfassenden, funktionalen Gemeinschaft in Schulen haben kann, die Normen durchsetzen, es aber auch die negativen Folgen einer übermäßigen Überwachung mit sich bringt.
Diese Schulen erforschen eine andere Art von sozialem Kapital, beispielsweise Informationen über Möglichkeiten in den erweiterten sozialen Netzwerken von Eltern und anderen Erwachsenen.
Die Ähnlichkeit dieser Zustände besteht darin, dass Eltern stärker mit der Bildung ihrer Kinder in Verbindung gebracht wurden.
Ohne soziales Kapital im Bildungsbereich können Lehrer und Eltern, die eine Verantwortung für das Lernen eines Schülers übernehmen, erhebliche Auswirkungen auf das schulische Lernen ihres Kindes auf diese Faktoren haben.
Wie Tedin und Weiher (2010) feststellen, „ist einer der wichtigsten Faktoren für die Förderung des Schülererfolgs die aktive Beteiligung der Eltern an der Bildung eines Kindes.“
Unterstützende Netzwerke als eine Form des sozialen Kapitals sind notwendig, um das kulturelle Kapital der neu angekommenen Studierenden zu aktivieren.
Ethnische Solidarität ist besonders wichtig, wenn Einwanderer gerade erst in der Aufnahmegesellschaft ankommen.
Ethnische Unterstützung gibt Impulse für den akademischen Erfolg.
Sein Hauptargument für die Klassifizierung von Sozialkapital als geografisches Konzept ist, dass die Beziehungen von Menschen durch die Gebiete, in denen sie leben, geprägt und geprägt werden.
In seinen Studien untersucht er nicht die einzelnen Teilnehmer dieser Strukturen, sondern wie die Strukturen und die daraus resultierenden sozialen Verbindungen über den Raum verteilt sind.
Ein weiterer Bereich, in dem Sozialkapital als Studiengebiet in der Geographie betrachtet werden kann, ist die Analyse der Beteiligung an Freiwilligenarbeit und deren Unterstützung durch verschiedene Regierungen.
Es besteht ein signifikanter Zusammenhang zwischen Freizeit und demokratischem Sozialkapital.
In einer späteren Studie zeigt Kislev (2020) den Zusammenhang zwischen dem Wunsch nach einer romantischen Beziehung und dem Single-Sein.
Ähnliche Ergebnisse ergab eine von Sarker in Bangladesch durchgeführte Querschnittsstudie.
Epo tat dies, indem es die Wohlfahrtsergebnisse der Unternehmer verglich, die sowohl Zugang als auch keinen Zugang hatten.
Gruppenzusammenhalt (auch Gruppenzusammenhalt und sozialer Zusammenhalt genannt) entsteht, wenn Bindungen Mitglieder einer sozialen Gruppe untereinander und mit der Gruppe als Ganzes verbinden.
Zusammenhalt kann genauer definiert werden als die Tendenz einer Gruppe, in Einheit zu bleiben, während sie auf ein Ziel hinarbeitet oder die emotionalen Bedürfnisse ihrer Mitglieder befriedigt.
Seine dynamische Natur bezieht sich darauf, wie sich seine Stärke und Form im Laufe der Zeit von der Bildung einer Gruppe bis zur Auflösung einer Gruppe allmählich verändert.
Diese Definition kann auf die meisten Gruppen verallgemeinert werden, die durch die oben diskutierte Gruppendefinition gekennzeichnet sind.
In einer Studie baten sie die Gruppenmitglieder, alle ihre guten Freunde zu nennen, und berechneten das Verhältnis der Entscheidungen innerhalb der Gruppe zu den Entscheidungen außerhalb der Gruppe.
Der Gruppenzusammenhalt ähnelt einer Art Anziehung auf Gruppenebene, die laut Hogg als soziale Anziehung bekannt ist.
Lott und Lott (1965), die zwischenmenschliche Anziehung als Gruppenzusammenhalt bezeichnen, führten eine umfassende Literaturrecherche durch und stellten fest, dass die Ähnlichkeiten der einzelnen Personen im Hintergrund (z. B. Rasse, ethnische Zugehörigkeit, Beruf, Alter), Einstellungen, Werte und Persönlichkeitsmerkmale im Allgemeinen positiv sind Zusammenhang mit Gruppenzusammenhalt.
Darüber hinaus erhöht ein ähnlicher Hintergrund die Wahrscheinlichkeit, dass Mitglieder ähnliche Ansichten zu verschiedenen Themen teilen, einschließlich Gruppenzielen, Kommunikationsmethoden und der Art der gewünschten Führung.
Dies wird oft durch soziales Faulenzen verursacht, eine Theorie, die besagt, dass einzelne Mitglieder einer Gruppe tatsächlich weniger Anstrengungen unternehmen, weil sie glauben, dass andere Mitglieder die Lücke ausgleichen werden.
Die meisten Metaanalysen (Studien, die die Ergebnisse vieler Studien zusammenfassen) haben gezeigt, dass es einen Zusammenhang zwischen Zusammenhalt und Leistung gibt.
Wenn es als Aufgabenengagement definiert wird, korreliert es auch mit der Leistung, wenn auch in geringerem Maße als Zusammenhalt als Anziehung.
Bei einigen Gruppen besteht jedoch möglicherweise eine stärkere Beziehung zwischen Zusammenhalt und Leistung als bei anderen.
Es gibt Hinweise darauf, dass der Zusammenhalt bei Gruppen mit stark voneinander abhängigen Rollen möglicherweise stärker mit der Leistung zusammenhängt als bei Gruppen, deren Mitglieder unabhängig sind.
Darüber hinaus waren Gruppen mit hohen Leistungszielen äußerst produktiv.
Mitglieder in zusammenhängenden Gruppen sind außerdem optimistischer und leiden weniger unter sozialen Problemen als Mitglieder in nicht zusammenhängenden Gruppen.
Es zeigte sich, dass die Maurer und Zimmerleute zufriedener waren, wenn sie in geschlossenen Gruppen arbeiteten.
Eine Studie hat gezeigt, dass Zusammenhalt als Aufgabenengagement die Entscheidungsfindung in der Gruppe verbessern kann, wenn die Gruppe unter Stress steht, und zwar stärker als wenn sie nicht unter Stress steht.
Die Studie ergab, dass Teams mit geringem Zusammenhalt und hoher Dringlichkeit schlechtere Leistungen erbrachten als Teams mit hohem Zusammenhalt und hoher Dringlichkeit.
Die Theorie des Gruppendenkens legt nahe, dass der Druck die Gruppe daran hindert, kritisch über die von ihr getroffenen Entscheidungen nachzudenken.
Ein weiterer Grund liegt darin, dass die Menschen die Gruppe schätzen und daher eher bereit sind, dem Konformitätsdruck nachzugeben, um ihre Beziehungen aufrechtzuerhalten oder zu verbessern.
Es wurde angenommen, dass der Grad der Sympathie der Mitglieder ein Zeichen für den Zusammenhalt der Gruppe ist.
Laut den von der Regierung in Auftrag gegebenen thematischen Berichten „State of the English Cities“ gibt es fünf verschiedene Dimensionen des sozialen Zusammenhalts: materielle Bedingungen, passive Beziehungen, aktive Beziehungen, Solidarität, Inklusion und Gleichheit.
Diese Grundbedürfnisse des Lebens sind die Grundlage eines starken sozialen Gefüges und wichtige Indikatoren für den sozialen Fortschritt.
Die dritte Dimension bezieht sich auf die positiven Interaktionen, den Austausch und die Netzwerke zwischen Einzelpersonen und Gemeinschaften oder auf „aktive soziale Beziehungen“.
Dazu gehören auch das Zugehörigkeitsgefühl der Menschen zu einer Stadt und die Stärke gemeinsamer Erfahrungen, Identitäten und Werte zwischen Menschen mit unterschiedlichem Hintergrund.
Auf gesellschaftlicher Ebene definiert Albrekt Larsen sozialen Zusammenhalt „als den Glauben der Bürger eines bestimmten Nationalstaats, dass sie eine moralische Gemeinschaft teilen, die es ihnen ermöglicht, einander zu vertrauen“.
Soziale Bildung ist ein marxistisches Konzept (synonym mit „Gesellschaft“), das sich auf die konkrete, historische Artikulation zwischen der kapitalistischen Produktionsweise, der Aufrechterhaltung vorkapitalistischer Produktionsweisen und dem institutionellen Kontext der Wirtschaft bezieht (Begriffsklärung).
In den Sozialwissenschaften sind soziale Strukturen die strukturierten sozialen Arrangements in der Gesellschaft, die sowohl aus den Handlungen des Einzelnen hervorgehen als auch diese bestimmen.
Es steht im Gegensatz zum „sozialen System“, das sich auf die übergeordnete Struktur bezieht, in die diese verschiedenen Strukturen eingebettet sind.
Es bestimmt die Normen und Muster der Beziehungen zwischen den verschiedenen Institutionen der Gesellschaft.
Dies ist auch in der modernen Organisationsforschung wichtig, da die Struktur einer Organisation ihre Flexibilität, Fähigkeit zur Veränderung usw. bestimmen kann.
Auf der Meso-Skala geht es um die Struktur sozialer Netzwerke zwischen Einzelpersonen oder Organisationen.
Beispielsweise hat John Levi Martin die Theorie aufgestellt, dass bestimmte Strukturen auf Makroebene die entstehenden Eigenschaften von Kulturinstitutionen auf Mikroebene sind (d. h. „Struktur“ ähnelt der vom Anthropologen Claude Levi-Strauss verwendeten Struktur).
Alexis de Tocqueville war angeblich der erste, der den Begriff „Sozialstruktur“ verwendete.
Eine der frühesten und umfassendsten Darstellungen der Gesellschaftsstruktur stammt von Karl Marx, der das politische, kulturelle und religiöse Leben mit der Produktionsweise (einer zugrunde liegenden Wirtschaftsstruktur) in Zusammenhang brachte.
Émile Durkheim führte unter Berufung auf die von Herbert Spencer und anderen populär gemachten Analogien zwischen biologischen und sozialen Systemen die Idee ein, dass verschiedene soziale Institutionen und Praktiken eine Rolle bei der Gewährleistung der funktionalen Integration der Gesellschaft durch die Assimilation verschiedener Teile zu einem einheitlichen und sich selbst reproduzierenden System spielten ganz.
Andere folgen Lévi-Strauss und suchen nach logischer Ordnung in kulturellen Strukturen.
Die einflussreichsten Versuche, das Konzept der sozialen Struktur mit der Handlungsfähigkeit zu verbinden, sind die Strukturierungstheorie von Anthony Giddens und die Praxistheorie von Pierre Bourdieu.
In dieser Hinsicht ähnelt Giddens‘ Analyse eng der Dekonstruktion der Binärsysteme, die dem klassischen soziologischen und anthropologischen Denken zugrunde liegen (insbesondere den universalisierenden Tendenzen des Strukturalismus von Lévi-Strauss) durch Jacques Derrida.
Dies wurde von Jacob L. Moreno untersucht.
Die Soziobiologie ist ein Fachgebiet der Biologie, dessen Ziel es ist, soziales Verhalten im Hinblick auf die Evolution zu untersuchen und zu erklären.
Die Soziobiologie untersucht soziale Verhaltensweisen wie Paarungsmuster, Revierkämpfe, Rudeljagd und die Bienenstockgesellschaft sozialer Insekten.
Sie sagt voraus, dass Tiere auf eine Weise handeln werden, die sich im Laufe der Zeit als evolutionär erfolgreich erwiesen hat.
Verhalten wird daher als Versuch angesehen, die eigenen Gene in der Bevölkerung zu erhalten.
Altmann entwickelte seine eigene Soziobiologie, um das Sozialverhalten von Rhesusaffen mithilfe von Statistiken zu untersuchen, und wurde 1965 als „Soziobiologe“ am Yerkes Regional Primate Research Center eingestellt.
„Soziobiologie“, einst ein Fachbegriff, erlangte 1975 große Bekanntheit, als Wilson sein Buch Sociobiology: The New Synthesis veröffentlichte, das eine heftige Kontroverse auslöste.
Der Einfluss der Evolution auf das Verhalten war jedoch schon bald nach der Entdeckung der Evolution selbst für Biologen und Philosophen von Interesse.
Edward H. Hagen schreibt in The Handbook of Evolutionary Psychology, dass die Soziobiologie trotz der öffentlichen Kontroverse hinsichtlich der Anwendung auf den Menschen „einer der wissenschaftlichen Triumphe des 20. Jahrhunderts“ ist.
Daher waren diese Merkmale wahrscheinlich „adaptiv“ an die Umgebung, in der sich die Art entwickelte.
Daher sind sie oft an instinktivem oder intuitivem Verhalten interessiert und an der Erklärung der Ähnlichkeiten statt der Unterschiede zwischen Kulturen.
Dieser elterliche Schutz würde in der Bevölkerung an Häufigkeit zunehmen.
E.O. Wilson argumentierte, dass die Evolution auch auf Gruppen wirken könne.
Wenn Altruismus genetisch bedingt ist, müssen altruistische Individuen ihre eigenen altruistischen genetischen Merkmale reproduzieren, damit Altruismus überleben kann. Wenn Altruisten jedoch ihre Ressourcen auf Kosten ihrer eigenen Art an Nicht-Altruisten verschwenden, sterben die Altruisten tendenziell aus und die anderen tendieren dazu Zunahme.
Innerhalb der Soziobiologie wird ein soziales Verhalten zunächst als soziobiologische Hypothese erklärt, indem eine evolutionär stabile Strategie gefunden wird, die zum beobachteten Verhalten passt.
Altruismus zwischen sozialen Insekten und Wurfgeschwistern wurde auf diese Weise erklärt.
Im Allgemeinen legen Weibchen mit mehr Möglichkeiten, Nachkommen zu gebären, weniger wert und arrangieren möglicherweise auch Möglichkeiten zur Geburt, um die Nahrung und den Schutz vor Partnern zu maximieren.
Studien zur menschlichen Verhaltensgenetik haben im Allgemeinen ergeben, dass Verhaltensmerkmale wie Kreativität, Extraversion, Aggressivität und IQ eine hohe Vererbbarkeit aufweisen.
Wenn FEV genetisch aus dem Mausgenom gelöscht wird, greifen männliche Mäuse andere Männchen sofort an, während ihre Wildtyp-Gegenstücke deutlich länger brauchen, um gewalttätiges Verhalten auszulösen.
Während eines Treffens der Sociobiology Study Group im Jahr 1976 plädierte Chomsky, wie Ullica Segerstråle berichtet, für die Bedeutung einer soziobiologisch fundierten Vorstellung von der menschlichen Natur.
Wilson hat behauptet, dass er nie andeuten wollte, was sein sollte, sondern nur, was der Fall ist.
Geschäft ist die Tätigkeit, seinen Lebensunterhalt zu verdienen oder Geld zu verdienen, indem man Produkte (wie Waren und Dienstleistungen) herstellt oder kauft und verkauft.
Wenn das Unternehmen Schulden macht, können die Gläubiger die persönlichen Besitztümer des Eigentümers in Anspruch nehmen.
Der Begriff wird auch häufig umgangssprachlich (jedoch nicht von Anwälten oder Amtsträgern) für die Bezeichnung eines Unternehmens verwendet.
Ein privat geführtes, gewinnorientiertes Unternehmen ist Eigentum seiner Aktionäre, die einen Vorstand wählen, der das Unternehmen leitet und dessen Führungspersonal einstellt.
Eine Genossenschaft unterscheidet sich von einer Kapitalgesellschaft dadurch, dass sie Mitglieder und keine Aktionäre hat und diese die Entscheidungsbefugnis teilen.
Gesellschaften mit beschränkter Haftung (Limited Liability Companies, LLC), Partnerschaften mit beschränkter Haftung und andere spezifische Arten von Unternehmensorganisationen schützen ihre Eigentümer oder Anteilseigner vor dem Scheitern des Unternehmens, indem sie ihre Geschäfte unter einer separaten juristischen Person mit bestimmten rechtlichen Schutzmaßnahmen abwickeln.
Die Mitglieder garantieren die Zahlung bestimmter (in der Regel nominaler) Beträge, wenn das Unternehmen in die Insolvenz geht, haben jedoch ansonsten keine wirtschaftlichen Rechte gegenüber dem Unternehmen.
Diese Art von Gesellschaft darf im Vereinigten Königreich nicht mehr gegründet werden, obwohl es noch immer gesetzliche Bestimmungen für ihre Existenz gibt.
Beachten Sie, dass „Ltd“ hinter dem Firmennamen eine Gesellschaft mit beschränkter Haftung bedeutet und „PLC“ (Public Limited Company) darauf hinweist, dass die Anteile weit verbreitet sind.“
In einer Gesellschaft mit beschränkter Haftung sind dies die Bürgen.
Private Unternehmen haben keine öffentlich gehandelten Aktien und unterliegen häufig Beschränkungen für die Übertragung von Aktien.
Unterhaltungsunternehmen und Massenmedienagenturen erwirtschaften Gewinne hauptsächlich aus dem Verkauf geistigen Eigentums.
Dazu gehören materielle Güter wie Autos, Busse, medizinische Geräte, Glas oder Flugzeuge.
Die meisten Geschäfte und Katalogunternehmen sind Vertriebshändler oder Einzelhändler.
Sie erzielen ihre Gewinne durch den Verkauf von Waren und Dienstleistungen, die mit dem Sport zu tun haben.
Das moderne Fachgebiet wurde 1494 vom italienischen Mathematiker Luca Pacioli begründet.
Finanzen können auch als die Wissenschaft des Geldmanagements definiert werden.
Eigentümer können ihr Unternehmen selbst leiten oder Manager damit beauftragen.
Geschäftsprozessmanagement (BPM) ist ein ganzheitlicher Managementansatz, der darauf abzielt, alle Aspekte einer Organisation an den Wünschen und Bedürfnissen der Kunden auszurichten.
Viele Unternehmen werden von einer separaten Einheit betrieben, beispielsweise einer Kapitalgesellschaft oder einer Personengesellschaft (entweder mit oder ohne beschränkter Haftung).
Im Allgemeinen sind Anteilseigner einer Kapitalgesellschaft, Kommanditisten einer Kommanditgesellschaft und Mitglieder einer Gesellschaft mit beschränkter Haftung von der persönlichen Haftung für die Schulden und Verpflichtungen des Rechtsträgers, der rechtlich als eigenständige „Person“ behandelt wird, ausgeschlossen.
Die Bedingungen einer Partnerschaft werden zum Teil durch einen Partnerschaftsvertrag (sofern ein solcher gegründet wird) und zum Teil durch das Recht der Gerichtsbarkeit geregelt, in der die Partnerschaft ihren Sitz hat.
In manchen Steuersystemen kann dies zu einer sogenannten Doppelbesteuerung führen, da die Körperschaft zunächst Steuern auf den Gewinn zahlt und dann, wenn die Körperschaft ihre Gewinne an ihre Eigentümer ausschüttet, Einzelpersonen nach Abschluss ihrer Tätigkeit Dividenden in ihr Einkommen einbeziehen müssen persönliche Steuererklärungen, bei denen eine zweite Ebene der Einkommensteuer erhoben wird.
Der „Börsengang“ durch einen als Initial Public Offering (IPO) bezeichneten Prozess bedeutet, dass ein Teil des Unternehmens in den Besitz von Mitgliedern der Öffentlichkeit übergeht.
Der Kodex von Hammurabi stammt beispielsweise aus dem Jahr 1772 v. Chr. und enthält Bestimmungen, die sich unter anderem auf Versandkosten und Geschäfte zwischen Kaufleuten und Maklern beziehen.
Lokale Gerichtsbarkeiten erfordern möglicherweise auch spezielle Lizenzen und Steuern, nur um ein Unternehmen zu betreiben.
Die meisten Länder mit Kapitalmärkten haben mindestens einen.
In anderen westlichen Ländern gibt es vergleichbare Regulierungsbehörden.
Die Verbreitung und zunehmende Komplexität des Wirtschaftsrechts hat eine zunehmende Spezialisierung auf das Gesellschaftsrecht erzwungen.
Die meisten Unternehmen verfügen über Namen, Logos und ähnliche Markentechniken, die von der Eintragung einer Marke profitieren könnten.
Wirtschaftswissenschaften sind die Sozialwissenschaften, die untersuchen, wie Menschen mit Werten interagieren; insbesondere die Produktion, der Vertrieb und der Konsum von Gütern und Dienstleistungen.
Er bestätigte, dass frühere Ökonomen ihre Studien in der Regel auf die Analyse des Reichtums konzentrierten: wie Reichtum geschaffen (produziert), verteilt und konsumiert wird; und wie Wohlstand wachsen kann.
Wenn der Krieg nicht zu gewinnen ist oder wenn die erwarteten Kosten den Nutzen überwiegen, werden die entscheidenden Akteure (vorausgesetzt, sie sind rational) möglicherweise nie in den Krieg ziehen (eine Entscheidung treffen), sondern stattdessen andere Alternativen prüfen.
In den Schriften des böotischen Dichters Hesiod finden sich ökonomische Grundsätze, und mehrere Wirtschaftshistoriker haben Hesiod selbst als den „ersten Ökonomen“ bezeichnet.
Zwei Gruppen, die später „Merkantilisten“ und „Physiokraten“ genannt wurden, hatten einen direkteren Einfluss auf die weitere Entwicklung des Fachs.
Darin hieß es, dass der Wohlstand einer Nation von der Anhäufung von Gold und Silber abhänge.
Physiokraten, eine Gruppe französischer Denker und Schriftsteller des 18. Jahrhunderts, entwickelten die Idee der Wirtschaft als einem Kreislauf von Einkommen und Produktion.
Physiokraten plädierten dafür, die verwaltungstechnisch aufwendige Steuererhebung durch eine einzige Steuer auf das Einkommen der Grundeigentümer zu ersetzen.
Smith erörtert potenzielle Vorteile der Spezialisierung durch Arbeitsteilung, einschließlich erhöhter Arbeitsproduktivität und Gewinnen aus dem Handel, sei es zwischen Stadt und Land oder zwischen Ländern.
Der Druck einer schnell wachsenden Bevölkerung auf eine begrenzte Landfläche führte zu sinkenden Arbeitserträgen.
Während Adam Smith die Einkommensproduktion betonte, konzentrierte sich David Ricardo (1817) auf die Einkommensverteilung zwischen Grundbesitzern, Arbeitern und Kapitalisten.
Ricardo war der erste, der das Prinzip des komparativen Vorteils darlegte und bewies, wonach sich jedes Land auf die Produktion und den Export von Gütern spezialisieren sollte, indem es niedrigere relative Produktionskosten hat, anstatt sich nur auf seine eigene Produktion zu verlassen.
Mill wies auf einen deutlichen Unterschied zwischen den beiden Rollen des Marktes hin: der Ressourcenverteilung und der Einkommensverteilung.
Smith schrieb, dass der „wahre Preis jeder Sache … die Mühe und Mühe ist, sie zu erwerben“.
Says Definition hat sich bis in unsere Zeit durchgesetzt, indem er das Wort „Reichtum“ durch „Güter und Dienstleistungen“ ersetzte, was bedeutet, dass Reichtum auch immaterielle Objekte umfassen kann.
Für Robbins wurde die Unzulänglichkeit behoben, und seine Definition ermöglicht es uns, guten Gewissens die Bildungsökonomie, die Sicherheitsökonomie, die Gesundheitsökonomie, die Kriegsökonomie und natürlich die Produktions-, Verteilungs- und Konsumökonomie als gültige Themen der Wissenschaft zu proklamieren Wirtschaftswissenschaft.“
Obwohl alles andere als einhellig, würden die meisten Mainstream-Ökonomen irgendeine Version von Robbins‘ Definition akzeptieren, auch wenn viele ernsthafte Einwände gegen den Umfang und die Methode der Wirtschaftswissenschaften erhoben haben, die sich aus dieser Definition ergeben.
Der Begriff „Ökonomie“ wurde von neoklassischen Ökonomen wie Alfred Marshall als prägnantes Synonym für „Wirtschaftswissenschaft“ und als Ersatz für die frühere „politische Ökonomie“ populär gemacht.
Sie verzichtete auf die aus der klassischen Ökonomie übernommene Arbeitswerttheorie zugunsten einer Grenznutzentheorie des Werts auf der Nachfrageseite und einer allgemeineren Kostentheorie auf der Angebotsseite.
Ein unmittelbares Beispiel hierfür ist die Verbrauchertheorie der individuellen Nachfrage, die isoliert, wie sich Preise (als Kosten) und Einkommen auf die nachgefragte Menge auswirken.
Die moderne Mainstream-Ökonomie baut auf der neoklassischen Ökonomie auf, jedoch mit vielen Verfeinerungen, die frühere Analysen entweder ergänzen oder verallgemeinern, wie z. B. Ökonometrie, Spieltheorie, Analyse von Marktversagen und unvollkommenem Wettbewerb sowie das neoklassische Modell des Wirtschaftswachstums zur Analyse langfristiger Variablen, die sich auf das Volkseinkommen auswirken .
Es liegt ein wirtschaftliches Problem vor, das Gegenstand der wirtschaftswissenschaftlichen Untersuchung ist, wenn eine Entscheidung (Wahl) von einem oder mehreren ressourcenkontrollierenden Akteuren getroffen wird, um unter begrenzten rationalen Bedingungen das bestmögliche Ergebnis zu erzielen.
Das Buch konzentrierte sich auf Determinanten des Nationaleinkommens auf kurze Sicht, wenn die Preise relativ unflexibel sind.
Die keynesianische Ökonomie hat zwei Nachfolger.
Es wird im Allgemeinen mit der Universität Cambridge und der Arbeit von Joan Robinson in Verbindung gebracht.
Ben Bernanke, ehemaliger Vorsitzender der Federal Reserve, gehört heute zu den Ökonomen, die Friedmans Analyse der Ursachen der Weltwirtschaftskrise allgemein akzeptieren.
Bei der Erstellung von Theorien besteht das Ziel darin, Theorien zu finden, deren Informationsanforderungen mindestens ebenso einfach sind, die Vorhersagen präziser machen und bei der Generierung zusätzlicher Forschung fruchtbarer sind als frühere Theorien.
Frühe makroökonomische Modelle konzentrierten sich auf die Modellierung der Beziehungen zwischen aggregierten Variablen, aber da sich die Beziehungen im Laufe der Zeit zu ändern schienen, formulierten Makroökonomen, darunter auch neue Keynesianer, ihre Modelle in Mikrofundamenten um.
Manchmal ist eine ökonomische Hypothese nur qualitativ und nicht quantitativ.
Allerdings wächst der Bereich der experimentellen Ökonomie und es werden zunehmend natürliche Experimente genutzt.
Auf diese Weise kann eine Hypothese Akzeptanz finden, wenn auch eher im probabilistischen als im sicheren Sinne.
Kritik, die auf professionellen Standards und der Nichtreproduzierbarkeit der Ergebnisse basiert, dient als weitere Kontrolle gegen Voreingenommenheit, Fehler und übermäßige Verallgemeinerung, obwohl vielen wirtschaftswissenschaftlichen Forschungsarbeiten vorgeworfen wurde, sie seien nicht reproduzierbar, und renommierten Fachzeitschriften wurde vorgeworfen, dass sie die Replikation dadurch nicht erleichtern Bereitstellung des Codes und der Daten.
In der angewandten Ökonomie sind Input-Output-Modelle, die lineare Programmiermethoden verwenden, weit verbreitet.
Dies hat die seit langem bekannte Unterscheidung zwischen Wirtschaftswissenschaften und Naturwissenschaften verringert, da es direkte Tests dessen ermöglicht, was zuvor als Axiome galt.
Ähnliche empirische Tests finden in der Neuroökonomie statt.
Auf Märkten mit vollkommenem Wettbewerb ist kein Teilnehmer groß genug, um über die Marktmacht zu verfügen, den Preis eines homogenen Produkts festzulegen.
Die Mikroökonomie untersucht einzelne Märkte, indem sie das Wirtschaftssystem vereinfacht, indem sie davon ausgeht, dass die Aktivität auf dem analysierten Markt keine Auswirkungen auf andere Märkte hat.
Die allgemeine Gleichgewichtstheorie untersucht verschiedene Märkte und ihr Verhalten.
Es müssen Entscheidungen zwischen wünschenswerten, sich jedoch gegenseitig ausschließenden Aktionen getroffen werden.
Ein Teil der Kosten für die Herstellung von Brezeln besteht darin, dass weder das Mehl noch der Brezel mehr für eine andere Verwendung zur Verfügung stehen.
Zu den im Produktionsprozess verwendeten Inputs gehören primäre Produktionsfaktoren wie Arbeitsleistungen, Kapital (langlebige produzierte Güter, die in der Produktion verwendet werden, z. B. eine bestehende Fabrik) und Land (einschließlich natürlicher Ressourcen).
Die Effizienz steigt, wenn bei gleichbleibendem Input mehr Output erzeugt wird, also die Menge an „Verschwendung“ reduziert wird.
Im einfachsten Fall kann eine Volkswirtschaft nur zwei Güter produzieren (z. B. „Waffen“ und „Butter“).
Die Knappheit wird in der Abbildung dadurch dargestellt, dass die Menschen insgesamt bereit, aber nicht in der Lage sind, über den PPF hinaus zu konsumieren (z. B. bei X), und durch die negative Steigung der Kurve.
Die Steigung der Kurve an einem Punkt darauf gibt den Kompromiss zwischen den beiden Gütern an.
Entlang des PPF bedeutet Knappheit, dass die Auswahl von mehr von einem Gut insgesamt bedeutet, dass man weniger von dem anderen Gut ausgibt.
Ein Punkt innerhalb der Kurve (wie bei A) ist machbar, stellt jedoch eine Produktionsineffizienz dar (verschwenderischer Einsatz von Inputs), da der Output eines oder beider Güter steigen könnte, wenn man sich in nordöstlicher Richtung zu einem Punkt auf der Kurve bewegt.
Es wurde beobachtet, dass ein hohes Handelsvolumen zwischen Regionen stattfindet, selbst wenn sie Zugang zu einer ähnlichen Technologie und einem ähnlichen Mix an Faktorinputs haben, einschließlich Ländern mit hohem Einkommen.
Unter jedem dieser Produktionssysteme kann es eine entsprechende Arbeitsteilung mit unterschiedlichen Spezialisierungen der Arbeitsgruppen oder entsprechend unterschiedlichen Arten von Investitionsgütern und differenzierten Landnutzungen geben.
Theorie und Beobachtung legen die Bedingungen fest, unter denen die Marktpreise von Outputs und produktiven Inputs eine Zuordnung von Faktorinputs nach komparativen Vorteilen auswählen, sodass (relativ) kostengünstige Inputs zur Produktion kostengünstiger Outputs verwendet werden.
In der Mikroökonomie bezieht es sich auf die Preis- und Produktionsbestimmung für einen Markt mit vollkommenem Wettbewerb, was die Bedingung einschließt, dass es keine Käufer oder Verkäufer gibt, die groß genug sind, um über Preissetzungsmacht zu verfügen.
Die Nachfragetheorie beschreibt, dass einzelne Verbraucher rational die am meisten bevorzugte Menge jedes Gutes wählen, abhängig von Einkommen, Preisen, Geschmack usw.
Das Gesetz der Nachfrage besagt, dass im Allgemeinen der Preis und die nachgefragte Menge auf einem bestimmten Markt in einem umgekehrten Verhältnis zueinander stehen.
Darüber hinaus erhöht die Kaufkraft aufgrund des Preisrückgangs die Kauffähigkeit (Einkommenseffekt).
Das Angebot ist das Verhältnis zwischen dem Preis einer Ware und der zu diesem Preis zum Verkauf verfügbaren Menge.
Das Angebot wird typischerweise als Funktion von Preis und Menge dargestellt, wenn andere Faktoren unverändert bleiben.
Ebenso wie auf der Nachfrageseite kann sich die Position des Angebots ändern, beispielsweise aufgrund einer Preisänderung eines Produktionseinsatzes oder einer technischen Verbesserung.
Ein Marktgleichgewicht entsteht, wenn die angebotene Menge der nachgefragten Menge entspricht, dem Schnittpunkt der Angebots- und Nachfragekurven in der Abbildung oben.
Bei einem Preis oberhalb des Gleichgewichts liegt ein Überschuss der angebotenen Menge gegenüber der nachgefragten Menge vor.
Die offensichtlichsten Arten von Unternehmen sind Kapitalgesellschaften, Personengesellschaften und Trusts.
Auf Märkten mit vollkommenem Wettbewerb, die in der Theorie von Angebot und Nachfrage untersucht werden, gibt es viele Produzenten, von denen keiner den Preis wesentlich beeinflusst.
Zu den gängigen Marktstrukturen, die neben dem vollkommenen Wettbewerb untersucht werden, gehören der monopolistische Wettbewerb, verschiedene Formen des Oligopols und das Monopol.
Aufgrund ihrer unterschiedlichen Formen gibt es verschiedene Möglichkeiten, die Unsicherheit darzustellen und die Reaktionen der Wirtschaftsakteure darauf zu modellieren.
In der Verhaltensökonomie wird es verwendet, um die Strategien zu modellieren, die Agenten wählen, wenn sie mit anderen interagieren, deren Interessen zumindest teilweise im Widerspruch zu ihren eigenen stehen.
Es hat scheinbar bedeutende Anwendungen außerhalb der Wirtschaftswissenschaften in so unterschiedlichen Bereichen wie der Formulierung nuklearer Strategien, Ethik, Politikwissenschaft und Evolutionsbiologie.
Es analysiert auch die Preisgestaltung von Finanzinstrumenten, die Finanzstruktur von Unternehmen, die Effizienz und Fragilität der Finanzmärkte, Finanzkrisen und die damit verbundene Regierungspolitik oder -regulierung.
Kunden, die nicht wissen, ob ein Auto eine „Zitrone“ ist, drücken den Preis unter den Preis eines hochwertigen Gebrauchtwagens.
Beide Probleme können die Versicherungskosten erhöhen und die Effizienz verringern, indem sie ansonsten willige Händler aus dem Markt verdrängen („unvollständige Märkte“).
Informationsasymmetrien und unvollständige Märkte können zu wirtschaftlicher Ineffizienz führen, bieten aber auch die Möglichkeit, die Effizienz durch Markt-, Rechts- und Regulierungsmaßnahmen zu verbessern, wie oben erläutert.
Öffentliche Güter sind Güter, die auf einem typischen Markt unterversorgt sind.
Beispielsweise kann Luftverschmutzung einen negativen externen Effekt erzeugen, und Bildung kann einen positiven externen Effekt (weniger Kriminalität usw.) erzeugen.
In vielen Bereichen wird eine Form der Preisbindung postuliert, die eher auf Mengen als auf Preise abzielt und sich kurzfristig an Veränderungen auf der Nachfrage- oder Angebotsseite anpasst.
Beispiele für eine solche Preisstarrheit auf bestimmten Märkten sind die Lohnsätze auf Arbeitsmärkten und die angegebenen Preise auf Märkten, die vom vollständigen Wettbewerb abweichen.
Zu diesen Aggregaten gehören das Volkseinkommen und die Volksproduktion, die Arbeitslosenquote und die Preisinflation sowie Unteraggregate wie der Gesamtkonsum und die Investitionsausgaben sowie deren Komponenten.
Damit wurde ein seit langem bestehendes Problem hinsichtlich inkonsistenter Entwicklungen zum gleichen Thema ausgeräumt.
Keynes argumentierte, dass die Gesamtnachfrage nach Gütern während eines wirtschaftlichen Abschwungs möglicherweise unzureichend sei, was zu unnötig hoher Arbeitslosigkeit und Verlusten an potenzieller Produktion führen könnte.
Die neue klassische Makroökonomie geht im Gegensatz zur keynesianischen Sicht des Konjunkturzyklus davon aus, dass der Markt mit unvollständigen Informationen bereinigt wird.
Zur Erwerbsbevölkerung zählen nur Arbeitnehmer, die aktiv auf der Suche nach einem Arbeitsplatz sind.
Klassische Modelle der Arbeitslosigkeit treten auf, wenn die Löhne zu hoch sind, als dass Arbeitgeber bereit wären, mehr Arbeitskräfte einzustellen.
Wenn eine Volkswirtschaft Branchen umstellt und Arbeitnehmer feststellen, dass ihre bisherigen Qualifikationen nicht mehr gefragt sind, kann es zu großer struktureller Arbeitslosigkeit kommen.
Geld genießt allgemeine Akzeptanz, relative Wertkonsistenz, Teilbarkeit, Haltbarkeit, Portabilität, Angebotselastizität und Langlebigkeit und genießt das Vertrauen der breiten Öffentlichkeit.
Mit den Worten von Francis Amasa Walker, einem bekannten Ökonomen des 19. Jahrhunderts: „Geld ist das, was Geld tut“ („Money is that money done“ im Original).
Seine wirtschaftliche Funktion kann dem Tauschhandel (nichtmonetärer Tausch) gegenübergestellt werden.
Wenn die Gesamtnachfrage unter das Produktionspotenzial der Wirtschaft fällt, entsteht eine Produktionslücke, bei der einige Produktionskapazitäten ungenutzt bleiben.
Beispielsweise können arbeitslose Hausbauer für den Ausbau von Autobahnen engagiert werden.
Die Wirkung der Fiskalpolitik kann durch Crowding Out begrenzt werden.
Einige Ökonomen glauben, dass Verdrängung immer ein Problem darstellt, während andere nicht glauben, dass es sich um ein großes Problem handelt, wenn die Produktion zurückgeht.
Letzteres, ein Aspekt der Public-Choice-Theorie, modelliert das Verhalten des öffentlichen Sektors analog zur Mikroökonomie und umfasst die Interaktion von eigennützigen Wählern, Politikern und Bürokraten.
Es geht auch um die Größe und Verteilung der Handelsgewinne.
Es wird oft behauptet, dass Carlyle der Wirtschaftswissenschaft den Spitznamen „die düstere Wissenschaft“ gab, als Reaktion auf die Schriften von Reverend Thomas Robert Malthus aus dem späten 18 Ernährungsversorgung.
Die enge Verbindung von Wirtschaftstheorie und -praxis mit der Politik ist ein Streitpunkt, der die unprätentiösesten ursprünglichen Grundsätze der Wirtschaftswissenschaften verfälschen oder verfälschen kann und häufig mit spezifischen gesellschaftlichen Agenden und Wertesystemen verwechselt wird.
Einige wissenschaftliche Wirtschaftszeitschriften haben ihre Bemühungen verstärkt, den Konsens von Ökonomen zu bestimmten politischen Fragen zu ermitteln, in der Hoffnung, ein informierteres politisches Umfeld zu schaffen.
Themen wie die Unabhängigkeit der Zentralbank, die Zentralbankpolitik und -rhetorik im Diskurs der Zentralbankgouverneure oder die Prämissen der makroökonomischen Politik (Geld- und Fiskalpolitik) des Staates stehen im Mittelpunkt der Auseinandersetzung und Kritik.
Der Bereich der Informationsökonomie umfasst sowohl mathematisch-ökonomische Forschung als auch Verhaltensökonomie, ähnlich wie Studien zur Verhaltenspsychologie, und Störfaktoren zu den neoklassischen Annahmen sind in vielen Bereichen der Wirtschaftswissenschaften Gegenstand umfangreicher Untersuchungen.
Joskow hatte das starke Gefühl, dass die wichtige Arbeit im Oligopol durch informelle Beobachtungen geleistet wurde, während formale Modelle „ex post“ ausprobiert wurden.
Ein weiteres großes Thema ist die Evolution, die die Einheit und Vielfalt des Lebens erklärt.
Seine Werke wie „Geschichte der Tiere“ waren besonders wichtig, weil sie seine naturalistischen Neigungen offenbarten, und später empirischere Werke, die sich auf biologische Ursachen und die Vielfalt des Lebens konzentrierten.
Die Medizin wurde besonders gut von islamischen Gelehrten untersucht, die in der Tradition griechischer Philosophen arbeiteten, während sich die Naturgeschichte stark auf das aristotelische Denken stützte, insbesondere bei der Aufrechterhaltung einer festen Hierarchie des Lebens.
Untersuchungen von Jan Swammerdam führten zu neuem Interesse an der Entomologie und trugen zur Entwicklung der grundlegenden Techniken der mikroskopischen Präparation und Färbung bei.
Dann, im Jahr 1838, begannen Schleiden und Schwann, die heute universelle Idee zu vertreten, dass (1) die Grundeinheit von Organismen die Zelle ist und (2) dass einzelne Zellen alle Merkmale des Lebens besitzen, obwohl sie sich gegen die Idee wandten, dass (3) alle Zellen entstehen durch die Teilung anderer Zellen.
Carl Linnaeus veröffentlichte 1735 eine grundlegende Taxonomie für die natürliche Welt (von der seitdem Variationen verwendet werden) und führte in den 1750er Jahren wissenschaftliche Namen für alle seine Arten ein.
Lamarck glaubte, dass diese erworbenen Eigenschaften dann an die Nachkommen des Tieres weitergegeben werden könnten, die sie weiterentwickeln und perfektionieren würden.
Die Grundlage für die moderne Genetik entstand mit der Arbeit von Gregor Mendel, der 1865 seine Arbeit „Versuche über Pflanzenhybriden“ vorlegte, in der er die Prinzipien der biologischen Vererbung darlegte und als Grundlage für die moderne Genetik diente .
Die Konzentration auf neue Arten von Modellorganismen wie Viren und Bakterien sowie die Entdeckung der Doppelhelixstruktur der DNA durch James Watson und Francis Crick im Jahr 1953 markierten den Übergang zum Zeitalter der Molekulargenetik.
Schließlich wurde 1990 das Human Genome Project mit dem Ziel ins Leben gerufen, das allgemeine menschliche Genom zu kartieren.
Das Leben auf der Erde begann im Wasser und blieb dort etwa drei Milliarden Jahre lang, bevor es an Land übersiedelte.
Der Kern besteht aus einem oder mehreren Protonen und mehreren Neutronen.
Das Atom jedes einzelnen Elements enthält eine eindeutige Anzahl von Protonen, die als Ordnungszahl bezeichnet wird, und die Summe seiner Protonen und Neutronen ist die Massenzahl eines Atoms.
Kohlenstoff kann beispielsweise als stabiles Isotop (Kohlenstoff-12 oder Kohlenstoff-13) oder als radioaktives Isotop (Kohlenstoff-14) vorliegen, wobei letzteres bei der radiometrischen Datierung (insbesondere der Radiokarbondatierung) zur Altersbestimmung verwendet werden kann aus organischen Materialien.
Bei der Ionenbindung handelt es sich um die elektrostatische Anziehung zwischen entgegengesetzt geladenen Ionen oder zwischen zwei Atomen mit stark unterschiedlichen Elektronegativitäten. Sie ist die primäre Wechselwirkung, die in ionischen Verbindungen auftritt.
Im Gegensatz zu Ionenbindungen beinhaltet eine kovalente Bindung die gemeinsame Nutzung von Elektronenpaaren zwischen Atomen.
Ein allgegenwärtiges Beispiel für eine Wasserstoffbindung findet sich zwischen Wassermolekülen.
Wasser ist lebenswichtig, da es ein wirksames Lösungsmittel ist, das gelöste Stoffe wie Natrium- und Chloridionen oder andere kleine Moleküle auflösen kann, um eine wässrige Lösung zu bilden.
Da die O-H-Bindungen polar sind, ist das Sauerstoffatom leicht negativ und die beiden Wasserstoffatome leicht positiv geladen.
Wasser ist außerdem haftend, da es an der Oberfläche aller polaren oder geladenen Nichtwassermoleküle haften kann.
Die geringere Dichte von Eis im Vergleich zu flüssigem Wasser ist auf die geringere Anzahl von Wassermolekülen zurückzuführen, die die Kristallgitterstruktur von Eis bilden, wodurch viel Platz zwischen den Wassermolekülen verbleibt.
Daher ist eine große Energiemenge erforderlich, um die Wasserstoffbrückenbindungen zwischen Wassermolekülen aufzubrechen und flüssiges Wasser in Gas (oder Wasserdampf) umzuwandeln.
Mit Ausnahme von Wasser enthalten fast alle Moleküle, aus denen jeder Organismus besteht, Kohlenstoff.
Beispielsweise kann ein einzelnes Kohlenstoffatom vier einfache kovalente Bindungen wie bei Methan, zwei doppelte kovalente Bindungen wie bei Kohlendioxid oder eine dreifache kovalente Bindung wie bei Kohlenmonoxid (CO) bilden.
Ein Kohlenwasserstoffgerüst kann durch andere Elemente wie Sauerstoff (O), Wasserstoff (H), Phosphor (P) und Schwefel (S) ersetzt werden, was das chemische Verhalten dieser Verbindung verändern kann.
Wenn zwei Monosaccharide wie Glucose und Fructose miteinander verbunden werden, können sie ein Disaccharid wie Saccharose bilden.
Bei diesen Lipiden handelt es sich um organische Verbindungen, die weitgehend unpolar und hydrophob sind.
Die Glycerin- und Phosphatgruppe bilden zusammen die polare und hydrophile (oder Kopf-)Region des Moleküls, während die Fettsäuren die unpolare und hydrophobe (oder Schwanz-)Region bilden.
Proteine sind die vielfältigsten Makromoleküle, zu denen Enzyme, Transportproteine, große Signalmoleküle, Antikörper und Strukturproteine gehören.
Die Polarität und Ladung der Seitenketten beeinflussen die Löslichkeit von Aminosäuren.
Die Primärstruktur besteht aus einer einzigartigen Sequenz von Aminosäuren, die durch Peptidbindungen kovalent miteinander verbunden sind.
Die Faltung von Alpha-Helices und Beta-Faltblättern verleiht einem Protein seine dreidimensionale oder tertiäre Struktur.
Zu den Purinen gehören Guanin (G) und Adenin (A), während die Pyrimidine aus Cytosin (T), Uracil (U) und Thymin (T) bestehen.
Eine Zellmembran besteht aus einer Lipiddoppelschicht, einschließlich Cholesterinen, die zwischen Phospholipiden sitzen, um ihre Fließfähigkeit bei verschiedenen Temperaturen aufrechtzuerhalten.
Zellmembranen sind an verschiedenen zellulären Prozessen wie Zelladhäsion, Speicherung elektrischer Energie und Zellsignalisierung beteiligt und dienen als Befestigungsoberfläche für mehrere extrazelluläre Strukturen wie Zellwand, Glykokalyx und Zytoskelett.
Der Alberts-Text diskutiert, wie sich die „zellulären Bausteine“ bewegen, um sich entwickelnde Embryonen zu formen.
Pflanzenzellen verfügen über zusätzliche Organellen, die sie von tierischen Zellen unterscheiden, wie z. B. eine Zellwand, die die Pflanzenzelle stützt, Chloroplasten, die Sonnenlichtenergie nutzen, um Zucker zu produzieren, und Vakuolen, die für die Speicherung und strukturelle Unterstützung sorgen und an der Fortpflanzung und dem Abbau beteiligt sind von Pflanzensamen.
Nach dem ersten Hauptsatz der Thermodynamik bleibt Energie erhalten, d. h. sie kann weder erzeugt noch zerstört werden.
Infolgedessen benötigt ein Organismus eine kontinuierliche Energiezufuhr, um einen niedrigen Entropiezustand aufrechtzuerhalten.
Normalerweise setzt der Katabolismus Energie frei und der Anabolismus verbraucht Energie.
Die Gesamtreaktion erfolgt in einer Reihe biochemischer Schritte, von denen einige Redoxreaktionen sind.
Acetyl-Coa gelangt in den Zitronensäurezyklus, der in der mitochondrialen Matrix stattfindet.
Die oxidative Phosphorylierung umfasst die Elektronentransportkette, eine Reihe von vier Proteinkomplexen, die Elektronen von einem Komplex auf einen anderen übertragen und dabei Energie aus NADH und FADH2 freisetzen, die mit dem Pumpen von Protonen (Wasserstoffionen) durch die innere Mitochondrienmembran gekoppelt ist ( Chemiosmose), die eine protonentreibende Kraft erzeugt.
Wenn kein Sauerstoff vorhanden wäre, würde Pyruvat nicht durch Zellatmung verstoffwechselt, sondern einem Fermentationsprozess unterzogen.
Durch die Fermentation wird NADH zu NAD+ oxidiert, sodass es in der Glykolyse wiederverwendet werden kann.
In der Skelettmuskulatur ist das Abfallprodukt Milchsäure.
Während der anaeroben Glykolyse regeneriert sich NAD+, wenn sich Wasserstoffpaare mit Pyruvat zu Laktat verbinden.
Während der Erholung, wenn Sauerstoff verfügbar wird, bindet NAD+ an Wasserstoff aus Laktat und bildet ATP.
In den meisten Fällen wird auch Sauerstoff als Abfallprodukt freigesetzt.
Dies ist analog zur protonentreibenden Kraft, die bei der aeroben Atmung über die innere Mitochondrienmembran erzeugt wird.
Bei der autokrinen Signalübertragung beeinflusst der Ligand dieselbe Zelle, die ihn freisetzt.
Bei Eukaryoten (d. h. Tier-, Pflanzen-, Pilz- und Protistenzellen) gibt es zwei verschiedene Arten der Zellteilung: Mitose und Meiose.
Nach der Zellteilung beginnt für jede Tochterzelle die Interphase eines neuen Zyklus.
Beide Zellteilungszyklen werden irgendwann in ihrem Lebenszyklus im Prozess der sexuellen Fortpflanzung genutzt.
Im Gegensatz zu den Prozessen der Mitose und Meiose bei Eukaryoten erfolgt die binäre Spaltung bei Prokaryoten ohne die Bildung eines Spindelapparates auf der Zelle.
Insbesondere ist die Mendelsche Vererbung der Prozess, durch den Gene und Merkmale von den Eltern an die Nachkommen weitergegeben werden.
Das erste ist, dass genetische Merkmale, die heute Allele genannt werden, diskret sind und alternative Formen haben (z. B. lila vs. weiß oder groß vs. zwergartig), die jeweils von einem von zwei Elternteilen geerbt werden.
Mendel stellte fest, dass sich während der Gametenbildung die Allele für jedes Gen voneinander trennen, sodass jeder Gamet nur ein Allel für jedes Gen trägt, was in seinem Segregationsgesetz festgelegt ist.
Die Nukleotide sind durch kovalente Bindungen zwischen dem Zucker eines Nukleotids und dem Phosphat des nächsten in einer Kette miteinander verbunden, was zu einem alternierenden Zucker-Phosphat-Rückgrat führt.
Die Basen werden in zwei Gruppen eingeteilt: Pyrimidine und Purine.
Sobald sich die beiden Stränge trennen, wird die DNA repliziert.
Ein Chromosom ist eine organisierte Struktur, die aus DNA und Histonen besteht.
Bei Prokaryoten befindet sich die DNA in einem unregelmäßig geformten Körper im Zytoplasma, dem sogenannten Nukleoid.
Die in der DNA gespeicherten genetischen Informationen stellen den Genotyp dar, während der Phänotyp aus der Synthese von Proteinen resultiert, die die Struktur und Entwicklung eines Organismus steuern oder als Enzyme fungieren, die bestimmte Stoffwechselwege katalysieren.
Unter dem genetischen Code spezifizieren diese mRNA-Stränge die Sequenz von Aminosäuren innerhalb von Proteinen in einem Prozess namens Translation, der in Ribosomen stattfindet.
Die Sequenzierung und Analyse von Genomen kann mithilfe von Hochdurchsatz-DNA-Sequenzierung und Bioinformatik erfolgen, um die Funktion und Struktur ganzer Genome zusammenzustellen und zu analysieren.
Die Genome von Prokaryoten sind klein, kompakt und vielfältig.
Der Entwicklung liegen vier Schlüsselprozesse zugrunde: Determination, Differenzierung, Morphogenese und Wachstum.
Stammzellen sind undifferenzierte oder teilweise differenzierte Zellen, die sich in verschiedene Zelltypen differenzieren und sich unbegrenzt vermehren können, um mehr von derselben Stammzelle zu produzieren.
Apoptose oder programmierter Zelltod tritt auch während der Morphogenese auf, beispielsweise beim Absterben von Zellen zwischen den Fingern in der menschlichen Embryonalentwicklung, wodurch einzelne Finger und Zehen frei werden.
Diese Toolkit-Gene sind unter den Phyla stark konserviert, was bedeutet, dass sie uralt und in weit voneinander entfernten Tiergruppen sehr ähnlich sind.
Hox-Gene bestimmen, wo sich wiederholende Teile, wie zum Beispiel die vielen Wirbel von Schlangen, in einem sich entwickelnden Embryo oder einer Larve wachsen.
Ein Toolkit-Gen kann in einem anderen Muster exprimiert werden, beispielsweise wenn der Schnabel des großen Darwin-Grundfinkens durch das BMP-Gen vergrößert wurde oder wenn Schlangen ihre Beine verloren, weil Distal-less-Gene (Dlx) unterexprimiert oder nicht exprimiert wurden alles an den Stellen, an denen andere Reptilien weiterhin ihre Gliedmaßen bildeten.
Diese Perspektive geht davon aus, dass Evolution stattfindet, wenn sich die Allelfrequenzen innerhalb einer Population sich kreuzender Organismen ändern.
Wenn selektive Kräfte fehlen oder relativ schwach sind, ist es wahrscheinlich, dass die Allelfrequenzen bei jeder nachfolgenden Generation nach oben oder unten driften, da die Allele Stichprobenfehlern unterliegen.
Auch die reproduktive Isolation nimmt tendenziell mit der genetischen Divergenz zu.
Wenn sich eine Abstammungslinie in zwei Teile teilt, wird sie im Stammbaum als Knoten (oder Spaltung) dargestellt.
Innerhalb eines Baums ist jede mit einem Namen bezeichnete Artengruppe ein Taxon (z. B. Menschen, Primaten, Säugetiere oder Wirbeltiere), und ein Taxon, das aus allen seinen evolutionären Nachkommen besteht, ist eine Klade, auch als monophyletisches Taxon bekannt.
Eine Art oder Gruppe, die eng mit der Eigengruppe verwandt ist, aber phylogenetisch außerhalb dieser liegt, wird als Außengruppe bezeichnet und dient als Bezugspunkt im Baum.
Basierend auf dem Prinzip der Sparsamkeit (oder Occams Rasiermesser) wird der Baum bevorzugt, der die geringsten evolutionären Veränderungen aufweist, die bei allen Merkmalen in allen Gruppen angenommen werden müssen.
Basierend auf diesem System erhält jede Art zwei Namen, einen für ihre Gattung und einen für ihre Art.
Biologen betrachten die Allgegenwärtigkeit des genetischen Codes als Beweis für die universelle gemeinsame Abstammung aller Bakterien, Archaeen und Eukaryoten.
Später, vor etwa 1,7 Milliarden Jahren, tauchten mehrzellige Organismen auf, bei denen differenzierte Zellen spezielle Funktionen erfüllten.
Landpflanzen waren so erfolgreich, dass man annimmt, dass sie zum Aussterben im späten Devon beigetragen haben.
Während der Erholung von dieser Katastrophe wurden Archosaurier zu den am häufigsten vorkommenden Landwirbeltieren; Eine Archosauriergruppe, die Dinosaurier, dominierte die Jura- und Kreidezeit.
Bakterien bewohnen Böden, Wasser, saure heiße Quellen, radioaktive Abfälle und die tiefe Biosphäre der Erdkruste.
Archaeen stellen die andere Domäne prokaryotischer Zellen dar und wurden ursprünglich als Bakterien klassifiziert und erhielten den Namen Archaebakterien (im Reich der Archaebakterien), ein Begriff, der nicht mehr verwendet wird.
Archaeen und Bakterien sind im Allgemeinen in Größe und Form ähnlich, obwohl einige Archaeen sehr unterschiedliche Formen haben, wie beispielsweise die flachen und quadratischen Zellen von Haloquadratum walsbyi.
Archaeen verbrauchen mehr Energiequellen als Eukaryoten: Diese reichen von organischen Verbindungen wie Zucker bis hin zu Ammoniak, Metallionen oder sogar Wasserstoffgas.
Die ersten beobachteten Archaeen waren Extremophile und lebten in extremen Umgebungen wie heißen Quellen und Salzseen ohne andere Organismen.
Archaeen sind ein wichtiger Teil des Lebens auf der Erde.
Fünf dieser Kladen werden zusammenfassend auch als Protisten bezeichnet, bei denen es sich meist um mikroskopisch kleine eukaryotische Organismen handelt, die keine Pflanzen, Pilze oder Tiere sind.
Die meisten Protisten sind Einzeller, die auch als mikrobielle Eukaryoten bezeichnet werden.
Dinoflagellaten betreiben Photosynthese und kommen im Meer vor, wo sie als Primärproduzenten organischer Stoffe eine Rolle spielen.
Ciliaten sind Alveolaten, die zahlreiche haarartige Strukturen, sogenannte Zilien, besitzen.
Bei den Ausgrabungen handelt es sich um Gruppen von Protisten, die vor etwa 1,5 Milliarden Jahren kurz nach der Entstehung der Eukaryoten begannen, sich zu diversifizieren.
Zu den Stramenopilen, von denen die meisten durch das Vorhandensein von röhrenförmigen Haaren auf der längeren ihrer beiden Geißeln gekennzeichnet sind, gehören Kieselalgen und Braunalgen.
Die Rhizarien umfassen drei Hauptgruppen: Cercozoen, Foraminiferen und Radiolarien.
Algen umfassen mehrere unterschiedliche Kladen, wie etwa Glaukophyten, bei denen es sich um mikroskopisch kleine Süßwasseralgen handelt, die in ihrer Form möglicherweise den frühen einzelligen Vorfahren der Plantae ähnelten.
Landpflanzen (Embryophyten) tauchten erstmals vor etwa 450 bis 500 Millionen Jahren in terrestrischen Umgebungen auf.
Im Gegensatz dazu sind die anderen drei Gruppen keine Gefäßpflanzen, da sie keine Tracheiden haben.
Sie sind in der Regel in Gebieten zu finden, in denen Wasser leicht verfügbar ist.
Die meisten nicht vaskulären Pflanzen kommen auf dem Land vor, einige wenige leben in Süßwasserumgebungen und keine lebt in den Ozeanen.
Zu den Gymnospermen zählen Koniferen, Palmfarne, Ginkgo und Gnetophyten.
Sie tun dies durch einen Prozess namens absorbierende Heterotrophie, bei dem sie zunächst Verdauungsenzyme absondern, die große Nahrungsmoleküle abbauen, bevor sie sie durch ihre Zellmembranen aufnehmen.
Pilze können zusammen mit zwei anderen Abstammungslinien, Choanoflagellaten und Tieren, als Opisthokonten gruppiert werden.
Vielzellige Pilze hingegen haben einen Körper namens Myzel, der aus einer Masse einzelner röhrenförmiger Filamente, sogenannten Hyphen, besteht und die Nährstoffaufnahme ermöglicht.
Mit wenigen Ausnahmen verbrauchen Tiere während der Embryonalentwicklung organisches Material, atmen Sauerstoff, sind bewegungsfähig, können sich sexuell vermehren und wachsen aus einer hohlen Zellkugel, der Blastula.
Tiere können anhand ihrer Entwicklungsmerkmale in zwei Gruppen eingeteilt werden.
Bei Protostomen entsteht aus der Blastopore der Mund, an den sich dann der Anus anschließt.
Die Körper der meisten Tiere sind symmetrisch, wobei die Symmetrie entweder radial oder bilateral ist.
Schließlich können Tiere anhand der Art und Lage ihrer Gliedmaßen unterschieden werden, wie z. B. Fühlern zur Wahrnehmung der Umgebung oder Krallen zum Fangen von Beutetieren.
Die Mehrheit (~97 %) der Tierarten sind Wirbellose, also Tiere, die weder eine vom Chorda dorsalis abgeleitete Wirbelsäule (allgemein bekannt als Rückgrat oder Wirbelsäule) besitzen noch entwickeln.
Viele Wirbellose-Taxa weisen eine größere Artenzahl und -vielfalt auf als die gesamte Untergruppe der Vertebrata.
Mehr als 6.000 Virusarten wurden detailliert beschrieben.
Wenn sich Viren nicht in einer infizierten Zelle befinden oder dabei sind, eine Zelle zu infizieren, liegen Viren in Form unabhängiger Partikel oder Virionen vor, die aus dem genetischen Material (DNA oder RNA), einer Proteinhülle namens Kapsid und in einigen Fällen einer Außenseite bestehen Hülle aus Lipiden.
Die Ursprünge von Viren in der Evolutionsgeschichte des Lebens sind unklar: Einige könnten sich aus Plasmiden entwickelt haben – DNA-Stücken, die sich zwischen Zellen bewegen können –, während andere möglicherweise aus Bakterien entstanden sind.
Viren können sich auf viele Arten verbreiten.
Noroviren und Rotaviren, häufige Erreger einer viralen Gastroenteritis, werden fäkal-oral, durch Hand-zu-Mund-Kontakt oder über die Nahrung oder das Wasser übertragen.
Das Sprosssystem besteht aus Stängel, Blättern und Blüten.
Die Richtung der Wasserbewegung durch eine semipermeable Membran wird durch das Wasserpotential entlang dieser Membran bestimmt.
Die meisten Pflanzensamen befinden sich normalerweise in einem Ruhezustand, einem Zustand, in dem die normale Aktivität des Samens unterbrochen ist.
Imbibition ist der erste Schritt der Keimung, bei dem Wasser vom Samen aufgenommen wird.
Diese Monomere werden durch Hydrolyse von Stärke, Proteinen und Lipiden gewonnen, die entweder in den Keimblättern oder im Endosperm gespeichert sind.
Ihre Blüten sind Organe, die die Fortpflanzung erleichtern, indem sie normalerweise einen Mechanismus für die Vereinigung von Spermien mit Eiern bereitstellen.
Unter Kreuzbestäubung versteht man die Übertragung von Pollen vom Staubbeutel einer Blüte auf die Narbe einer anderen Blüte auf ein anderes Individuum derselben Art.
Diese Veränderungen können durch genetische, chemische und physikalische Faktoren beeinflusst werden.
Die Photorezeptorproteine übermitteln Informationen wie Tag oder Nacht, Tagesdauer, verfügbare Lichtintensität und Lichtquelle.
Viele Blütenpflanzen blühen aufgrund lichtempfindlicher Verbindungen, die auf die Länge der Nacht reagieren, zum richtigen Zeitpunkt, ein Phänomen, das als Photoperiodismus bekannt ist.
Tiere können entweder als Regulatoren oder Konformere klassifiziert werden.
Im Gegensatz dazu sind Tiere wie Fische und Frösche Konformer, da sie ihre innere Umgebung (z. B. Körpertemperatur) an ihre äußere Umgebung anpassen.
Mäuse können beispielsweise im Verhältnis zu ihrem Gewicht dreimal mehr Nahrung aufnehmen als Kaninchen, da der Grundumsatz pro Gewichtseinheit bei Mäusen höher ist als bei Kaninchen.
Bei schwimmenden oder fliegenden Tieren ist der Zusammenhang jedoch nichtlinear.
Bei niedrigen Fluggeschwindigkeiten muss ein Vogel einen hohen Stoffwechsel aufrechterhalten, um in der Luft zu bleiben.
Schließlich haben Süßwassertiere Körperflüssigkeiten, die gegenüber Süßwasser hyperosmotisch sind.
Wenn ein Tier Nahrung zu sich nimmt, die einen Überschuss an chemischer Energie enthält, speichert es den größten Teil dieser Energie in Form von Lipiden für die zukünftige Verwendung und einen Teil dieser Energie als Glykogen für eine unmittelbarere Verwendung (z. B. zur Deckung des Energiebedarfs des Gehirns). ).
Zusätzlich zu ihrem Verdauungstrakt verfügen Wirbeltiere über Nebendrüsen wie Leber und Bauchspeicheldrüse als Teil ihres Verdauungssystems.
Beim Verlassen des Magens gelangt die Nahrung in den Mitteldarm, den ersten Teil des Darms (oder Dünndarms bei Säugetieren) und den Hauptort der Verdauung und Absorption.
Der Gasaustausch in der Lunge findet in Millionen kleiner Luftbläschen statt; Bei Säugetieren und Reptilien werden diese Alveolen genannt, bei Vögeln werden sie Vorhöfe genannt.
Diese gelangen in die Lunge, wo sie sich in zunehmend schmalere sekundäre und tertiäre Bronchien verzweigen, die sich in zahlreiche kleinere Röhren, die Bronchiolen, verzweigen.
Es gibt zwei Arten von Kreislaufsystemen: offene und geschlossene.
Bei Tieren findet die Zirkulation zwischen zwei Gewebearten statt: systemischen Geweben und Atmungsorganen (oder Lungenorganen).
Bei Vögeln und Säugetieren sind das Körper- und Lungensystem in Reihe geschaltet.
Kontraktionen der Skelettmuskulatur sind neurogen, da sie synaptische Eingaben von Motoneuronen erfordern.
Die erzeugte Kontraktion kann je nach Häufigkeit der Aktionspotentiale als Zuckung, Summation oder Tetanus beschrieben werden.
Die Kontraktionsmechanismen sind in allen drei Muskelgeweben ähnlich.
Andere Tiere wie Mollusken und Nematoden besitzen schräg gestreifte Muskeln, die Bänder aus dicken und dünnen Filamenten enthalten, die spiralförmig und nicht quer angeordnet sind, wie in den Skelett- oder Herzmuskeln von Wirbeltieren.
Sie können Informationen an Kontaktstellen, sogenannten Synapsen, übertragen oder empfangen.
Zellen wie Neuronen oder Muskelzellen können erregt oder gehemmt werden, wenn sie ein Signal von einem anderen Neuron empfangen.
Bei Wirbeltieren besteht das Nervensystem aus dem zentralen Nervensystem (ZNS), das Gehirn und Rückenmark umfasst, und dem peripheren Nervensystem (PNS), das aus Nerven besteht, die das ZNS mit allen anderen Körperteilen verbinden.
Das PNS ist in drei separate Subsysteme unterteilt: das somatische, autonome und enterische Nervensystem.
Das sympathische Nervensystem wird in Notfällen aktiviert, um Energie zu mobilisieren, während das parasympathische Nervensystem aktiviert wird, wenn sich der Organismus in einem entspannten Zustand befindet.
Nerven, die direkt aus dem Gehirn austreten, werden Hirnnerven genannt, während solche, die aus dem Rückenmark austreten, Spinalnerven genannt werden.
Insbesondere beim Menschen sind die Schilddrüse und die Nebennieren die wichtigsten endokrinen Drüsen.
Hormone können Aminosäurekomplexe, Steroide, Eicosanoide, Leukotriene oder Prostaglandine sein.
Sie produzieren durch Meiose haploide Gameten.
In den meisten Fällen entwickelt sich zwischen ihnen auch eine dritte Keimschicht, das Mesoderm.
Bei der Gastrulation wird die Zellmasse durch morphogenetische Bewegungen in drei Keimschichten umgewandelt, die aus Ektoderm, Mesoderm und Endoderm bestehen.
Die Zelldifferenzierung wird durch extrazelluläre Signale wie Wachstumsfaktoren beeinflusst, die an benachbarte Zellen ausgetauscht werden (juxtrakrine Signalübertragung) oder über kurze Distanzen an benachbarte Zellen (parakrine Signalübertragung).
Das adaptive Immunsystem reagiert auf jeden Reiz maßgeschneidert, indem es lernt, Moleküle zu erkennen, denen es zuvor begegnet ist.
Bakterien verfügen über ein rudimentäres Immunsystem in Form von Enzymen, die vor Virusinfektionen schützen.
Wirbeltiere mit Kiefern, darunter auch Menschen, verfügen über noch ausgefeiltere Abwehrmechanismen, einschließlich der Fähigkeit, sich anzupassen, um Krankheitserreger effizienter zu erkennen.
Feste Handlungsmuster beispielsweise sind genetisch bedingte und stereotype Verhaltensweisen, die ohne Lernen ablaufen.
Die Gemeinschaft lebender (biotischer) Organismen in Verbindung mit den nichtlebenden (abiotischen) Bestandteilen (z. B. Wasser, Licht, Strahlung, Temperatur, Feuchtigkeit, Atmosphäre, Säuregehalt und Boden) ihrer Umwelt wird als Ökosystem bezeichnet.
Indem sie sich von Pflanzen und untereinander ernähren, spielen Tiere eine wichtige Rolle bei der Bewegung von Materie und Energie durch das System.
Die physische Umwelt der Erde wird durch Sonnenenergie und Topographie geprägt.
Unter Wetter versteht man die tägliche Temperatur- und Niederschlagsaktivität, während Klima der langfristige Durchschnitt des Wetters ist, typischerweise gemittelt über einen Zeitraum von 30 Jahren.
Infolgedessen ermöglichen feuchte Umgebungen das Wachstum üppiger Vegetation.
Das Bevölkerungswachstum in kurzfristigen Zeiträumen kann mithilfe der Gleichung für die Bevölkerungswachstumsrate bestimmt werden, die Geburten-, Sterbe- und Einwanderungsraten berücksichtigt.
Eine biologische Interaktion ist die Wirkung, die ein Paar in einer Gemeinschaft zusammenlebender Organismen aufeinander hat.
Eine langfristige Interaktion wird als Symbiose bezeichnet.
Innerhalb jedes Nahrungsnetzes gibt es verschiedene trophische Ebenen, wobei die niedrigste Ebene die Primärproduzenten (oder Autotrophen) wie Pflanzen und Algen sind, die Energie und anorganisches Material in organische Verbindungen umwandeln, die dann vom Rest der Gemeinschaft genutzt werden können.
Und diejenigen, die Sekundärverbraucher essen, sind Tertiärverbraucher und so weiter.
In manchen Kreisläufen gibt es Reservoire, in denen ein Stoff über einen längeren Zeitraum verbleibt oder gebunden wird.
Der größte Treiber der Erwärmung ist der Ausstoß von Treibhausgasen, von denen mehr als 90 % Kohlendioxid und Methan sind.
Die biologische Vielfalt beeinflusst das Funktionieren von Ökosystemen, die eine Vielzahl von Dienstleistungen erbringen, auf die die Menschen angewiesen sind.
Traditionell umfasst die Botanik auch die Erforschung von Pilzen und Algen durch Mykologen bzw. Phykologen, wobei die Erforschung dieser drei Organismengruppen weiterhin im Interessenbereich des Internationalen Botanischen Kongresses liegt.
In mittelalterlichen Heilgärten, die oft an Klöster angeschlossen waren, befanden sich Pflanzen von medizinischer Bedeutung.
Diese Gärten erleichterten das akademische Studium von Pflanzen.
In den letzten zwei Jahrzehnten des 20. Jahrhunderts nutzten Botaniker Techniken der molekulargenetischen Analyse, einschließlich Genomik und Proteomik sowie DNA-Sequenzen, um Pflanzen genauer zu klassifizieren.
Die Wurzeln der modernen Botanik gehen auf das antike Griechenland zurück, insbesondere auf Theophrastus (ca. 371–287 v. Chr.), einen Schüler von Aristoteles, der viele ihrer Prinzipien erfand und beschrieb und in der wissenschaftlichen Gemeinschaft weithin als „Vater der Botanik“ gilt.
De Materia Medica wurde mehr als 1.500 Jahre lang viel gelesen.
Mitte des 16. Jahrhunderts wurden an mehreren italienischen Universitäten botanische Gärten gegründet.
Sie unterstützten die Entwicklung der Botanik als akademisches Fach.
Während dieser Zeit blieb die Botanik der Medizin stark untergeordnet.
Bock schuf sein eigenes System zur Pflanzenklassifizierung.
Die Wahl und Reihenfolge der Zeichen kann bei Schlüsseln, die ausschließlich der Identifizierung dienen (Diagnoseschlüssel), künstlich sein oder bei synoptischen Schlüsseln enger mit der natürlichen oder phyletischen Reihenfolge der Taxa verknüpft sein.
Dadurch wurde ein standardisiertes binomiales oder zweiteiliges Benennungsschema eingeführt, bei dem der erste Name die Gattung darstellte und der zweite die Art innerhalb der Gattung identifizierte.
Zunehmende Kenntnisse über die Anatomie, Morphologie und Lebenszyklen von Pflanzen führten zu der Erkenntnis, dass es mehr natürliche Verwandtschaften zwischen Pflanzen gab als das künstliche Sexualsystem von Linnaeus.
Die Arbeiten von Katherine Esau (1898–1997) zur Pflanzenanatomie sind noch immer eine wichtige Grundlage der modernen Botanik.
Das Konzept, dass sich die Zusammensetzung von Pflanzengemeinschaften wie Laubwäldern der gemäßigten Zonen durch einen Prozess der ökologischen Sukzession verändert, wurde von Henry Chandler Cowles, Arthur Tansley und Frederic Clements entwickelt.
Die Entdeckung und Identifizierung der Auxin-Pflanzenhormone durch Kenneth V. Thimann im Jahr 1948 ermöglichte die Regulierung des Pflanzenwachstums durch äußerlich angewendete Chemikalien.
Die Entwicklungen in der Pflanzenbiochemie des 20. Jahrhunderts wurden durch moderne Techniken der organischen chemischen Analyse wie Spektroskopie, Chromatographie und Elektrophorese vorangetrieben.
Diese Technologien ermöglichen die biotechnologische Nutzung ganzer Pflanzen oder in Bioreaktoren gezüchteter Pflanzenzellkulturen zur Synthese von Pestiziden, Antibiotika oder anderen Arzneimitteln sowie die praktische Anwendung gentechnisch veränderter Nutzpflanzen, die auf Eigenschaften wie einen verbesserten Ertrag ausgelegt sind.
Ziel der modernen Systematik ist es, phylogenetische Beziehungen zwischen Pflanzen zu reflektieren und zu entdecken.
Als Nebenprodukt der Photosynthese geben Pflanzen Sauerstoff an die Atmosphäre ab, ein Gas, das fast alle Lebewesen für die Zellatmung benötigen.
Historisch gesehen wurden alle Lebewesen entweder als Tiere oder als Pflanzen klassifiziert, und die Botanik umfasste die Untersuchung aller Organismen, die nicht als Tiere galten.
Die strengste Definition von „Pflanze“ umfasst nur die „Landpflanzen“ oder Embryophyten, zu denen Samenpflanzen (Gymnospermen, einschließlich Kiefern und Blütenpflanzen) und die frei sporenbildenden Kryptogamen einschließlich Farnen, Bärenmoosen, Lebermoosen, Hornmoosen und Moosen gehören.
Die sexuelle haploide Phase von Embryophyten, bekannt als Gametophyt, ernährt den sich entwickelnden diploiden Embryo-Sporophyten zumindest für einen Teil seines Lebens in seinem Gewebe, selbst in den Samenpflanzen, wo der Gametophyt selbst von seinem Eltern-Sporophyten ernährt wird.
Paläobotaniker untersuchen antike Pflanzen im Fossilienbestand, um Informationen über die Evolutionsgeschichte der Pflanzen zu erhalten.
Dies nennen Ökologen die erste trophische Ebene.
Botaniker untersuchen auch Unkräuter, die in der Landwirtschaft ein erhebliches Problem darstellen, sowie die Biologie und Bekämpfung von Pflanzenpathogenen in der Landwirtschaft und in natürlichen Ökosystemen.
Die von Chlorophyll a eingefangene Lichtenergie liegt zunächst in Form von Elektronen (und später eines Protonengradienten) vor, die zur Herstellung von ATP- und NADPH-Molekülen verwendet werden, die vorübergehend Energie speichern und transportieren.
Ein Teil der Glukose wird in Stärke umgewandelt, die im Chloroplasten gespeichert wird.
Anders als bei Tieren (denen Chloroplasten fehlen) haben Pflanzen und ihre eukaryotischen Verwandten viele biochemische Aufgaben an ihre Chloroplasten delegiert, darunter die Synthese aller ihrer Fettsäuren und der meisten Aminosäuren.
Gefäßlandpflanzen produzieren Lignin, ein Polymer, das zur Stärkung der sekundären Zellwände von Xylem-Tracheiden und Gefäßen verwendet wird, um zu verhindern, dass diese kollabieren, wenn eine Pflanze bei Wasserstress Wasser durch sie saugt.
Andere, wie die ätherischen Öle Pfefferminzöl und Zitronenöl, sind wegen ihres Aromas, als Aromastoffe und Gewürze (z. B. Capsaicin) und in der Medizin als Arzneimittel wie Opium aus Schlafmohn nützlich.
Beispielsweise ist das Schmerzmittel Aspirin der Acetylester der Salicylsäure, der ursprünglich aus der Rinde von Weidenbäumen isoliert wurde, und eine Vielzahl von Opiat-Schmerzmitteln wie Heroin werden durch chemische Modifikation von Morphin gewonnen, das aus dem Schlafmohn gewonnen wird.
Die amerikanischen Ureinwohner nutzen seit Tausenden von Jahren verschiedene Pflanzen zur Behandlung von Krankheiten.
Zucker, Stärke, Baumwolle, Leinen, Hanf, einige Arten von Seilen, Holz und Spanplatten, Papyrus und Papier, Pflanzenöle, Wachs und Naturkautschuk sind Beispiele für kommerziell wichtige Materialien, die aus Pflanzengeweben oder deren Folgeprodukten hergestellt werden.
Zu den aus Zellulose hergestellten Produkten zählen Viskose und Zellophan, Tapetenkleister, Biobutanol und Schießbaumwolle.
Einige Ökologen stützen sich sogar auf empirische Daten indigener Völker, die von Ethnobotanikern gesammelt werden.
Pflanzen sind von bestimmten edaphischen (Boden) und klimatischen Faktoren in ihrer Umgebung abhängig, können diese Faktoren jedoch auch verändern.
Sie interagieren mit ihren Nachbarn auf verschiedenen räumlichen Ebenen in Gruppen, Populationen und Gemeinschaften, die gemeinsam die Vegetation bilden.
Gregor Mendel entdeckte die genetischen Gesetze der Vererbung, indem er vererbte Merkmale wie die Form bei Pisum sativum (Erbsen) untersuchte.
Dennoch gibt es einige deutliche genetische Unterschiede zwischen Pflanzen und anderen Organismen.
Die vielen angebauten Weizensorten sind das Ergebnis mehrfacher inter- und intraspezifischer Kreuzungen zwischen Wildarten und ihren Hybriden.
In vielen Landpflanzen werden die männlichen und weiblichen Gameten von getrennten Individuen produziert.
Ein Beispiel ist die Bildung von Stängelknollen bei Kartoffeln.
Apomixis kann auch bei einem Samen auftreten, wobei ein Samen entsteht, der einen Embryo enthält, der genetisch mit dem Elternteil identisch ist.
Eine allopolyploide Pflanze kann aus einem Hybridisierungsereignis zwischen zwei verschiedenen Arten entstehen.
Einige ansonsten sterile Pflanzenpolyploide können sich immer noch vegetativ oder durch Samenapomixis vermehren und klonale Populationen identischer Individuen bilden.
Gewöhnlicher Löwenzahn ist ein Triploid, das durch apomiktische Samen lebensfähige Samen produziert.
Die Sequenzierung einiger anderer relativ kleiner Genome, von Reis (Oryza sativa) und Brachypodium distachyon, hat sie zu wichtigen Modellarten für das Verständnis der Genetik, Zell- und Molekularbiologie von Getreide, Gräsern und Monokotyledonen im Allgemeinen gemacht.
Spinat, Erbsen, Sojabohnen und ein Moos Physcomitrella patens werden häufig zur Untersuchung der Pflanzenzellbiologie verwendet.
Die Genexpression kann auch durch Repressorproteine gesteuert werden, die sich an Silencer-Regionen der DNA binden und die Expression dieser Region des DNA-Codes verhindern.
Es hat sich gezeigt, dass einige epigenetische Veränderungen vererbbar sind, während andere in den Keimzellen zurückgesetzt werden.
Im Gegensatz zu Tieren differenzieren sich viele Pflanzenzellen, insbesondere die des Parenchyms, nicht endgültig, sondern bleiben totipotent und haben die Fähigkeit, eine neue einzelne Pflanze hervorzubringen.
Die Algen sind eine polyphyletische Gruppe und werden in verschiedene Abteilungen eingeteilt, von denen einige näher mit Pflanzen verwandt sind als andere.
Die Charophytenklasse Charophyceae und das Landpflanzen-Unterreich Embryophyta bilden zusammen die monophyletische Gruppe oder Klade Streptophytina.
Pteridophytische Gefäßpflanzen mit echtem Xylem und Phloem, die sich durch Sporen vermehrten, die zu frei lebenden Gametophyten keimten, entwickelten sich während der Silurzeit und diversifizierten sich im späten Silur und frühen Devon in mehrere Abstammungslinien.
Ihre reduzierten Gametophyten entwickelten sich aus Megasporen, die in den sporenproduzierenden Organen (Megasporangien) des Sporophyten zurückblieben, ein Zustand, der als Endosporie bekannt ist.
Die frühesten bekannten Samenpflanzen stammen aus dem letzten Famennium-Stadium des Devon.
Aus der Luft, dem Boden und dem Wasser gewonnene Chemikalien bilden die Grundlage des gesamten pflanzlichen Stoffwechsels.
Heterotrophe Organismen, darunter alle Tiere, alle Pilze, alle vollständig parasitären Pflanzen und nicht photosynthetischen Bakterien, nehmen von Photoautotrophen produzierte organische Moleküle auf und veratmen sie oder verwenden sie beim Aufbau von Zellen und Geweben.
Der subzelluläre Transport von Ionen, Elektronen und Molekülen wie Wasser und Enzymen erfolgt über Zellmembranen.
Beispiele für Elemente, die Pflanzen transportieren müssen, sind Stickstoff, Phosphor, Kalium, Kalzium, Magnesium und Schwefel.
Diese Verbindung vermittelt die tropischen Reaktionen von Trieben und Wurzeln auf Licht und Schwerkraft.
Das natürliche Zytokinin Zeatin wurde im Mais, Zea mays, entdeckt und ist ein Derivat des Purin-Adenins.
Sie sind an der Förderung der Keimung und der Unterbrechung der Samenruhe sowie an der Regulierung der Pflanzenhöhe durch Steuerung der Stängelverlängerung und der Blüte beteiligt.
Es wurde so genannt, weil ursprünglich angenommen wurde, dass es die Abszision kontrolliert.
Eine weitere Klasse von Phytohormonen sind die Jasmonate, die erstmals aus dem Öl von Jasminum grandiflorum isoliert wurden und die Wundreaktionen in Pflanzen regulieren, indem sie die Expression von Genen freigeben, die für die systemisch erworbene Resistenzreaktion auf einen Krankheitserregerangriff erforderlich sind.
Nicht-Gefäßpflanzen, Leberblümchen, Hornkraut und Moose, produzieren keine bodendurchdringenden Gefäßwurzeln und der größte Teil der Pflanze ist an der Photosynthese beteiligt.
Zellen in jedem System sind in der Lage, Zellen des anderen zu erzeugen und Adventivsprossen oder Wurzeln zu produzieren.
Für den Fall, dass eines der Systeme verloren geht, kann es oft durch das andere nachwachsen.
Bei Gefäßpflanzen sind Xylem und Phloem die leitenden Gewebe, die Ressourcen zwischen Trieben und Wurzeln transportieren.
Blätter sammeln Sonnenlicht und betreiben Photosynthese.
Angiospermen sind samenproduzierende Pflanzen, die Blüten produzieren und über umschlossene Samen verfügen.
Manche Pflanzen vermehren sich sexuell, manche ungeschlechtlich und manche auf beide Arten.
Die biologische Klassifikation ist eine Form der wissenschaftlichen Taxonomie.
Während sich Wissenschaftler nicht immer darüber einig sind, wie Organismen zu klassifizieren sind, hat die molekulare Phylogenetik, die DNA-Sequenzen als Daten verwendet, in jüngster Zeit viele Überarbeitungen entlang evolutionärer Linien vorangetrieben und wird dies wahrscheinlich auch weiterhin tun.
Die Nomenklatur botanischer Organismen ist im International Code of Nomenclature for Algae, Fungi, and Plants (ICN) kodifiziert und wird vom International Botanical Congress verwaltet.
Der wissenschaftliche Name einer Pflanze repräsentiert ihre Gattung und ihre Art innerhalb der Gattung, was zu einem einzigen weltweiten Namen für jeden Organismus führt.
Die Kombination ist der Name der Art.
Die evolutionären Beziehungen und die Vererbung einer Gruppe von Organismen werden als Phylogenie bezeichnet.
Pereskia-Arten sind beispielsweise Bäume oder Sträucher mit hervorstehenden Blättern.
Die Beurteilung von Beziehungen auf der Grundlage gemeinsamer Merkmale erfordert Sorgfalt, da Pflanzen einander durch konvergente Evolution ähneln können, bei der Merkmale unabhängig voneinander entstanden sind.
Lediglich abgeleitete Merkmale wie die stachelbildenden Areolen von Kakteen belegen die Abstammung von einem gemeinsamen Vorfahren.
Der Unterschied besteht darin, dass der genetische Code selbst zur Entscheidung über evolutionäre Beziehungen verwendet wird, anstatt indirekt über die Charaktere, die er hervorbringt.
Genetische Beweise deuten darauf hin, dass die wahre evolutionäre Verwandtschaft mehrzelliger Organismen wie im folgenden Cladogramm dargestellt ist – Pilze sind enger mit Tieren als mit Pflanzen verwandt.
Durch die Untersuchung der Verwandtschaft zwischen Pflanzenarten können Botaniker den Prozess der Pflanzenevolution besser verstehen.
Obwohl sich der Mensch schon immer für die Naturgeschichte der Tiere, die er um sich herum sah, interessierte und dieses Wissen nutzte, um bestimmte Arten zu domestizieren, kann man sagen, dass das formale Studium der Zoologie seinen Ursprung bei Aristoteles hat.
Die moderne Zoologie hat ihren Ursprung in der Renaissance und der frühen Neuzeit mit Carl Linnaeus, Antonie van Leeuwenhoek, Robert Hooke, Charles Darwin, Gregor Mendel und vielen anderen.
In Frankreich gibt es Höhlenmalereien, Gravuren und Skulpturen, die 15.000 Jahre alt sind und Bisons, Pferde und Hirsche in sorgfältiger Detaildarstellung zeigen.
Das alte Wissen über Wildtiere wird durch realistische Darstellungen von Wild- und Haustieren im Nahen Osten, Mesopotamien und Ägypten veranschaulicht, einschließlich Haltungspraktiken und -techniken, Jagd und Fischerei.
Aristoteles betrachtete im vierten Jahrhundert v. Chr. Tiere als lebende Organismen und untersuchte ihre Struktur, Entwicklung und Lebensphänomene.
Vierhundert Jahre später sezierte der römische Arzt Galen Tiere, um ihre Anatomie und die Funktion der verschiedenen Teile zu untersuchen, da die Sektion menschlicher Kadaver damals verboten war.
In Europa blieb Galens Werk zur Anatomie bis zum 16. Jahrhundert weitgehend unübertroffen und unangefochten.
War die Zoologie zuvor im 18., 19. und 20. Jahrhundert eine Domäne der Herren Naturforscher, entwickelte sie sich zu einer zunehmend professionellen wissenschaftlichen Disziplin.
Diese Entwicklungen sowie die Ergebnisse der Embryologie und Paläontologie wurden in der Veröffentlichung von Charles Darwins Theorie der Evolution durch natürliche Selektion im Jahr 1859 zusammengefasst; Damit stellte Darwin die Theorie der organischen Evolution auf eine neue Grundlage, indem er die Prozesse erklärte, durch die sie stattfinden kann, und Beobachtungsnachweise dafür lieferte.
Darwin gab der Morphologie und Physiologie eine neue Richtung, indem er sie in einer gemeinsamen biologischen Theorie vereinte: der Theorie der organischen Evolution.
Eine frühe Notwendigkeit bestand darin, die Organismen zu identifizieren und sie nach ihren Merkmalen, Unterschieden und Beziehungen zu gruppieren, und dies ist das Fachgebiet des Taxonomen.
Im Mittelpunkt seiner Ideen stand die Morphologie der Tiere.
Diese Gruppierungen wurden inzwischen überarbeitet, um die Übereinstimmung mit dem darwinistischen Prinzip der gemeinsamen Abstammung zu verbessern.
Homo ist die Gattung und Sapiens das Artepitheton; beide zusammen ergeben den Artnamen.
Das vorherrschende Klassifizierungssystem wird als Linné-Taxonomie bezeichnet.
Das Verständnis der Struktur und Funktion von Zellen ist für alle biologischen Wissenschaften von grundlegender Bedeutung.
Der Schwerpunkt liegt auf der Frage, wie Organe und Organsysteme im Körper von Menschen und Tieren zusammenarbeiten und wie sie unabhängig voneinander funktionieren.
Physiologische Studien wurden traditionell in Pflanzenphysiologie und Tierphysiologie unterteilt, einige Prinzipien der Physiologie sind jedoch universell, unabhängig davon, welcher bestimmte Organismus untersucht wird.
Dabei handelt es sich beispielsweise im Allgemeinen um Wissenschaftler, die über eine spezielle Ausbildung in bestimmten Organismen wie Säugetierkunde, Ornithologie, Herpetologie oder Entomologie verfügen, diese Organismen jedoch als Systeme zur Beantwortung allgemeiner Fragen zur Evolution nutzen.
Ethologen haben sich insbesondere mit der Entwicklung des Verhaltens und dem Verhaltensverständnis im Hinblick auf die Theorie der natürlichen Selektion beschäftigt.
Während Forscher spezifische Techniken der Molekularbiologie anwenden, ist es üblich, diese mit Methoden aus der Genetik und Biochemie zu kombinieren.
Die biologische Systematik befasst sich mit der Diversifizierung vergangener und gegenwärtiger Lebewesen und den Beziehungen zwischen Lebewesen im Laufe der Zeit.
Phylogenetische Bäume von Arten und höheren Taxa werden verwendet, um die Entwicklung von Merkmalen (z. B. anatomische oder molekulare Merkmale) und die Verbreitung von Organismen (Biogeographie) zu untersuchen.
Die biologische Systematik klassifiziert Arten anhand von drei spezifischen Zweigen.
Die experimentelle Systematik identifiziert und klassifiziert Tiere anhand der evolutionären Einheiten, aus denen eine Art besteht, sowie ihrer Bedeutung für die Evolution selbst.
Erklärung der Artenvielfalt des Planeten und seiner Organismen.
Taxonomie ist der Teil der Systematik, der sich mit den oben genannten Themen (a) bis (d) befasst.
Im modernen Sprachgebrauch können sie jedoch alle als Synonyme voneinander betrachtet werden.
Einige behaupten, dass sich die Systematik allein speziell mit Beziehungen im Zeitverlauf befasst und dass sie ein Synonym für die Phylogenetik sein kann, die sich im Großen und Ganzen mit der abgeleiteten Hierarchie von Organismen befasst.
Wissenschaftliche Klassifikationen sind Hilfsmittel zur Erfassung und Weitergabe von Informationen an andere Wissenschaftler und Laien.
In der Biologie ist eine Art die grundlegende Klassifizierungseinheit und ein taxonomischer Rang eines Organismus sowie eine Einheit der Artenvielfalt.
Darüber hinaus verwenden Paläontologen das Konzept der Chronospezies, da die Vermehrung von Fossilien nicht untersucht werden kann.
Alle Arten (außer Viren) erhalten einen zweiteiligen Namen, ein „Binom“.
Beispielsweise ist Boa constrictor eine von vier Arten der Gattung Boa, wobei constrictor der Beiname der Art ist.
Auch bei Organismen, die sich nur ungeschlechtlich vermehren, bricht das Konzept einer reproduktiven Art zusammen und jeder Klon ist potenziell eine Mikroart.
Von der Zeit des Aristoteles bis zum 18. Jahrhundert galten Arten als feste Kategorien, die in einer Hierarchie, der großen Kette des Seins, angeordnet werden konnten.
Dieses Verständnis wurde im 20. Jahrhundert durch Genetik und Populationsökologie erheblich erweitert.
Ernst Mayr betonte die reproduktive Isolation, aber diese ist wie andere Artenkonzepte schwer oder gar nicht zu überprüfen.
Diese Methode wurde als „klassische“ Methode zur Artenbestimmung verwendet, beispielsweise bei Linnaeus zu Beginn der Evolutionstheorie.
Als Faustregel gehen Mikrobiologen davon aus, dass Arten von Bakterien oder Archaeen mit 16S-ribosomalen RNA-Gensequenzen, die einander zu mehr als 97 % ähneln, durch DNA-DNA-Hybridisierung überprüft werden müssen, um zu entscheiden, ob sie zur gleichen Art gehören oder nicht.
Moderne Ansätze vergleichen Sequenzähnlichkeit mithilfe rechnerischer Methoden.
Eine Datenbank, Barcode of Life Data Systems (BOLD), enthält DNA-Barcode-Sequenzen von über 190.000 Arten.
In einer Studie über Pilze beispielsweise lieferte die Untersuchung der Nukleotidmerkmale anhand kladistischer Arten die genauesten Ergebnisse bei der Erkennung der zahlreichen Pilzarten aller untersuchten Konzepte.
Wieder andere verteidigen diesen Ansatz, indem sie „taxonomische Inflation“ als abwertend betrachten und die gegenteilige Ansicht als „taxonomischen Konservatismus“ bezeichnen; Sie behaupten, es sei politisch sinnvoll, Arten aufzuspalten und kleinere Populationen auf Artenebene anzuerkennen, weil dies bedeute, dass sie leichter als gefährdet in die Rote Liste der IUCN aufgenommen werden könnten und Schutzgesetze und -finanzierungen anziehen könnten.
Wenn Wissenschaftler meinen, dass etwas für alle Arten innerhalb einer Gattung gilt, verwenden sie den Gattungsnamen ohne den spezifischen Namen oder Beinamen.
Wenn weitere Informationen vorliegen, kann die Hypothese bestätigt oder widerlegt werden.
Die Aufteilung eines Taxons in mehrere, oft neue Taxa wird als Aufteilung bezeichnet.
Der Begriff Quasispezies wird manchmal für schnell mutierende Entitäten wie Viren verwendet.
Bei Ringarten kreuzen sich Mitglieder benachbarter Populationen in einem weitgehend zusammenhängenden Verbreitungsgebiet erfolgreich, Mitglieder weiter entfernter Populationen jedoch nicht.
Ringarten stellen daher eine Schwierigkeit für jedes Artenkonzept dar, das auf reproduktiver Isolation beruht.
Die Artbildung hängt von einem Maß der reproduktiven Isolation ab, einem reduzierten Genfluss.
Bakterien können Plasmide mit Bakterien anderer Arten austauschen, einschließlich einiger scheinbar entfernt verwandter Bakterien in unterschiedlichen phylogenetischen Domänen, was die Analyse ihrer Beziehungen erschwert und das Konzept einer Bakterienart schwächt.
Massenaussterben hatten eine Vielzahl von Ursachen, darunter vulkanische Aktivität, Klimawandel und Veränderungen in der Chemie der Ozeane und der Atmosphäre, und sie hatten wiederum erhebliche Auswirkungen auf die Ökologie, die Atmosphäre, die Landoberfläche und die Gewässer der Erde.
Einige Beobachter behaupten, dass es einen inhärenten Konflikt zwischen dem Wunsch, die Prozesse der Artbildung zu verstehen, und der Notwendigkeit, sie zu identifizieren und zu kategorisieren, gibt.
Einer der klassischen Fälle in Nordamerika ist der geschützte Waldkauz, der mit dem ungeschützten Waldkauz und dem Streifenkauz hybridisiert; Dies hat zu rechtlichen Debatten geführt.
Eine Form zeichnete sich dadurch aus, dass sie allen ihren Mitgliedern gemeinsam war und die Jungen etwaige Variationen von ihren Eltern erbten.
Er begründete die Idee einer taxonomischen Klassifizierungshierarchie, die auf beobachtbaren Merkmalen basiert und natürliche Zusammenhänge widerspiegeln soll.
Jean-Baptiste Lamarck beschrieb in seiner Zoologischen Philosophie von 1809 die Transmutation von Arten und schlug vor, dass sich eine Art im Laufe der Zeit verändern könne, was eine radikale Abkehr vom aristotelischen Denken darstellt.
Gattung (Plural Gattungen) ist eine taxonomische Rangordnung, die bei der biologischen Klassifizierung lebender und fossiler Organismen sowie Viren verwendet wird.
Z.B. Panthera leo (Löwe) und Panthera onca (Jaguar) sind zwei Arten innerhalb der Gattung Panthera.
Ein botanisches Beispiel wäre Hibiscus arnottianus, eine besondere Art der Gattung Hibiscus, die auf Hawaii heimisch ist.
Verfügbare Namen sind diejenigen, die gemäß dem International Code of Zoological Nomenclature veröffentlicht und nicht anderweitig durch spätere Entscheidungen der International Commission on Zoological Nomenclature (ICZN) unterdrückt wurden; Der früheste derartige Name für ein Taxon (z. B. eine Gattung) sollte dann als „gültiger“ (d. h. aktueller oder akzeptierter) Name für das betreffende Taxon ausgewählt werden.
In der Botanik gibt es ähnliche Konzepte, jedoch mit unterschiedlichen Bezeichnungen.
Viele Namen wurden jedoch (normalerweise unbeabsichtigt) zwei oder mehr verschiedenen Gattungen zugeordnet.
Ein Name, der zwei verschiedene Dinge bedeutet, ist ein Homonym.
Eine Gattung in einem Königreich darf jedoch einen wissenschaftlichen Namen tragen, der als Gattungsname (oder als Name eines Taxons in einem anderen Rang) in einem Königreich verwendet wird, für das ein anderer Nomenklaturcode gilt.
Beispielsweise gibt es unter den (Nicht-Vogel-)Reptilien, die etwa 1180 Gattungen haben, die meisten (>300) nur eine Art, ~360 haben zwischen 2 und 4 Arten, 260 haben 5–10 Arten, ~200 haben 11–50 Arten, und nur 27 Gattungen haben mehr als 50 Arten.
Welche Arten einer Gattung zugeordnet werden, ist eher willkürlich.
Was zu einer Familie gehört – oder ob eine beschriebene Familie überhaupt anerkannt werden soll – wird von praktizierenden Taxonomen vorgeschlagen und festgelegt.
Oft gibt es keine genaue Übereinstimmung, da verschiedene Taxonomen jeweils eine unterschiedliche Position vertreten.
Michael Novacek (1986) hat sie an der gleichen Stelle eingefügt.
Es gibt keine objektiven Regeln für die Beschreibung einer Klasse, aber für bekannte Tiere besteht wahrscheinlich ein Konsens.
In der Botanik wird heute kaum noch über Unterricht gesprochen.
Informell kann man sich Phyla als Gruppierungen von Organismen vorstellen, die auf einer allgemeinen Spezialisierung des Körperbaus basieren.
Daher können Stämme verschmolzen oder geteilt werden, wenn sich herausstellt, dass sie miteinander verwandt sind oder nicht.
Nach der Definition von Budd und Jensen wird ein Stamm durch eine Reihe von Charakteren definiert, die allen seinen lebenden Vertretern gemeinsam sind.
Da es jedoch charakterbasiert ist, lässt es sich leicht auf den Fossilienbestand anwenden.
Der Nachweis, dass ein Fossil zur Kronengruppe eines Stammes gehört, ist jedoch schwierig, da es einen Charakter aufweisen muss, der für eine Untergruppe der Kronengruppe einzigartig ist.
Die folgende Tabelle folgt dem einflussreichen (wenn auch umstrittenen) Cavalier-Smith-System bei der Gleichsetzung von „Plantae“ mit Archaeplastida, einer Gruppe, die Viridiplantae und die Algenabteilungen Rhodophyta und Glaucophyta umfasst.
Die Unterteilung Pinophyta kann für alle Gymnospermen (d. h. einschließlich Palmfarne, Ginkgos und Gnetophyten) oder nur für Nadelbäume verwendet werden, wie unten beschrieben.
Protista ist ein polyphyletisches Taxon, das von heutigen Biologen weniger akzeptiert wird als in der Vergangenheit.
Carl Linnaeus (1707–1778) legte 1735 den Grundstein für die moderne biologische Nomenklatur, die heute durch die Nomenklaturkodizes geregelt wird.
1937 führte Édouard Chatton die Begriffe „Prokaryoten“ und „Eukaryoten“ ein, um diese Organismen zu unterscheiden.
Robert Whittaker erkannte ein weiteres Königreich für die Pilze.
Die verbleibenden beiden Königreiche, Protista und Monera, umfassten einzellige und einfachzellige Kolonien.
In anderen Systemen, wie dem System der fünf Königreiche von Lynn Margulis, umfassten die Pflanzen nur die Landpflanzen (Embryophyta), und Protoctista hat eine umfassendere Definition.
Technologische Fortschritte in der Elektronenmikroskopie ermöglichten die Trennung der Chromista vom Plantae-Königreich.
Schließlich wurden einige Protisten ohne Mitochondrien entdeckt.
Dieses Superreich stand im Gegensatz zum Metakaryota-Superreich und umfasste die fünf anderen eukaryotischen Königreiche (Animalia, Protozoa, Fungi, Plantae und Chromista).
Cavalier-Smith akzeptierte die Bedeutung der grundlegenden Eubakterien-Archaebakterien-Kluft, die von Woese und anderen vertreten und durch neuere Forschungen gestützt wurde, nicht länger.
Cavalier-Smith akzeptiert nicht, dass die Anforderung, dass Taxa monophyletisch („holophyletisch“ in seiner Terminologie) sein müssen, gültig ist.
Die Fortschritte phylogenetischer Studien ermöglichten es Cavalier-Smith zu erkennen, dass alle Phyla, die man für Archezoen hielt (d. h. primitiv amitochondrische Eukaryoten), tatsächlich sekundär ihre Mitochondrien verloren hatten, typischerweise durch die Umwandlung in neue Organellen: Hydrogenosomen.
Basierend auf solchen RNA-Studien dachte Carl Woese, dass das Leben in drei große Bereiche unterteilt werden könnte und bezeichnete sie als „Modell der drei primären Königreiche“ oder „Urkönigreich“-Modell.
Woese teilte die Prokaryoten (früher als Königreich Monera klassifiziert) in zwei Gruppen ein, die Eubakterien und Archaebakterien genannt wurden, und betonte, dass es zwischen diesen beiden Gruppen ebenso große genetische Unterschiede gebe wie zwischen jeder von ihnen und allen Eukaryoten.
Sie vertraten die Auffassung, dass nur monophyletische Gruppen als formale Ränge in einer Klassifikation akzeptiert werden sollten und dass – obwohl dieser Ansatz zuvor unpraktisch gewesen sei (was „buchstäblich Dutzende von eukaryotischen Königreichen“ erforderlich machte) – es nun möglich geworden sei, die Eukaryoten in „nur wenige“ zu unterteilen Hauptgruppen, die wahrscheinlich alle monophyletisch sind.
Es teilte die Eukaryoten in dieselben sechs „Supergruppen“ ein.
Es wird angenommen, dass Pflanzen weiter entfernt mit Tieren und Pilzen verwandt sind.
Zu den zehn Argumenten dagegen gehört die Tatsache, dass es sich um obligat intrazelluläre Parasiten handelt, denen es an Stoffwechsel mangelt und die nicht in der Lage sind, sich außerhalb einer Wirtszelle zu vermehren.
Bei den ersten beiden handelt es sich ausschließlich um prokaryotische Mikroorganismen, also meist einzellige Organismen, deren Zellen einen verzerrten oder nicht membrangebundenen Zellkern haben.
Beispiele für Archaeen sind Halophile, Organismen, die in stark salzhaltigen Umgebungen gedeihen, und Hyperthermophile, Organismen, die in extrem heißen Umgebungen gedeihen.
Cyanobakterien und Mykoplasmen sind zwei Beispiele für Bakterien.
Unter Evolution versteht man die Veränderung der vererbbaren Merkmale biologischer Populationen im Laufe aufeinanderfolgender Generationen.
Evolution liegt vor, wenn evolutionäre Prozesse wie natürliche Selektion (einschließlich sexueller Selektion) und genetische Drift auf diese Variation einwirken und dazu führen, dass bestimmte Merkmale innerhalb einer Population häufiger oder seltener werden.
Die wissenschaftliche Theorie der Evolution durch natürliche Selektion wurde Mitte des 19. Jahrhunderts unabhängig voneinander von Charles Darwin und Alfred Russel Wallace konzipiert und in Darwins Buch „On the Origin of Species“ ausführlich dargelegt.
Daher ist es wahrscheinlicher, dass Mitglieder einer Population in aufeinanderfolgenden Generationen durch Nachkommen von Eltern mit günstigen Eigenschaften ersetzt werden, die es ihnen ermöglicht haben, in ihrer jeweiligen Umgebung zu überleben und sich fortzupflanzen.
Der Fossilienbestand umfasst eine Entwicklung vom frühen biogenen Graphit über mikrobielle Mattenfossilien bis hin zu versteinerten mehrzelligen Organismen.
Sie suchte nach Erklärungen für Naturphänomene anhand physikalischer Gesetze, die für alle sichtbaren Dinge gleich waren und die keine festen natürlichen Kategorien oder eine göttliche kosmische Ordnung erforderten.
Die von Carl Linnaeus 1735 eingeführte biologische Klassifikation erkannte ausdrücklich die hierarchische Natur der Artenbeziehungen an, betrachtete die Arten jedoch weiterhin als nach einem göttlichen Plan festgelegt.
Diese Ideen wurden von etablierten Naturforschern als Spekulationen ohne empirische Unterstützung verurteilt.
Teilweise beeinflusst durch An Essay on the Principle of Population (1798) von Thomas Robert Malthus stellte Darwin fest, dass das Bevölkerungswachstum zu einem „Kampf ums Dasein“ führen würde, in dem günstige Variationen vorherrschten, während andere zugrunde gingen.
Darwin entwickelte ab 1838 seine Theorie der „natürlichen Auslese“ und schrieb gerade sein „großes Buch“ zu diesem Thema, als Alfred Russel Wallace ihm 1858 eine Version praktisch derselben Theorie schickte.
Zu diesem Zweck entwickelte Darwin seine vorläufige Theorie der Pangenese.
Um zu erklären, wie neue Varianten entstehen, entwickelte de Vries eine Mutationstheorie, die zu einer vorübergehenden Kluft zwischen denen führte, die die darwinistische Evolution akzeptierten, und Biometrikern, die sich mit de Vries verbündeten.
Die Veröffentlichung der DNA-Struktur durch James Watson und Francis Crick unter Beteiligung von Rosalind Franklin im Jahr 1953 zeigte einen physikalischen Mechanismus der Vererbung.
Im Jahr 1973 schrieb der Evolutionsbiologe Theodosius Dobzhansky, dass „nichts in der Biologie Sinn macht, außer im Licht der Evolution“, weil sie die Zusammenhänge dessen, was zunächst unzusammenhängend schien, in der Naturgeschichte zu einem zusammenhängenden, erklärenden Wissensbestand ans Licht gebracht hat, der und beschreibt sagt viele beobachtbare Fakten über das Leben auf diesem Planeten voraus.
Der gesamte Satz beobachtbarer Merkmale, die die Struktur und das Verhalten eines Organismus ausmachen, wird als Phänotyp bezeichnet.
Beispielsweise entsteht sonnengebräunte Haut durch die Wechselwirkung zwischen dem Genotyp einer Person und dem Sonnenlicht; Daher wird die Sonnenbräune nicht an die Kinder weitergegeben.
DNA ist ein langes Biopolymer, das aus vier Arten von Basen besteht.
Teile eines DNA-Moleküls, die eine einzelne funktionelle Einheit spezifizieren, werden Gene genannt; Verschiedene Gene haben unterschiedliche Basensequenzen.
Wenn die DNA-Sequenz an einem Ort von Individuum zu Individuum variiert, werden die verschiedenen Formen dieser Sequenz Allele genannt.
Während diese einfache Korrespondenz zwischen einem Allel und einem Merkmal in einigen Fällen funktioniert, sind die meisten Merkmale jedoch komplexer und werden durch quantitative Merkmalsorte (mehrere interagierende Gene) gesteuert.
DNA-Methylierung, die Chromatin markiert, sich selbst erhaltende Stoffwechselschleifen, Gen-Stilllegung durch RNA-Interferenz und die dreidimensionale Konformation von Proteinen (z. B. Prionen) sind Bereiche, in denen epigenetische Vererbungssysteme auf der Ebene des Organismus entdeckt wurden.
Beispielsweise wird die ökologische Vererbung durch den Prozess der Nischenkonstruktion durch die regelmäßigen und wiederholten Aktivitäten von Organismen in ihrer Umgebung definiert.
Trotz der ständigen Einführung neuer Variationen durch Mutation und Genfluss ist der größte Teil des Genoms einer Art bei allen Individuen dieser Art identisch.
Ein wesentlicher Teil der phänotypischen Variation in einer Population wird durch genotypische Variation verursacht.
Die Variation verschwindet, wenn ein neues Allel den Punkt der Fixierung erreicht – wenn es entweder aus der Population verschwindet oder das angestammte Allel vollständig ersetzt.
Wenn Mutationen auftreten, können sie das Produkt eines Gens verändern, die Funktion des Gens verhindern oder keine Wirkung haben.
Zusätzliche Kopien von Genen sind eine wichtige Quelle des Rohmaterials, das für die Entwicklung neuer Gene benötigt wird.
Neue Gene können aus einem Vorfahrengen erzeugt werden, wenn eine doppelte Kopie mutiert und eine neue Funktion erhält.
Bei der Generierung neuer Gene können auch kleine Teile mehrerer Gene verdoppelt werden, die sich dann wieder zu neuen Kombinationen mit neuen Funktionen zusammenfügen.
Durch Rekombination und Neusortierung werden die Allelfrequenzen nicht verändert, sondern vielmehr, welche Allele miteinander assoziiert sind, wodurch Nachkommen mit neuen Allelkombinationen entstehen.
Der erste Nachteil besteht darin, dass bei sexuell dimorphen Arten nur eines der beiden Geschlechter Junge gebären kann.
Bei Eukaryoten und vielzelligen Organismen ist die sexuelle Fortpflanzung jedoch die gebräuchlichste Fortpflanzungsmethode.
Der Gentransfer zwischen Arten umfasst die Bildung von Hybridorganismen und den horizontalen Gentransfer.
Es kam zu einem horizontalen Transfer von Genen von Bakterien auf Eukaryoten wie die Hefe Saccharomyces cerevisiae und den Adzukibohnen-Rüsselkäfer Callosobruchus chinensis.
Unterschiedliche Merkmale führen zu unterschiedlichen Überlebens- und Reproduktionsraten (differenzielle Fitness).
Folglich geben Organismen mit Merkmalen, die ihnen einen Vorteil gegenüber ihren Konkurrenten verschaffen, ihre Merkmale eher an die nächste Generation weiter als Organismen mit Merkmalen, die ihnen keinen Vorteil verschaffen.
Das zentrale Konzept der natürlichen Selektion ist die evolutionäre Fitness eines Organismus.
Wenn ein Organismus beispielsweise gut überleben und sich schnell vermehren könnte, seine Nachkommen jedoch zu klein und zu schwach wären, um zu überleben, würde dieser Organismus nur einen geringen genetischen Beitrag für künftige Generationen leisten und daher nur über eine geringe Fitness verfügen.
Beispiele für Merkmale, die die Fitness steigern können, sind eine verbesserte Überlebensrate und eine erhöhte Fruchtbarkeit.
Doch selbst wenn sich die Selektionsrichtung auf diese Weise umkehrt, entwickeln sich Merkmale, die in der Vergangenheit verloren gegangen sind, möglicherweise nicht in identischer Form wieder (siehe Dollos Gesetz).
Die erste ist die Richtungsselektion, bei der es sich um eine Verschiebung des Durchschnittswerts eines Merkmals im Laufe der Zeit handelt – zum Beispiel, wenn Organismen langsam größer werden.
Schließlich findet bei der Stabilisierung der Selektion an beiden Enden eine Selektion gegen extreme Merkmalswerte statt, was zu einer Verringerung der Varianz um den Durchschnittswert und einer geringeren Diversität führt.
Dieses umfassende Verständnis der Natur ermöglicht es Wissenschaftlern, spezifische Kräfte abzugrenzen, die zusammen die natürliche Selektion ausmachen.
Allerdings ist die Rekombinationsrate gering (ungefähr zwei Ereignisse pro Chromosom und Generation).
Eine Reihe von Allelen, die normalerweise in einer Gruppe vererbt werden, wird als Haplotyp bezeichnet.
Diese Drift stoppt, wenn ein Allel schließlich fixiert wird, indem es entweder aus der Population verschwindet oder die anderen Allele vollständig ersetzt.
Die neutrale Theorie der molekularen Evolution ging davon aus, dass die meisten evolutionären Veränderungen das Ergebnis der Fixierung neutraler Mutationen durch genetische Drift sind.
Eine neuere und besser unterstützte Version dieses Modells ist jedoch die nahezu neutrale Theorie, bei der eine Mutation, die in einer kleinen Population effektiv neutral wäre, nicht unbedingt auch in einer großen Population neutral ist.
Die Anzahl der Individuen in einer Population ist nicht entscheidend, sondern ein Maß, das als effektive Populationsgröße bezeichnet wird.
Das Vorhandensein oder Fehlen eines Genflusses verändert den Verlauf der Evolution grundlegend.
Dieses Argument des gegensätzlichen Drucks wurde lange Zeit verwendet, um die Möglichkeit interner Tendenzen in der Evolution auszuschließen, bis das molekulare Zeitalter ein erneutes Interesse an der neutralen Evolution hervorrief.
Beispielsweise werden in Modellen der Codon-Nutzung häufig Mutationsverzerrungen herangezogen.
Unterschiedliche Insertions- und Deletionsverzerrungen in verschiedenen Taxa können zur Entwicklung unterschiedlicher Genomgrößen führen.
Das zeitgenössische Denken über die Rolle von Mutationsverzerrungen spiegelt eine andere Theorie wider als die von Haldane und Fisher.
Organismen können auf die Selektion auch dadurch reagieren, dass sie miteinander kooperieren, in der Regel indem sie ihren Verwandten helfen oder eine für beide Seiten vorteilhafte Symbiose eingehen.
Unter Makroevolution versteht man die Evolution, die auf oder über der Ebene der Arten stattfindet, insbesondere Artbildung und Aussterben; wohingegen sich Mikroevolution auf kleinere evolutionäre Veränderungen innerhalb einer Art oder Population bezieht, insbesondere auf Verschiebungen in der Allelfrequenz und Anpassung.
Bei der Makroevolution können jedoch die Merkmale der gesamten Art wichtig sein.
Ein weit verbreitetes Missverständnis ist, dass die Evolution Ziele, langfristige Pläne oder eine angeborene Tendenz zum „Fortschritt“ hat, wie sie in Überzeugungen wie Orthogenese und Evolutionismus zum Ausdruck kommt; Realistisch gesehen hat die Evolution jedoch kein langfristiges Ziel und führt nicht unbedingt zu größerer Komplexität.
Der Begriff Anpassung kann sich auch auf eine Eigenschaft beziehen, die für das Überleben eines Organismus wichtig ist.
Ein adaptives Merkmal ist ein Aspekt des Entwicklungsmusters des Organismus, der die Überlebens- und Fortpflanzungswahrscheinlichkeit dieses Organismus ermöglicht oder erhöht.
Weitere eindrucksvolle Beispiele sind das Bakterium Escherichia coli, das in einem langfristigen Laborexperiment die Fähigkeit entwickelt hat, Zitronensäure als Nährstoff zu nutzen, Flavobacterium, das ein neuartiges Enzym entwickelt, das es diesen Bakterien ermöglicht, auf den Nebenprodukten der Nylonherstellung zu wachsen, und das Bodenbakterium Sphingobium entwickelt einen völlig neuen Stoffwechselweg, der das synthetische Pestizid Pentachlorphenol abbaut.
Folglich können Strukturen mit ähnlicher interner Organisation in verwandten Organismen unterschiedliche Funktionen haben.
Da jedoch alle lebenden Organismen bis zu einem gewissen Grad miteinander verwandt sind, können selbst Organe, die scheinbar keine oder nur geringe strukturelle Ähnlichkeiten aufweisen, wie etwa die Augen von Arthropoden, Tintenfischen und Wirbeltieren oder die Gliedmaßen und Flügel von Arthropoden und Wirbeltieren, von einem gemeinsamen Satz abhängen homologe Gene, die ihren Aufbau und ihre Funktion steuern; das nennt man tiefe Homologie.
Beispiele hierfür sind Pseudogene, die nicht funktionsfähigen Augenreste blinder Höhlenfische, Flügel flugunfähiger Vögel, das Vorhandensein von Hüftknochen bei Walen und Schlangen sowie sexuelle Merkmale bei Organismen, die sich durch asexuelle Fortpflanzung fortpflanzen.
Ein Beispiel ist die afrikanische Eidechse Holaspis guentheri, die einen extrem flachen Kopf entwickelt hat, um sich in Spalten zu verstecken, wie man an ihren nahen Verwandten sehen kann.
Ein weiteres Beispiel ist die Rekrutierung von Enzymen aus der Glykolyse und dem xenobiotischen Stoffwechsel, die als Strukturproteine, sogenannte Kristalline, in den Augenlinsen von Organismen dienen.
Diese Studien haben gezeigt, dass die Evolution die Entwicklung verändern kann, um neue Strukturen hervorzubringen, wie zum Beispiel embryonale Knochenstrukturen, die sich bei anderen Tieren zum Kiefer entwickeln, anstatt bei Säugetieren einen Teil des Mittelohrs zu bilden.
Diese Veränderungen bei der zweiten Art führen dann wiederum zu neuen Anpassungen bei der ersten Art.
Beispielsweise besteht eine extreme Zusammenarbeit zwischen Pflanzen und den Mykorrhizapilzen, die auf ihren Wurzeln wachsen und der Pflanze dabei helfen, Nährstoffe aus dem Boden aufzunehmen.
Es haben sich auch Koalitionen zwischen Organismen derselben Art entwickelt.
Hier reagieren Körperzellen auf spezifische Signale, die ihnen mitteilen, ob sie wachsen, so bleiben, wie sie sind, oder sterben.
Es gibt mehrere Möglichkeiten, den Begriff „Art“ zu definieren.
Trotz der Vielfalt verschiedener Artenkonzepte können diese verschiedenen Konzepte einem von drei breiten philosophischen Ansätzen zugeordnet werden: Kreuzung, ökologisch und phylogenetisch.
Trotz seiner breiten und langfristigen Verwendung ist die BSC wie andere nicht unumstritten, beispielsweise weil diese Konzepte nicht auf Prokaryoten anwendbar sind, was als Artenproblem bezeichnet wird.
Der Genfluss kann diesen Prozess verlangsamen, indem er die neuen genetischen Varianten auch auf andere Populationen verbreitet.
In diesem Fall können sich eng verwandte Arten regelmäßig kreuzen, es werden jedoch Hybriden gegeneinander selektiert und die Art bleibt unterscheidbar.
Artbildung wurde mehrfach sowohl unter kontrollierten Laborbedingungen (siehe Laborexperimente zur Artbildung) als auch in der Natur beobachtet.
Am häufigsten kommt es bei Tieren zur allopatrischen Artbildung, die in ursprünglich geografisch isolierten Populationen auftritt, etwa durch Lebensraumzerschneidung oder Migration.
Die zweite Artbildungsart ist die peripatrische Artbildung, die auftritt, wenn kleine Populationen von Organismen in einer neuen Umgebung isoliert werden.
Der dritte Modus ist die parapatrische Artbildung.
Im Allgemeinen tritt dies auf, wenn sich die Umwelt im Lebensraum der Elternart drastisch verändert hat.
Die Selektion gegen die Kreuzung mit der metallempfindlichen Elternpopulation führte zu einer allmählichen Änderung der Blütezeit der metallresistenten Pflanzen, was schließlich zu einer vollständigen reproduktiven Isolation führte.
Diese Form ist selten, da bereits ein geringer Genfluss genetische Unterschiede zwischen Teilen einer Population beseitigen kann.
Dies ist bei Tieren nicht üblich, da Tierhybriden normalerweise unfruchtbar sind.
Dadurch können die Chromosomen jeder Elternart während der Meiose passende Paare bilden, da die Chromosomen jedes Elternteils bereits durch ein Paar repräsentiert werden.
Tatsächlich kann die Chromosomenverdoppelung innerhalb einer Art eine häufige Ursache für reproduktive Isolation sein, da die Hälfte der verdoppelten Chromosomen bei der Zucht mit unverdoppelten Organismen nicht übereinstimmen.
Nahezu alle Tier- und Pflanzenarten, die auf der Erde gelebt haben, sind inzwischen ausgestorben, und das Aussterben scheint das endgültige Schicksal aller Arten zu sein.
Trotz des geschätzten Aussterbens von mehr als 99 Prozent aller Arten, die jemals auf der Erde gelebt haben, gibt es derzeit schätzungsweise etwa eine Billion Arten auf der Erde, von denen nur ein Tausendstel Prozent beschrieben ist.
Der früheste unbestrittene Beweis für Leben auf der Erde stammt aus der Zeit vor mindestens 3,5 Milliarden Jahren, während der Eoarchäischen Ära, nachdem sich nach dem früheren geschmolzenen Hadäischen Zeitalter eine geologische Kruste zu verfestigen begann.
Stephen Blair Hedges kommentierte die australischen Erkenntnisse wie folgt: „Wenn Leben relativ schnell auf der Erde entstehen würde, dann könnte es im Universum häufig vorkommen.“
Schätzungen über die Zahl der heutigen Arten auf der Erde reichen von 10 bis 14 Millionen, von denen Schätzungen zufolge bisher etwa 1,9 Millionen benannt und 1,6 Millionen in einer zentralen Datenbank dokumentiert wurden, sodass mindestens 80 Prozent noch nicht beschrieben sind.
Die gemeinsame Abstammung von Organismen wurde zunächst aus vier einfachen Fakten über Organismen abgeleitet: Erstens haben sie geografische Verbreitungen, die nicht durch lokale Anpassung erklärt werden können.
Viertens können Organismen anhand dieser Ähnlichkeiten in eine Hierarchie verschachtelter Gruppen eingeteilt werden, ähnlich einem Stammbaum.
Diese Ansicht geht auf eine von Darwin kurz erwähnte, aber später verworfene Idee zurück.
Durch den Vergleich der Anatomien moderner und ausgestorbener Arten können Paläontologen Rückschlüsse auf die Abstammungslinien dieser Arten ziehen.
In jüngerer Zeit wurden Beweise für eine gemeinsame Abstammung aus der Untersuchung biochemischer Ähnlichkeiten zwischen Organismen gewonnen.
Die eukaryotischen Zellen entstanden vor 1,6 bis 2,7 Milliarden Jahren.
Ein erneuter Befall mit Cyanobakterien-ähnlichen Organismen führte zur Bildung von Chloroplasten in Algen und Pflanzen.
Im Januar 2016 berichteten Wissenschaftler, dass vor etwa 800 Millionen Jahren eine geringfügige genetische Veränderung in einem einzelnen Molekül namens GK-PID es Organismen ermöglicht haben könnte, von einem einzelligen Organismus zu einem von vielen Zellen zu gelangen.
Es wurden verschiedene Auslöser für die kambrische Explosion vorgeschlagen, darunter die Ansammlung von Sauerstoff in der Atmosphäre durch Photosynthese.
Künstliche Selektion ist die absichtliche Auswahl von Merkmalen in einer Population von Organismen.
Proteine mit wertvollen Eigenschaften haben sich durch wiederholte Mutations- und Selektionsrunden (z. B. modifizierte Enzyme und neue Antikörper) in einem Prozess entwickelt, der als gerichtete Evolution bezeichnet wird.
Die Kreuzung verschiedener Populationen dieses blinden Fisches brachte einige Nachkommen mit funktionierenden Augen hervor, da in den isolierten Populationen, die sich in verschiedenen Höhlen entwickelt hatten, unterschiedliche Mutationen aufgetreten waren.
Viele menschliche Krankheiten sind keine statischen Phänomene, sondern können sich weiterentwickeln.
Es ist möglich, dass wir vor dem Ende der Wirksamkeit der meisten verfügbaren Antibiotika stehen und die Vorhersage der Evolution und Evolvierbarkeit unserer Krankheitserreger sowie die Entwicklung von Strategien zu deren Verlangsamung oder Umgehung ein tieferes Wissen über die komplexen Kräfte erfordern, die die Evolution auf molekularer Ebene vorantreiben.
Er nutzte Evolutionsstrategien, um komplexe technische Probleme zu lösen.
In einigen Ländern, insbesondere in den Vereinigten Staaten, haben diese Spannungen zwischen Wissenschaft und Religion die aktuelle Kontroverse zwischen Schöpfung und Evolution angeheizt, einem religiösen Konflikt, der sich auf Politik und öffentliche Bildung konzentriert.
Die Scopes-Trial-Entscheidung von 1925 führte dazu, dass das Thema eine Generation lang in amerikanischen sekundären Biologielehrbüchern sehr selten wurde, später jedoch schrittweise wieder eingeführt wurde und mit der Epperson v. Arkansas-Entscheidung von 1968 gesetzlich geschützt wurde.
Unter natürlicher Selektion versteht man das unterschiedliche Überleben und die Fortpflanzung von Individuen aufgrund von Unterschieden im Phänotyp.
Variationen gibt es in allen Populationen von Organismen.
Die Umgebung eines Genoms umfasst die Molekularbiologie in der Zelle, anderen Zellen, anderen Individuen, Populationen, Arten sowie der abiotischen Umgebung.
Natürliche Selektion ist ein Grundpfeiler der modernen Biologie.
Das Konzept der natürlichen Selektion wurde ursprünglich in Ermangelung einer gültigen Vererbungstheorie entwickelt; Zur Zeit von Darwins Schriften hatte die Wissenschaft noch keine modernen Theorien zur Genetik entwickelt.
Die klassischen Argumente wurden im 18. Jahrhundert von Pierre Louis Maupertuis und anderen, darunter Darwins Großvater Erasmus Darwin, wieder eingeführt.
Der Erfolg dieser Theorie schärfte das Bewusstsein für die gewaltige Skala der geologischen Zeit und machte die Idee plausibel, dass winzige, praktisch unmerkliche Veränderungen in aufeinanderfolgenden Generationen Konsequenzen auf der Skala der Unterschiede zwischen den Arten haben könnten.
Er war gerade dabei, sein „großes Buch“ zu schreiben, um seine Forschungsergebnisse vorzustellen, als der Naturforscher Alfred Russel Wallace das Prinzip unabhängig erfand und es in einem Aufsatz beschrieb, den er an Darwin schickte, um ihn an Charles Lyell weiterzuleiten.
In der 3. Auflage von 1861 gab Darwin zu, dass andere – wie William Charles Wells im Jahr 1813 und Patrick Matthew im Jahr 1831 – ähnliche Ideen vorgeschlagen, diese jedoch weder weiterentwickelt noch in nennenswerten wissenschaftlichen Publikationen vorgestellt hatten.
In einem Brief an Charles Lyell im September 1860 bedauerte Darwin die Verwendung des Begriffs „natürliche Auslese“ und bevorzugte den Begriff „natürliche Erhaltung“.
Allerdings blieb die natürliche Selektion als Mechanismus umstritten, teils, weil sie als zu schwach angesehen wurde, um die Bandbreite der beobachteten Merkmale lebender Organismen zu erklären, und teils, weil selbst Befürworter der Evolution sich gegen ihre „ungesteuerte“ und nicht fortschreitende Natur sträubten, a Reaktion, die als das größte Hindernis für die Akzeptanz der Idee bezeichnet wurde.
Mit der Integration der Evolution in die Mendelschen Vererbungsgesetze zu Beginn des 20. Jahrhunderts, der sogenannten modernen Synthese, akzeptierten Wissenschaftler im Allgemeinen die natürliche Selektion.
J. B. S. Haldane führte das Konzept der „Kosten“ der natürlichen Selektion ein.
Allerdings ist die natürliche Selektion „blind“ in dem Sinne, dass Veränderungen im Phänotyp einen Fortpflanzungsvorteil bringen können, unabhängig davon, ob das Merkmal vererbbar ist oder nicht.
Wenn die Merkmale, die diesen Individuen einen Fortpflanzungsvorteil verschaffen, auch vererbbar sind, also vom Elternteil an die Nachkommen weitergegeben werden, kommt es zu einer differentiellen Fortpflanzung, also zu einem etwas höheren Anteil an schnellen Kaninchen oder effizienten Algen in der nächsten Generation.
Dies erweckt den Anschein eines Zwecks, aber bei der natürlichen Selektion gibt es keine absichtliche Wahl.
Dies gab dunkel gefärbten Motten eine bessere Überlebenschance und brachte dunkel gefärbte Nachkommen hervor, und nur fünfzig Jahre nach dem Fang der ersten dunklen Motte waren fast alle Motten im industriellen Manchester dunkel.
Wenn ein Organismus halb so lange lebt wie andere seiner Art, aber doppelt so viele Nachkommen das Erwachsenenalter erreichen, werden seine Gene in der erwachsenen Population der nächsten Generation häufiger.
Dabei ist zwischen dem Konzept „survival of the fittest“ und der „Verbesserung der Fitness“ zu unterscheiden. "
Haldane nannte diesen Vorgang „Substitution“ oder in der Biologie häufiger als „Fixierung“.
Die Wahrscheinlichkeit, dass bei einem Mitglied einer Population eine vorteilhafte Mutation auftritt, hängt von der Gesamtzahl der Replikationen dieser Variante ab.
In diesem Experiment hängt die „Verbesserung der Fitness“ von der Anzahl der Replikationen der jeweiligen Variante ab, damit eine neue Variante auftaucht, die in der Lage ist, im nächsthöheren Wirkstoffkonzentrationsbereich zu wachsen.
Richard Lenskis klassisches E. coli-Langzeitevolutionsexperiment ist ein Beispiel für die Anpassung in einem Wettbewerbsumfeld („Verbesserung der Fitness“ während des „Survival of the Fittest“).
Die ungewöhnliche störende Selektion wirkt sich auch in Übergangsperioden aus, wenn der aktuelle Modus nicht optimal ist, verändert das Merkmal jedoch in mehr als eine Richtung.
Einige Biologen kennen nur zwei Arten: die Lebensfähigkeits- (oder Überlebens-)Selektion, die dazu dient, die Überlebenswahrscheinlichkeit eines Organismus zu erhöhen, und die Fruchtbarkeits- (oder Fruchtbarkeits- oder Fortpflanzungs-)Selektion, die dazu dient, die Reproduktionsrate bei gegebenem Überleben zu erhöhen.
Bei der Verwandtschaftsselektion und dem intragenomischen Konflikt liefert die Selektion auf Genebene eine treffendere Erklärung des zugrunde liegenden Prozesses.
Ökologische Selektion ist natürliche Selektion durch andere Mittel als sexuelle Selektion, wie z. B. Verwandtenselektion, Konkurrenz und Kindsmord.
Bei manchen Arten liegt die Partnerwahl jedoch in erster Linie bei den Männchen, wie etwa bei einigen Fischen der Familie Syngnathidae.
Seit der Entdeckung des Penicillins im Jahr 1928 werden Antibiotika zur Bekämpfung bakterieller Erkrankungen eingesetzt.
Genetische Variation ist das Ergebnis von Mutationen, genetischen Rekombinationen und Veränderungen im Karyotyp (der Anzahl, Form, Größe und inneren Anordnung der Chromosomen).
Viele Mutationen in der nichtkodierenden DNA haben jedoch schädliche Auswirkungen.
Veränderungen dieser Gene haben oft große Auswirkungen auf den Phänotyp des Individuums, da sie die Funktion vieler anderer Gene regulieren.
Wenn solche Mutationen zu einer höheren Fitness führen, begünstigt die natürliche Selektion diese Phänotypen und das neue Merkmal verbreitet sich in der Population.
Es ist jedoch ein wesentlicher Bestandteil des Konzepts einer Art, gegen die Hybriden selektiert werden, was der Entwicklung der reproduktiven Isolation entgegensteht, einem Problem, das von Darwin erkannt wurde.
Der Phänotyp wird durch die genetische Ausstattung (Genotyp) eines Organismus und die Umgebung, in der der Organismus lebt, bestimmt.
Ein Beispiel sind die ABO-Blutgruppenantigene beim Menschen, bei denen drei Allele den Phänotyp bestimmen.
Dieser Prozess kann fortgesetzt werden, bis das Allel fixiert ist und die gesamte Population den fitteren Phänotyp teilt.
Durch die stabilisierende Selektion werden funktionelle genetische Merkmale wie proteinkodierende Gene oder regulatorische Sequenzen im Laufe der Zeit durch Selektionsdruck gegen schädliche Varianten konserviert.
Einige Formen der ausgleichenden Selektion führen nicht zu einer Fixierung, sondern behalten ein Allel in mittleren Häufigkeiten in einer Population bei.
Die Aufrechterhaltung der Allelvariation kann auch durch störende oder diversifizierende Selektion erfolgen, die Genotypen bevorzugt, die in beide Richtungen vom Durchschnitt abweichen (d. h. das Gegenteil von Überdominanz), und zu einer bimodalen Verteilung der Merkmalswerte führen kann.
Nach einer Zeit ohne neue Mutationen wird die genetische Variation an diesen Stellen jedoch aufgrund der genetischen Drift eliminiert.
Das genaue Ergebnis der beiden Prozesse hängt sowohl von der Geschwindigkeit ab, mit der neue Mutationen auftreten, als auch von der Stärke der natürlichen Selektion, die davon abhängt, wie ungünstig sich die Mutation erweist.
Die Wahrscheinlichkeit, dass eine solche Umgruppierung zwischen zwei Allelen auftritt, hängt umgekehrt vom Abstand zwischen ihnen ab.
Ein starker selektiver Sweep führt zu einer Region des Genoms, in der der positiv ausgewählte Haplotyp (das Allel und seine Nachbarn) im Wesentlichen die einzigen sind, die in der Population existieren.
Die Hintergrundauswahl ist das Gegenteil eines selektiven Sweeps.
Mit den Worten des Philosophen Daniel Dennett ist „Darwins gefährliche Idee“ der Evolution durch natürliche Auslese eine „universelle Säure“, die nicht auf ein Gefäß oder einen Behälter beschränkt bleiben kann, da sie bald ausläuft und sich immer weiter ausdehnt Umfeld.
Diese Bedingungen sind: Erblichkeit, Variation des Typs und Konkurrenz um begrenzte Ressourcen.
Herbert Spencer und die Eugeniker befürworten Francis Galtons Interpretation der natürlichen Auslese als notwendigerweise progressiv, die zu angeblichen Fortschritten in Intelligenz und Zivilisation führt, und wurde zu einer Rechtfertigung für Kolonialismus, Eugenik und Sozialdarwinismus.
Der Rassengedanke als Grundlage unseres Staates hat in dieser Hinsicht bereits viel erreicht.“
Das bekannteste Beispiel der Evolutionspsychologie, die insbesondere in den frühen Arbeiten von Noam Chomsky und später von Steven Pinker vorangetrieben wurde, ist die Hypothese, dass sich das menschliche Gehirn an die grammatikalischen Regeln der natürlichen Sprache angepasst hat.
Er beobachtete, dass Organismen (Erbsenpflanzen) Merkmale über diskrete „Vererbungseinheiten“ erben.
Struktur und Funktion, Variation und Verteilung von Genen werden im Kontext der Zelle, des Organismus (z. B. Dominanz) und im Kontext einer Population untersucht.
Genetische Prozesse wirken in Kombination mit der Umgebung und den Erfahrungen eines Organismus, um Entwicklung und Verhalten zu beeinflussen, was oft als Natur versus Pflege bezeichnet wird.
Die moderne Wissenschaft der Genetik, die diesen Prozess verstehen wollte, begann mit der Arbeit des Augustinermönchs Gregor Mendel Mitte des 19. Jahrhunderts.
Sein zweites Gesetz ist dasselbe wie das, was Mendel veröffentlichte.
Eine populäre Theorie im 19. Jahrhundert, die in Charles Darwins Werk „Über die Entstehung der Arten“ von 1859 zum Ausdruck kam, war die Vermischungsvererbung: die Idee, dass Individuen eine gleichmäßige Mischung von Merkmalen von ihren Eltern erben.
In seiner Arbeit „Versuche über Pflanzenhybriden“, die er 1865 dem Naturforschenden Verein in Brünn vorlegte, zeichnete Mendel die Vererbungsmuster bestimmter Merkmale bei Erbsenpflanzen nach und beschrieb sie mathematisch.
William Bateson, ein Befürworter von Mendels Werk, prägte 1905 das Wort „Genetik“ (das Adjektiv „genetisch“, abgeleitet vom griechischen Wort genesis – γένεσις, „Ursprung“, geht dem Substantiv voraus und wurde erstmals 1860 im biologischen Sinne verwendet).
Im Laufe der nächsten 11 Jahre entdeckte sie, dass Frauen nur das X-Chromosom und Männer sowohl X- als auch Y-Chromosomen hatten.
James Watson und Francis Crick bestimmten 1953 die Struktur der DNA anhand der Röntgenkristallographie-Arbeiten von Rosalind Franklin und Maurice Wilkins, die darauf hindeuteten, dass DNA eine helikale Struktur hat (d. h. die Form eines Korkenziehers).
Die Struktur legte auch eine einfache Methode zur Replikation nahe: Wenn die Stränge getrennt werden, können für jeden neue Partnerstränge basierend auf der Sequenz des alten Strangs rekonstruiert werden.
In den folgenden Jahren versuchten Wissenschaftler zu verstehen, wie DNA den Prozess der Proteinproduktion steuert.
Mit dem neu entdeckten molekularen Verständnis der Vererbung kam es zu einer Explosion der Forschung.
Eine wichtige Entwicklung war die Kettenabbruch-DNA-Sequenzierung im Jahr 1977 durch Frederick Sanger.
Bei seinen Experimenten, die das Merkmal der Blütenfarbe untersuchten, stellte Mendel fest, dass die Blüten jeder Erbsenpflanze entweder lila oder weiß waren – aber niemals eine Zwischenfarbe zwischen den beiden Farben.
Viele Arten, darunter auch der Mensch, weisen dieses Vererbungsmuster auf.
Wenn Organismen bei einem Gen heterozygot sind, wird oft ein Allel als dominant bezeichnet, da seine Eigenschaften den Phänotyp des Organismus dominieren, während das andere Allel als rezessiv bezeichnet wird, da seine Eigenschaften zurückgehen und nicht beobachtet werden.
Oft wird ein „+“-Symbol verwendet, um das übliche, nicht mutierte Allel für ein Gen zu kennzeichnen.
Eines der gebräuchlichsten Diagramme zur Vorhersage des Ergebnisses einer Kreuzung ist das Punnett-Quadrat.
Einige Gene sortieren sich nicht unabhängig voneinander, was auf eine genetische Verknüpfung hinweist, ein Thema, das später in diesem Artikel behandelt wird.)
Ein anderes Gen steuert jedoch, ob die Blüten überhaupt Farbe haben oder weiß sind.
Viele Merkmale sind keine eigenständigen Merkmale (z. B. violette oder weiße Blüten), sondern kontinuierliche Merkmale (z. B. menschliche Größe und Hautfarbe).
Der Grad, in dem die Gene eines Organismus zu einem komplexen Merkmal beitragen, wird als Erblichkeit bezeichnet.
DNA besteht aus einer Kette von Nukleotiden, von denen es vier Arten gibt: Adenin (A), Cytosin (C), Guanin (G) und Thymin (T).
Viren können sich ohne Wirt nicht vermehren und werden von vielen genetischen Prozessen nicht beeinflusst, sodass sie in der Regel nicht als lebende Organismen betrachtet werden.
Diese Struktur der DNA ist die physikalische Grundlage für die Vererbung: Die DNA-Replikation dupliziert die genetische Information, indem sie die Stränge spaltet und jeden Strang als Vorlage für die Synthese eines neuen Partnerstrangs verwendet.
Diese DNA-Stränge sind oft extrem lang; Das größte menschliche Chromosom beispielsweise ist etwa 247 Millionen Basenpaare lang.
DNA kommt am häufigsten im Zellkern vor, aber Ruth Sager half bei der Entdeckung nichtchromosomaler Gene außerhalb des Zellkerns.
Während haploide Organismen nur eine Kopie jedes Chromosoms haben, sind die meisten Tiere und viele Pflanzen diploid und enthalten zwei Kopien jedes Chromosoms und somit zwei Kopien jedes Gens.
Beim Menschen und vielen anderen Tieren enthält das Y-Chromosom das Gen, das die Entwicklung der spezifisch männlichen Merkmale auslöst.
Dieser Mitose genannte Vorgang ist die einfachste Form der Fortpflanzung und die Grundlage für die ungeschlechtliche Fortpflanzung.
Eukaryontische Organismen nutzen häufig die sexuelle Fortpflanzung, um Nachkommen zu erzeugen, die eine Mischung aus genetischem Material enthalten, das von zwei verschiedenen Eltern geerbt wurde.
Einige Bakterien können eine Konjugation eingehen und dabei ein kleines kreisförmiges DNA-Stück auf ein anderes Bakterium übertragen.
Auf diese Weise können bei den Nachkommen eines Paarungspaares neue Genkombinationen entstehen.
Beim Crossover tauschen Chromosomen DNA-Abschnitte aus, wodurch die Gen-Allele effektiv zwischen den Chromosomen gemischt werden.
Der erste zytologische Nachweis des Crossing-Over wurde 1931 von Harriet Creighton und Barbara McClintock durchgeführt.
Bei einer beliebig langen Distanz ist die Wahrscheinlichkeit einer Überkreuzung so hoch, dass die Vererbung der Gene praktisch unkorreliert ist.
Die spezifische Reihenfolge der Aminosäuren führt zu einer einzigartigen dreidimensionalen Struktur für dieses Protein, und die dreidimensionalen Strukturen von Proteinen hängen mit ihren Funktionen zusammen.
Die Proteinstruktur ist dynamisch; Das Protein Hämoglobin biegt sich in leicht unterschiedliche Formen, da es die Aufnahme, den Transport und die Freisetzung von Sauerstoffmolekülen im Blut von Säugetieren erleichtert.
Sichelzellenanämie ist beispielsweise eine genetische Erkrankung des Menschen, die aus einem einzelnen Basenunterschied innerhalb der kodierenden Region für den β-Globin-Abschnitt des Hämoglobins resultiert und eine einzelne Aminosäureveränderung verursacht, die die physikalischen Eigenschaften des Hämoglobins verändert.
Einige DNA-Sequenzen werden in RNA transkribiert, aber nicht in Proteinprodukte übersetzt – solche RNA-Moleküle werden als nichtkodierende RNA bezeichnet.
Ein interessantes Beispiel ist die Fellfärbung der Siamkatze.
Aber diese Proteine, die dunkles Haar produzieren, sind temperaturempfindlich (d. h. sie weisen eine Mutation auf, die Temperaturempfindlichkeit verursacht) und denaturieren in Umgebungen mit höheren Temperaturen, sodass sie in Bereichen, in denen die Katze eine höhere Körpertemperatur hat, kein Pigment für dunkles Haar produzieren können.
Nach dem Untergang des Weströmischen Reiches verschlechterte sich das Wissen über griechische Weltvorstellungen in Westeuropa in den ersten Jahrhunderten (400 bis 1000 n. Chr.) des Mittelalters, blieb jedoch in der muslimischen Welt während des islamischen Goldenen Zeitalters erhalten.
Die moderne Wissenschaft ist typischerweise in drei Hauptzweige unterteilt: die Naturwissenschaften (z. B. Biologie, Chemie und Physik), die sich mit der Natur im weitesten Sinne befassen; die Sozialwissenschaften (z. B. Wirtschaftswissenschaften, Psychologie und Soziologie), die sich mit Individuen und Gesellschaften befassen; und die formalen Wissenschaften (z. B. Logik, Mathematik und theoretische Informatik), die sich mit Symbolen befassen, die durch Regeln gesteuert werden.
Neue wissenschaftliche Erkenntnisse werden durch die Forschung von Wissenschaftlern vorangetrieben, die von der Neugier auf die Welt und dem Wunsch, Probleme zu lösen, motiviert sind.
Insbesondere ging es um die Art von Wissen, das Menschen miteinander kommunizieren und teilen können.
Es wurde jedoch keine konsequente bewusste Unterscheidung zwischen dem Wissen über solche Dinge, die in jeder Gemeinschaft wahr sind, und anderen Arten gemeinschaftlichen Wissens, wie etwa Mythologien und Rechtssystemen, getroffen.
Sie entwickelten sogar einen offiziellen Kalender, der zwölf Monate, jeweils dreißig Tage und fünf Tage am Jahresende enthielt.
Aus diesem Grund wird behauptet, dass diese Männer die ersten Philosophen im engeren Sinne waren und auch die ersten Menschen, die klar zwischen „Natur“ und „Konvention“ unterschieden.
Im Gegensatz dazu wurde der Versuch, Naturwissen zur Nachahmung der Natur zu nutzen (Kunstgriff oder Technologie, griech. technē), von klassischen Wissenschaftlern als ein angemesseneres Interesse für Handwerker niedrigerer sozialer Schichten angesehen.
Die Atomtheorie wurde vom griechischen Philosophen Leukipp und seinem Schüler Demokrit entwickelt.
Die sokratische Methode, wie sie in Platons Dialogen dokumentiert ist, ist eine dialektische Methode zur Eliminierung von Hypothesen: Bessere Hypothesen werden gefunden, indem kontinuierlich diejenigen identifiziert und beseitigt werden, die zu Widersprüchen führen.
Sokrates kritisierte die ältere Art des Physikstudiums als zu rein spekulativ und ohne Selbstkritik.
Aristoteles schuf später ein systematisches Programm der teleologischen Philosophie: Bewegung und Veränderung werden als die Verwirklichung von Potenzialen beschrieben, die bereits in Dingen vorhanden sind, je nachdem, um welche Art von Dingen es sich handelt.
Die Sokratiker bestanden auch darauf, dass die Philosophie dazu genutzt werden sollte, die praktische Frage nach der besten Lebensweise eines Menschen zu prüfen (eine Studie, die Aristoteles in Ethik und politische Philosophie unterteilte).
Das Modell des Aristarchos wurde weitgehend abgelehnt, weil man glaubte, es verstoße gegen die Gesetze der Physik.
Johannes Philoponus, ein byzantinischer Gelehrter im 5. Jahrhundert, stellte die Physiklehre des Aristoteles in Frage und stellte deren Mängel fest.
Die vier Ursachen des Aristoteles schrieben vor, dass die Frage „Warum“ auf vier Arten beantwortet werden sollte, um Dinge wissenschaftlich zu erklären.
Die Originaltexte von Aristoteles gingen jedoch schließlich in Westeuropa verloren, und nur ein Text von Platon war weithin bekannt, der Timaios, der einzige platonische Dialog und eines der wenigen Originalwerke der klassischen Naturphilosophie, die den lateinischen Lesern in Europa zugänglich waren frühes Mittelalter.
Viele syrische Übersetzungen wurden von Gruppen wie den Nestorianern und Monophysiten angefertigt.
P. 465: „Erst wenn der Einfluss von ibn al-Haytam und anderen auf den Mainstream der späteren mittelalterlichen physikalischen Schriften ernsthaft untersucht wurde, kann Schramms Behauptung, dass ibn al-Haytam der wahre Begründer der modernen Physik war, bewertet werden.“
Avicennas Kanon gilt als eine der wichtigsten Veröffentlichungen der Medizin und beide trugen maßgeblich zur Praxis der experimentellen Medizin bei, indem sie ihre Behauptungen durch klinische Studien und Experimente untermauerten.
Darüber hinaus begann man, klassische griechische Texte aus dem Arabischen und Griechischen ins Lateinische zu übersetzen, was zu einem höheren Niveau der wissenschaftlichen Diskussion in Westeuropa führte.
Manuskriptkopien von Alhazens Buch der Optik verbreiteten sich ebenfalls vor 1240 in ganz Europa, wie die Einbindung in Vitellos Perspectiva beweist.
Der Zustrom antiker Texte löste die Renaissance des 12. Jahrhunderts und das Aufblühen einer als Scholastik bekannten Synthese von Katholizismus und Aristotelismus in Westeuropa aus, das zu einem neuen geografischen Zentrum der Wissenschaft wurde.
Ein Visionsmodell, das später als Perspektivismus bekannt wurde, wurde von den Künstlern der Renaissance genutzt und studiert.
Dies basierte auf einem Theorem, dass die Umlaufperioden der Planeten länger sind, je weiter ihre Umlaufbahnen vom Bewegungszentrum entfernt sind, was seiner Ansicht nach nicht mit dem Modell des Ptolemäus übereinstimmte.
Er fand heraus, dass das gesamte Licht von einem einzelnen Punkt der Szene an einem einzigen Punkt auf der Rückseite der Glaskugel abgebildet wurde.
Kepler lehnte die aristotelische Metaphysik nicht ab und beschrieb sein Werk als Suche nach der Harmonie der Sphären.
Galilei hatte Argumente des Papstes verwendet und sie in dem Werk „Dialog über die beiden Hauptweltsysteme“ in die Stimme des Einfaltspinsels gebracht, was Urban VIII. sehr beleidigte.
Descartes betonte das individuelle Denken und argumentierte, dass die Mathematik statt der Geometrie zum Studium der Natur herangezogen werden sollte.
Diese neue Wissenschaft begann sich als Beschreibung von „Naturgesetzen“ zu verstehen.
In Anlehnung an Francis Bacon ging Leibniz davon aus, dass verschiedene Arten von Dingen alle nach denselben allgemeinen Naturgesetzen funktionieren, ohne dass es für jede Art von Dingen besondere formale oder endgültige Ursachen gibt.
In Bacons Worten: „Das wahre und legitime Ziel der Wissenschaften ist die Ausstattung des menschlichen Lebens mit neuen Erfindungen und Reichtümern“, und er hielt Wissenschaftler davon ab, immaterielle philosophische oder spirituelle Ideen zu verfolgen, von denen er glaubte, dass sie über „den Dunst von“ hinaus wenig zum menschlichen Glück beitragen subtile, erhabene oder erfreuliche Spekulation“.
Eine weitere wichtige Entwicklung war die Popularisierung der Wissenschaft in einer zunehmend gebildeten Bevölkerung.
Die Philosophen der Aufklärung wählten eine kurze Geschichte wissenschaftlicher Vorgänger – hauptsächlich Galileo, Boyle und Newton – als Leitfaden und Garanten für ihre Anwendung des einzigartigen Konzepts der Natur und des Naturgesetzes auf alle physikalischen und sozialen Bereiche der Zeit.
Hume und andere schottische Aufklärungsdenker entwickelten eine „Wissenschaft vom Menschen“, die historisch in Werken von Autoren wie James Burnett, Adam Ferguson, John Millar und William Robertson zum Ausdruck kam, die alle eine wissenschaftliche Studie über das Verhalten der Menschen in der Antike und in der Antike zusammenführten Kulturen mit einem starken Bewusstsein für die bestimmenden Kräfte der Moderne.
Sowohl John Herschel als auch William Whewell systematisierten die Methodik: Letzterer prägte den Begriff Wissenschaftler.
Unabhängig davon stellte Gregor Mendel 1865 seine Arbeit „Versuche über Pflanzenhybriden“ vor, in der er die Prinzipien der biologischen Vererbung darlegte und als Grundlage für die moderne Genetik diente.
Die Phänomene, die die Dekonstruktion des Atoms ermöglichen würden, wurden im letzten Jahrzehnt des 19. Jahrhunderts entdeckt: Die Entdeckung der Röntgenstrahlen inspirierte die Entdeckung der Radioaktivität.
Darüber hinaus führte der umfassende Einsatz technologischer Innovationen, die durch die Kriege dieses Jahrhunderts angeregt wurden, zu Revolutionen im Transportwesen (Automobile und Flugzeuge), zur Entwicklung von Interkontinentalraketen, einem Wettlauf ins All und einem nuklearen Wettrüsten.
Die Entdeckung der kosmischen Mikrowellen-Hintergrundstrahlung im Jahr 1964 führte zu einer Ablehnung der Steady-State-Theorie des Universums zugunsten der Urknalltheorie von Georges Lemaître.
Der weit verbreitete Einsatz integrierter Schaltkreise im letzten Viertel des 20. Jahrhunderts in Verbindung mit Kommunikationssatelliten führte zu einer Revolution in der Informationstechnologie und zum Aufstieg des globalen Internets und der mobilen Datenverarbeitung, einschließlich Smartphones.
Sowohl die Natur- als auch die Sozialwissenschaften sind empirische Wissenschaften, da ihr Wissen auf empirischen Beobachtungen basiert und von anderen Forschern, die unter den gleichen Bedingungen arbeiten, auf ihre Gültigkeit überprüft werden kann.
Die Naturwissenschaften können beispielsweise in Physik, Chemie, Astronomie und Geowissenschaften unterteilt werden.
Dennoch bleiben philosophische Perspektiven, Vermutungen und Voraussetzungen, die oft übersehen werden, in der Naturwissenschaft notwendig.
Es umfasst Mathematik, Systemtheorie und theoretische Informatik.
Die formalen Wissenschaften sind daher apriorische Disziplinen und aus diesem Grund besteht Uneinigkeit darüber, ob sie tatsächlich eine Wissenschaft darstellen.
Das Ingenieurwesen selbst umfasst eine Reihe spezialisierterer Bereiche des Ingenieurwesens, die jeweils einen spezifischeren Schwerpunkt auf bestimmte Bereiche der angewandten Mathematik, Naturwissenschaften und Anwendungsarten legen.
Er antwortete: „Sir, was nützt ein neugeborenes Kind?“
Diese neue Erklärung wird verwendet, um falsifizierbare Vorhersagen zu treffen, die durch Experimente oder Beobachtungen überprüfbar sind.
Dies geschieht zum Teil durch die Beobachtung natürlicher Phänomene, aber auch durch Experimente, bei denen versucht wird, natürliche Ereignisse unter kontrollierten Bedingungen zu simulieren, die der Disziplin angemessen sind (in den Beobachtungswissenschaften wie der Astronomie oder der Geologie könnte eine vorhergesagte Beobachtung an die Stelle einer kontrollierten treten). Experiment).
Wenn die Hypothese die Prüfung überstanden hat, kann sie in den Rahmen einer wissenschaftlichen Theorie übernommen werden, eines logisch begründeten, in sich konsistenten Modells oder Rahmens zur Beschreibung des Verhaltens bestimmter Naturphänomene.
In diesem Sinne werden Theorien nach größtenteils denselben wissenschaftlichen Prinzipien formuliert wie Hypothesen.
Dies kann durch sorgfältige Versuchsplanung, Transparenz und einen gründlichen Peer-Review-Prozess der Versuchsergebnisse sowie etwaiger Schlussfolgerungen erreicht werden.
Statistik, ein Teilgebiet der Mathematik, dient der Zusammenfassung und Analyse von Daten, die es Wissenschaftlern ermöglichen, die Zuverlässigkeit und Variabilität ihrer experimentellen Ergebnisse zu beurteilen.
Im Gegensatz dazu steht der Antirealismus, die Ansicht, dass der Erfolg der Wissenschaft nicht davon abhängt, dass sie bei nicht beobachtbaren Einheiten wie Elektronen genau ist.
In der Wissenschaftsphilosophie gibt es verschiedene Denkrichtungen.
Dies ist notwendig, da die Anzahl der Vorhersagen, die diese Theorien machen, unendlich ist, was bedeutet, dass sie nicht allein mit deduktiver Logik aus der begrenzten Menge an Beweisen ermittelt werden können.
Der kritische Rationalismus ist ein kontrastierender Wissenschaftsansatz des 20. Jahrhunderts, der erstmals vom österreichisch-britischen Philosophen Karl Popper definiert wurde.
Popper schlug vor, die Überprüfbarkeit durch die Falsifizierbarkeit als Wahrzeichen wissenschaftlicher Theorien zu ersetzen und die Induktion durch die Falsifikation als empirische Methode zu ersetzen.
Ein anderer Ansatz, der Instrumentalismus, betont die Nützlichkeit von Theorien als Instrumente zur Erklärung und Vorhersage von Phänomenen.
Dem Instrumentalismus nahe steht der konstruktive Empirismus, nach dem das Hauptkriterium für den Erfolg einer wissenschaftlichen Theorie darin besteht, ob das, was sie über beobachtbare Einheiten sagt, wahr ist.
Jedes Paradigma hat seine eigenen spezifischen Fragen, Ziele und Interpretationen.
Das heißt, die Wahl eines neuen Paradigmas basiert auf Beobachtungen, auch wenn diese Beobachtungen vor dem Hintergrund des alten Paradigmas gemacht werden.
Der Hauptpunkt besteht darin, dass zwischen natürlichen und übernatürlichen Erklärungen unterschieden werden sollte und dass die Wissenschaft methodisch auf natürliche Erklärungen beschränkt werden sollte.
Das heißt, keine Theorie gilt jemals als absolut sicher, da die Wissenschaft das Konzept des Fallibilismus akzeptiert.
Neue wissenschaftliche Erkenntnisse führen selten zu großen Veränderungen in unserem Verständnis.
Wissenschaftliches Wissen wird durch eine schrittweise Synthese von Informationen aus verschiedenen Experimenten verschiedener Forscher aus verschiedenen Wissenschaftszweigen gewonnen; es gleicht eher einem Aufstieg als einem Sprung.
Der Philosoph Barry Stroud fügt hinzu, dass die beste Definition für „Wissen“ zwar umstritten ist, Skepsis und die Möglichkeit, dass man falsch liegt, jedoch mit Recht vereinbar sind.
Dies ist insbesondere in den eher makroskopischen Bereichen der Wissenschaft der Fall (z. B. Psychologie, physikalische Kosmologie).
Seitdem ist die Gesamtzahl der aktiven Zeitschriften stetig gestiegen.
Obwohl es die Zeitschriften in 39 Sprachen gibt, werden 91 Prozent der indexierten Artikel auf Englisch veröffentlicht.
Wissenschaftsmagazine wie „New Scientist“, „Science & Vie“ und „Scientific American“ richten sich an die Bedürfnisse einer viel breiteren Leserschaft und bieten eine nicht-technische Zusammenfassung beliebter Forschungsbereiche, einschließlich bemerkenswerter Entdeckungen und Fortschritte in bestimmten Forschungsbereichen.
In diese Kategorien können verschiedene Arten kommerzieller Werbung fallen, die von Hype bis hin zu Betrug reichen.
Viele Wissenschaftler verfolgen Karrieren in verschiedenen Wirtschaftszweigen wie der Wissenschaft, der Industrie, der Regierung und gemeinnützigen Organisationen.
Beispielsweise konnte Christine Ladd (1847–1930) einen Doktortitel beantragen. Programm als „C. Ladd“; Christine „Kitty“ Ladd erfüllte die Anforderungen im Jahr 1882, erhielt ihren Abschluss jedoch erst 1926, nach einer Karriere, die die Algebra der Logik (siehe Wahrheitstabelle), das Farbsehen und die Psychologie umfasste.
Im späten 20. Jahrhundert stieg die Zahl der Wissenschaftlerinnen durch die aktive Rekrutierung von Frauen und die Beseitigung der institutionellen Diskriminierung aufgrund des Geschlechts erheblich an, in einigen Bereichen bestehen jedoch nach wie vor große geschlechtsspezifische Unterschiede. Zu Beginn des 21. Jahrhunderts waren über die Hälfte der neuen Biologen weiblich, während 80 % der Doktortitel in Physik an Männer vergeben wurden.
Die Mitgliedschaft kann allen offen stehen, den Besitz einiger wissenschaftlicher Qualifikationen erfordern oder eine durch Wahl verliehene Ehre sein.
Die Wissenschaftspolitik befasst sich damit mit dem gesamten Themenbereich der Naturwissenschaften.
Prominente historische Beispiele sind die Chinesische Mauer, die im Laufe von zwei Jahrtausenden mit staatlicher Unterstützung mehrerer Dynastien fertiggestellt wurde, und der Große Kanal des Jangtsekiang, eine gewaltige Meisterleistung des Wasserbaus, die von Sunshu Ao (孫叔敖, 7. Jh.) begonnen wurde.
Solche Prozesse, die von Regierungen, Unternehmen oder Stiftungen durchgeführt werden, stellen knappe Mittel bereit.
Der staatliche Finanzierungsanteil ist in bestimmten Branchen höher und dominiert die Forschung in den Sozial- und Geisteswissenschaften.
Viele Faktoren können als Facetten der Politisierung der Wissenschaft wirken, wie etwa populistischer Antiintellektualismus, wahrgenommene Bedrohungen religiöser Überzeugungen, postmoderner Subjektivismus und Angst um Geschäftsinteressen.
Ein Experiment ist ein Verfahren, das durchgeführt wird, um eine Hypothese zu stützen oder zu widerlegen.
Experimente können die Testergebnisse verbessern und einem Schüler helfen, sich stärker für den Stoff zu interessieren, den er lernt, insbesondere wenn er im Laufe der Zeit verwendet wird.
Experimente umfassen typischerweise Kontrollen, die darauf ausgelegt sind, die Auswirkungen anderer Variablen als der einzelnen unabhängigen Variablen zu minimieren.
Forscher nutzen Experimente auch, um bestehende Theorien oder neue Hypothesen zu testen, um sie zu stützen oder zu widerlegen.
Wenn ein Experiment sorgfältig durchgeführt wird, stützen oder widerlegen die Ergebnisse normalerweise die Hypothese.
In der Medizin und den Sozialwissenschaften ist die Verbreitung experimenteller Forschung in den einzelnen Disziplinen sehr unterschiedlich.
Eine einzelne Studie beinhaltet normalerweise keine Wiederholungen des Experiments, aber einzelne Studien können durch systematische Überprüfung und Metaanalyse zusammengefasst werden.
Auf diese Weise können wir schließlich zu der Wahrheit gelangen, die das Herz befriedigt, und allmählich und vorsichtig das Ende erreichen, an dem Gewissheit erscheint. während wir durch Kritik und Vorsicht die Wahrheit erkennen können, die Meinungsverschiedenheiten zerstreut und zweifelhafte Fragen löst.
In diesem Prozess der kritischen Betrachtung sollte der Mensch selbst nicht vergessen, dass er – durch „Vorurteile“ und „Nachsicht“ – zu subjektiven Meinungen neigt und daher seine eigene Art der Hypothesenbildung kritisch hinterfragen muss.
Bacon wollte eine Methode, die auf wiederholbaren Beobachtungen oder Experimenten beruhte.
Beispielsweise maß Galileo Galilei (1564–1642) die Zeit genau und experimentierte, um genaue Messungen und Schlussfolgerungen über die Geschwindigkeit eines fallenden Körpers zu ziehen.
In einigen Disziplinen (z. B. Psychologie oder Politikwissenschaft) ist ein „wahres Experiment“ eine Methode der Sozialforschung, bei der es zwei Arten von Variablen gibt.
Ein gutes Beispiel wäre ein Medikamentenversuch.
Die Ergebnisse von Replikatproben können oft gemittelt werden, oder wenn eines der Replikate offensichtlich nicht mit den Ergebnissen der anderen Proben übereinstimmt, kann es als Ergebnis eines experimentellen Fehlers verworfen werden (ein Schritt des Testverfahrens könnte fehlerhaft sein). wurde für dieses Beispiel weggelassen).
Es ist bekannt, dass eine Negativkontrolle ein negatives Ergebnis liefert.
Am häufigsten wird der Wert der Negativkontrolle als „Hintergrundwert“ behandelt, der von den Testprobenergebnissen abgezogen wird.
Den Schülern könnte eine Flüssigkeitsprobe gegeben werden, die eine (für den Schüler) unbekannte Proteinmenge enthält.
Die Schüler könnten mehrere Positivkontrollproben herstellen, die verschiedene Verdünnungen des Proteinstandards enthalten.
Bei dem Assay handelt es sich um einen kolorimetrischen Assay, bei dem ein Spektrophotometer die Proteinmenge in Proben messen kann, indem es einen farbigen Komplex nachweist, der durch die Wechselwirkung von Proteinmolekülen und Molekülen eines zugesetzten Farbstoffs entsteht.
In diesem Fall beginnt das Experiment mit der Erstellung von zwei oder mehr Stichprobengruppen, die wahrscheinlich gleichwertig sind. Dies bedeutet, dass die Messungen der Merkmale in den Gruppen ähnlich sein sollten und dass die Gruppen bei gleicher Behandlung auf die gleiche Weise reagieren sollten.
Sobald äquivalente Gruppen gebildet wurden, versucht der Experimentator, sie bis auf die eine Variable, die er oder sie isolieren möchte, identisch zu behandeln.
Dadurch wird sichergestellt, dass etwaige Auswirkungen auf den Freiwilligen auf die Behandlung selbst zurückzuführen sind und nicht auf das Wissen, dass er behandelt wird, zurückzuführen sind.
Diese Hypothesen schlagen Gründe vor, um ein Phänomen zu erklären oder die Ergebnisse einer Handlung vorherzusagen.
Die Nullhypothese besagt, dass es durch die untersuchten Überlegungen keine Erklärung oder Vorhersagekraft für das Phänomen gibt.
So weit wie möglich versuchen sie, Daten für das System so zu sammeln, dass der Beitrag aller Variablen bestimmt werden kann und die Auswirkungen der Variation bei bestimmten Variablen annähernd konstant bleiben, sodass die Auswirkungen anderer Variablen erkannt werden können.
Normalerweise besteht jedoch eine gewisse Korrelation zwischen diesen Variablen, was die Zuverlässigkeit natürlicher Experimente im Vergleich zu den Schlussfolgerungen verringert, die bei der Durchführung eines kontrollierten Experiments möglich wären.
Beispielsweise ist es in der Astronomie eindeutig unmöglich, bei der Überprüfung der Hypothese „Sterne sind kollabierte Wasserstoffwolken“ mit einer riesigen Wasserstoffwolke zu beginnen und dann das Experiment durchzuführen, einige Milliarden Jahre darauf zu warten, dass sich daraus ein Stern bildet .
Aus diesem Grund wird Feldexperimenten manchmal eine höhere externe Validität zugeschrieben als Laborexperimenten.
In diesen Situationen sind Beobachtungsstudien wertvoll, da sie oft Hypothesen vorschlagen, die durch randomisierte Experimente oder durch die Sammlung neuer Daten überprüft werden können.
Darüber hinaus umfassen Beobachtungsstudien (z. B. in biologischen oder sozialen Systemen) häufig Variablen, die schwer zu quantifizieren oder zu kontrollieren sind.
Ohne ein statistisches Modell, das eine objektive Randomisierung widerspiegelt, stützt sich die statistische Analyse auf ein subjektives Modell.
Beispielsweise zeigen epidemiologische Studien zu Darmkrebs durchweg positive Zusammenhänge mit dem Verzehr von Brokkoli, während Experimente keinen Nutzen feststellen konnten.
Bei jedem randomisierten Versuch ist natürlich eine gewisse Abweichung vom Mittelwert zu erwarten, aber die Randomisierung stellt aufgrund des zentralen Grenzwertsatzes und der Markov-Ungleichung sicher, dass die Mittelwerte der Versuchsgruppen nahe beieinander liegen.
Um Bedingungen zu vermeiden, die ein Experiment weitaus weniger nützlich machen, quantifizieren und randomisieren Ärzte, die medizinische Studien durchführen – beispielsweise zur Genehmigung durch die US-amerikanische Lebensmittel- und Arzneimittelbehörde – die identifizierbaren Kovariaten.
Außerdem ist es im Allgemeinen unethisch (und oft illegal), zufällige Experimente zu den Auswirkungen minderwertiger oder schädlicher Behandlungen durchzuführen, beispielsweise zu den Auswirkungen der Einnahme von Arsen auf die menschliche Gesundheit.
Ein Physiklabor könnte einen Teilchenbeschleuniger oder eine Vakuumkammer enthalten, während ein Metallurgielabor über Geräte zum Gießen oder Raffinieren von Metallen oder zum Testen ihrer Festigkeit verfügen könnte.
Wissenschaftler in anderen Bereichen werden noch andere Arten von Laboren nutzen.
Entgegen der zugrunde liegenden Vorstellung vom Labor als begrenztem Raum für Experten wird der Begriff „Labor“ zunehmend auch für Werkstatträume wie Living Labs, Fab Labs oder Hackerspaces verwendet, in denen sich Menschen treffen, um an gesellschaftlichen Problemen zu arbeiten oder Prototypen herzustellen. zusammenarbeiten oder Ressourcen teilen.
Dieses Labor entstand, als Pythagoras ein Experiment über Klangtöne und Schwingungen von Saiten durchführte.
Im Jahr 2002 wurde zufällig ein unterirdisches alchemistisches Labor aus dem 16. Jahrhundert entdeckt.
Zu den Gefahren im Labor können Gifte gehören; Infektionserreger; brennbare, explosive oder radioaktive Materialien; bewegliche Maschinen; extreme Temperaturen; Laser, starke Magnetfelder oder Hochspannung.
Die Occupational Safety and Health Administration (OSHA) in den Vereinigten Staaten hat in Anerkennung der einzigartigen Eigenschaften des Laborarbeitsplatzes einen Standard für die berufsbedingte Exposition gegenüber gefährlichen Chemikalien in Laboren entwickelt.
Bei der Festlegung des richtigen Chemikalienhygieneplans für ein bestimmtes Unternehmen oder Labor ist es notwendig, die Anforderungen der Norm zu verstehen, die aktuellen Sicherheits-, Gesundheits- und Umweltpraktiken zu bewerten und die Gefahren einzuschätzen.
Darüber hinaus wird die Überprüfung durch Dritte auch genutzt, um eine objektive „Außenansicht“ zu ermöglichen, die einen neuen Blick auf Bereiche und Probleme ermöglicht, die möglicherweise als selbstverständlich angesehen oder aus Gewohnheit übersehen werden.
Schulungen sind für den fortlaufend sicheren Betrieb der Laboranlage von entscheidender Bedeutung.
Beispielsweise hat eine Forschungsgruppe einen Zeitplan, in dem sie an einem Tag der Woche zu ihrem eigenen Interessenthema recherchiert, während sie an den übrigen Tagen an einem bestimmten Gruppenprojekt arbeitet.
Ein Ortungsgerät ist ein Mitarbeiter eines Labors, der anhand eines eindeutigen Signals, das vom Ausweis jedes Mitarbeiters ausgesendet wird, dafür verantwortlich ist, herauszufinden, wo sich jedes Mitglied des Labors gerade befindet.
Ethnografische Studien ergaben, dass beim Personal jede Klasse (Forscher, Administratoren ...) einen unterschiedlichen Grad an Ansprüchen hat, der je nach Labor unterschiedlich ist.
Durch die Betrachtung der verschiedenen Interaktionen zwischen Mitarbeitern können wir ihre soziale Position in der Organisation bestimmen.
Eine Folge dieser sozialen Hierarchie ist also, dass der Locator je nach Mitarbeiter und seinen Rechten unterschiedliche Informationsgrade offenlegt.
Die soziale Hierarchie hängt auch mit der Einstellung gegenüber Technologien zusammen.
Beispielsweise würde ein Rezeptionist den Ausweis als nützlich erachten, da er ihm dabei helfen würde, Mitarbeiter tagsüber zu finden.
Mitarbeiter fühlen sich unwohl, wenn sie Anspruchsmuster, Pflichten, Respekt, informelle und formelle Hierarchien usw. ändern.
Natur im weitesten Sinne ist die natürliche, physische, materielle Welt oder das Universum. "
Obwohl der Mensch Teil der Natur ist, wird menschliches Handeln oft als eine von anderen Naturphänomenen getrennte Kategorie verstanden.
Der Begriff der Natur als Ganzes, des physikalischen Universums, ist eine von mehreren Erweiterungen des ursprünglichen Begriffs; Es begann mit bestimmten Kernanwendungen des Wortes φύσις durch vorsokratische Philosophen (obwohl dieses Wort damals eine dynamische Dimension hatte, insbesondere für Heraklit), und hat seitdem stetig an Bedeutung gewonnen.
Gleichzeitig wurde jedoch eine vitalistische Sicht der Natur wiedergeboren, die der vorsokratischen näher kam, insbesondere nach Charles Darwin.
Oft wird darunter die „natürliche Umwelt“ oder Wildnis verstanden – wilde Tiere, Felsen, Wälder und im Allgemeinen alles, was durch menschliches Eingreifen nicht wesentlich verändert wurde oder trotz menschlichem Eingreifen bestehen bleibt.
Seine hervorstechendsten klimatischen Merkmale sind seine zwei großen Polarregionen, zwei relativ schmale gemäßigte Zonen und eine weite äquatoriale tropische bis subtropische Region.
Der Rest besteht aus Kontinenten und Inseln, wobei der größte Teil des bewohnten Landes auf der Nordhalbkugel liegt.
Der Innenraum bleibt aktiv, mit einer dicken Schicht Kunststoffmantel und einem mit Eisen gefüllten Kern, der ein Magnetfeld erzeugt.
Gesteinseinheiten werden zunächst entweder durch Ablagerung an der Oberfläche eingelagert oder dringen in das darüber liegende Gestein ein.
Durch Ausgasungen und vulkanische Aktivität entstand die Uratmosphäre.
Kontinente bildeten sich, lösten sich dann auf und bildeten sich neu, während sich die Erdoberfläche im Laufe von Hunderten von Millionen Jahren neu formierte, und schlossen sich gelegentlich zu einem Superkontinent zusammen.
Während des Neoproterozoikums bedeckten eisige Temperaturen einen Großteil der Erde mit Gletschern und Eisschilden.
Das letzte Massenaussterben ereignete sich vor etwa 66 Millionen Jahren, als ein Meteoriteneinschlag wahrscheinlich das Aussterben der Nichtvogeldinosaurier und anderer großer Reptilien auslöste, kleine Tiere wie Säugetiere jedoch verschonte.
Das spätere Aufkommen des menschlichen Lebens sowie die Entwicklung der Landwirtschaft und der weiteren Zivilisation ermöglichten es dem Menschen, schneller als jede andere Lebensform auf die Erde einzuwirken und sowohl die Art und Menge anderer Organismen als auch das globale Klima zu beeinflussen.
Die dünne Gasschicht, die die Erde umhüllt, wird durch die Schwerkraft an Ort und Stelle gehalten.
Die Ozonschicht spielt eine wichtige Rolle bei der Reduzierung der Menge an ultravioletter (UV) Strahlung, die die Oberfläche erreicht.
Das terrestrische Wetter findet fast ausschließlich im unteren Teil der Atmosphäre statt und dient als konvektives System zur Wärmeumverteilung.
Ohne die Umverteilung der Wärmeenergie durch die Meeresströmungen und die Atmosphäre wären die Tropen außerdem viel heißer und die Polarregionen viel kälter.
Die Oberflächenvegetation hat sich in Abhängigkeit von den jahreszeitlichen Schwankungen des Wetters entwickelt, und plötzliche Veränderungen, die nur wenige Jahre dauern, können dramatische Auswirkungen sowohl auf die Vegetation als auch auf die Tiere haben, deren Wachstum als Nahrung dient.
Basierend auf historischen Aufzeichnungen ist bekannt, dass die Erde in der Vergangenheit drastische Klimaveränderungen, einschließlich Eiszeiten, erlebt hat.
Es gibt eine Reihe solcher Regionen, die vom tropischen Klima am Äquator bis zum Polarklima in den nördlichen und südlichen Extremen reichen.
Diese Belichtung wechselt, während sich die Erde auf ihrer Umlaufbahn dreht.
Wasser bedeckt 71 % der Erdoberfläche.
Kleinere Regionen der Ozeane werden Meere, Golfe, Buchten und andere Namen genannt.
Es ist nicht bekannt, ob die Seen von Titan von Flüssen gespeist werden, obwohl die Oberfläche von Titan von zahlreichen Flussbetten durchzogen ist.
Eine Vielzahl von künstlich angelegten Gewässern werden als Teiche klassifiziert, darunter Wassergärten zur ästhetischen Verzierung, Fischteiche zur kommerziellen Fischzucht und Solarteiche zur Speicherung von Wärmeenergie.
Kleine Flüsse können auch mit mehreren anderen Namen bezeichnet werden, darunter Bach, Bach, Bach, Rinnsal und Bach; Es gibt keine allgemeine Regel, die definiert, was als Fluss bezeichnet werden kann.
Die Struktur und Zusammensetzung wird durch verschiedene Umweltfaktoren bestimmt, die miteinander verknüpft sind.
Im Mittelpunkt des Ökosystemkonzepts steht die Idee, dass lebende Organismen mit jedem anderen Element in ihrer lokalen Umgebung interagieren.
Man könnte auch sagen, dass Leben einfach der charakteristische Zustand von Organismen ist.
Allerdings berücksichtigt nicht jede Definition von Leben alle diese Eigenschaften als wesentlich.
Aus der weitesten geophysiologischen Sicht ist die Biosphäre das globale Ökosystem, das alle Lebewesen und ihre Beziehungen, einschließlich ihrer Interaktion mit den Elementen Lithosphäre (Gesteine), Hydrosphäre (Wasser) und Atmosphäre (Luft), integriert.
Bisher wurden mehr als 2 Millionen Pflanzen- und Tierarten identifiziert, und Schätzungen über die tatsächliche Zahl der vorhandenen Arten reichen von mehreren Millionen bis weit über 50 Millionen.
Arten, die sich nicht an die veränderte Umwelt und die Konkurrenz durch andere Lebensformen anpassen konnten, starben aus.
Als grundlegende Formen des Pflanzenlebens den Prozess der Photosynthese entwickelten, konnte die Energie der Sonne genutzt werden, um Bedingungen zu schaffen, die komplexere Lebensformen ermöglichten.
Mikroorganismen sind einzellige Organismen, die im Allgemeinen mikroskopisch klein und kleiner sind, als das menschliche Auge sehen kann.
Ihre Fortpflanzung ist sowohl schnell als auch reichlich.
Seitdem ist klar geworden, dass die Plantae in ihrer ursprünglichen Definition mehrere nicht verwandte Gruppen umfassten und die Pilze und mehrere Algengruppen in neue Königreiche verlegt wurden.
Zu den vielen Möglichkeiten, Pflanzen zu klassifizieren, gehört die regionale Flora, die je nach Untersuchungszweck auch fossile Flora, Überreste pflanzlichen Lebens aus einer früheren Ära, umfassen kann.
Einige Arten der „einheimischen Flora“ wurden tatsächlich vor Jahrhunderten von Menschen eingeführt, die von einer Region oder einem Kontinent in eine andere migrierten, und wurden zu einem integralen Bestandteil der einheimischen oder natürlichen Flora des Ortes, an dem sie eingeführt wurden.
Tiere als Kategorie weisen mehrere Merkmale auf, die sie im Allgemeinen von anderen Lebewesen unterscheiden.
Sie unterscheiden sich von Pflanzen, Algen und Pilzen auch dadurch, dass ihnen Zellwände fehlen.
Typischerweise gibt es auch eine innere Verdauungskammer.
Eine in „Nature“ veröffentlichte Studie aus dem Jahr 2020 ergab, dass die anthropogene Masse (von Menschen hergestellte Materialien) die gesamte lebende Biomasse auf der Erde übersteigt, wobei allein Plastik die Masse aller Land- und Meerestiere zusammen übersteigt.
Trotz dieser Fortschritte bleibt das Schicksal der menschlichen Zivilisation jedoch weiterhin eng mit Veränderungen in der Umwelt verbunden.
Der Mensch hat zum Aussterben vieler Pflanzen und Tiere beigetragen, wobei etwa eine Million Arten innerhalb von Jahrzehnten vom Aussterben bedroht waren.
Dies verzerrt die Marktpreise für natürliche Ressourcen und führt gleichzeitig zu einer Unterinvestition in unsere natürlichen Vermögenswerte.
Die Regierungen haben diese wirtschaftlichen Externalitäten nicht verhindert.
Einige Aktivitäten wie Jagen und Angeln dienen sowohl der Ernährung als auch der Freizeit und werden oft von unterschiedlichen Personen ausgeübt.
Dass die Natur in so viel Kunst, Fotografie, Poesie und anderer Literatur dargestellt und gefeiert wurde, zeigt die Stärke, mit der viele Menschen Natur und Schönheit verbinden.
Natur und Wildnis waren in verschiedenen Epochen der Weltgeschichte wichtige Themen.
Obwohl Naturwunder in den Psalmen und im Buch Hiob gepriesen werden, wurden Wildnisdarstellungen in der Kunst im 19. Jahrhundert immer häufiger, insbesondere in den Werken der Romantik.
Aus diesem Grund wird im Allgemeinen die „Physik“ als die grundlegendste Wissenschaft verstanden – deren Name immer noch als „Lehre der Natur“ erkennbar ist.
Man geht heute davon aus, dass die sichtbaren Bestandteile des Universums nur 4,9 Prozent der Gesamtmasse ausmachen.
Das Verhalten von Materie und Energie im gesamten beobachtbaren Universum scheint klar definierten physikalischen Gesetzen zu folgen.
Es gibt keine eindeutige Grenze zwischen der Erdatmosphäre und dem Weltraum, da die Atmosphäre mit zunehmender Höhe allmählich schwächer wird.
Es gibt auch etwas Gas, Plasma und Staub sowie kleine Meteore.
Obwohl die Erde der einzige Körper im Sonnensystem ist, von dem bekannt ist, dass er Leben beherbergt, gibt es Hinweise darauf, dass der Planet Mars in der fernen Vergangenheit flüssiges Wasser auf der Oberfläche besaß.
Wenn es überhaupt Leben auf dem Mars gibt, ist es höchstwahrscheinlich unter der Erde, wo noch flüssiges Wasser existieren kann.
Beobachtung ist die aktive Erfassung von Informationen aus einer Primärquelle.
Der Einsatz von Messungen wurde entwickelt, um die Aufzeichnung und den Vergleich von Beobachtungen zu ermöglichen, die zu verschiedenen Zeiten und an verschiedenen Orten von verschiedenen Personen gemacht wurden.
Bei der Messung wird die Anzahl der Standardeinheiten gezählt, die der Beobachtung entspricht.
Wissenschaftliche Instrumente wurden entwickelt, um die menschliche Beobachtungsfähigkeit zu unterstützen, wie z. B. Waagen, Uhren, Teleskope, Mikroskope, Thermometer, Kameras und Tonbandgeräte, und um auch Ereignisse, die mit den Sinnen nicht wahrnehmbar sind, in wahrnehmbare Form zu übersetzen, wie z. B. Indikatorfarbstoffe und Voltmeter , Spektrometer, Infrarotkameras, Oszilloskope, Interferometer, Geigerzähler und Funkempfänger.
Beispielsweise ist es normalerweise nicht möglich, den Luftdruck in einem Autoreifen zu prüfen, ohne einen Teil der Luft abzulassen und dadurch den Druck zu verändern.
Im Zwillingsparadoxon beispielsweise unternimmt ein Zwilling eine Reise mit nahezu Lichtgeschwindigkeit und kommt jünger nach Hause als der Zwilling, der zu Hause geblieben ist.
Quantenmechanik: In der Quantenmechanik, die sich mit dem Verhalten sehr kleiner Objekte befasst, ist es nicht möglich, ein System zu beobachten, ohne das System zu verändern, und der „Beobachter“ muss als Teil des beobachteten Systems betrachtet werden.
Die menschliche Wahrnehmung erfolgt durch einen komplexen, unbewussten Prozess der Abstraktion, bei dem bestimmte Details der eingehenden Sinnesdaten wahrgenommen und erinnert werden und der Rest vergessen wird.
Später, wenn man sich an Ereignisse erinnert, können Erinnerungslücken sogar durch „plausible“ Daten gefüllt werden, die der Verstand erfindet, um sie an das Modell anzupassen; das nennt man rekonstruktives Gedächtnis.
In der Psychologie nennt man das Bestätigungsbias.
Nehmen wir zum Beispiel an, dass ein Beobachter sieht, wie ein Elternteil sein Kind schlägt; und kann folglich feststellen, dass eine solche Handlung entweder gut oder schlecht ist.
Forschung ist „kreative und systematische Arbeit zur Erweiterung des Wissensbestands“.
Um die Gültigkeit von Instrumenten, Verfahren oder Experimenten zu testen, kann die Forschung Elemente früherer Projekte oder das Projekt als Ganzes replizieren.
Dieses Material hat Primärquellencharakter.
Bei experimenteller Arbeit handelt es sich typischerweise um die direkte oder indirekte Beobachtung des/der untersuchten Subjekt(e), z. B. im Labor oder im Feld, um die Methodik, Ergebnisse und Schlussfolgerungen eines Experiments oder einer Reihe von Experimenten zu dokumentieren oder eine neuartige Interpretation anzubieten der bisherigen Ergebnisse.
Der Grad der Originalität der Forschung gehört zu den Hauptkriterien für die Veröffentlichung von Artikeln in Fachzeitschriften und wird in der Regel durch Peer-Review festgestellt.
Diese Forschung liefert wissenschaftliche Informationen und Theorien zur Erklärung der Natur und der Eigenschaften der Welt.
Die wissenschaftliche Forschung lässt sich nach ihren Fach- und Anwendungsdisziplinen in verschiedene Klassifikationen unterteilen.
Geisteswissenschaftler suchen in der Regel nicht nach der letztendlich richtigen Antwort auf eine Frage, sondern erforschen stattdessen die damit verbundenen Probleme und Details.
Historiker nutzen Primärquellen und andere Beweise, um ein Thema systematisch zu untersuchen und dann Geschichten in Form von Berichten über die Vergangenheit zu schreiben.
Die Forschung muss dadurch begründet werden, dass ihre Bedeutung mit bereits vorhandenem Wissen über das Thema verknüpft wird.
Im Allgemeinen wird eine Hypothese verwendet, um Vorhersagen zu treffen, die durch Beobachtung des Ergebnisses eines Experiments überprüft werden können.
Diese sorgfältige Sprache wird verwendet, weil Forscher erkennen, dass auch alternative Hypothesen mit den Beobachtungen übereinstimmen können.
Da die Genauigkeit der Beobachtung mit der Zeit zunimmt, liefert die Hypothese möglicherweise keine genaue Vorhersage mehr.
Künstlerische Forschung wurde von der Schule für Tanz und Zirkus (Dans och Cirkushögskolan, DOCH) in Stockholm wie folgt definiert: „Künstlerische Forschung ist das Erforschen und Testen mit dem Ziel, Wissen innerhalb und für unsere künstlerischen Disziplinen zu gewinnen.“
Ziel der künstlerischen Forschung ist es, Wissen und Verständnis durch die Präsentation der Künste zu erweitern.
Laut dem Künstler Hakan Topal wird in der künstlerischen Forschung „vielleicht mehr als in anderen Disziplinen die Intuition als Methode genutzt, um ein breites Spektrum neuer und unerwarteter produktiver Modalitäten zu identifizieren“.
Hintergrundrecherchen könnten beispielsweise geografische oder verfahrensbezogene Recherchen umfassen.
Die Literaturrecherche identifiziert Mängel oder Lücken in früheren Forschungsarbeiten, die die Studie rechtfertigen.
Die Forschungsfrage kann parallel zur Hypothese sein.
Anschließend analysiert und interpretiert der/die Forscher die Daten mithilfe verschiedener statistischer Methoden und führt dabei eine sogenannte empirische Forschung durch.
Einige Forscher befürworten jedoch den umgekehrten Ansatz: Beginnen Sie mit der Formulierung und Diskussion der Ergebnisse und gehen Sie „nach oben“ zur Identifizierung eines Forschungsproblems, das in den Ergebnissen und der Literaturrecherche zum Vorschein kommt.
Qualitative Forschung wird häufig als Methode der explorativen Forschung als Grundlage für spätere quantitative Forschungshypothesen verwendet.
Quantitative Forschung ist mit der philosophischen und theoretischen Haltung des Positivismus verbunden.
Bei der quantitativen Forschung geht es darum, aus der Theorie abgeleitete Hypothesen zu testen oder die Größe eines interessierenden Phänomens abschätzen zu können.
Wenn die Absicht darin besteht, von den Forschungsteilnehmern auf eine größere Population zu verallgemeinern, wird der Forscher Wahrscheinlichkeitsstichproben anwenden, um die Teilnehmer auszuwählen.
Sekundärdaten sind bereits vorhandene Daten, beispielsweise Volkszählungsdaten, die für die Forschung wiederverwendet werden können.
Diese Methode bietet Vorteile, die die Verwendung einer einzigen Methode nicht bieten kann.
Nicht-empirische Forschung ist keine absolute Alternative zur empirischen Forschung, da sie zusammen verwendet werden können, um einen Forschungsansatz zu stärken.
Die Verwaltung der Forschungsethik ist in den einzelnen Ländern uneinheitlich und es gibt keinen allgemein akzeptierten Ansatz, wie damit umgegangen werden sollte.
Unabhängig vom Ansatz wird die Anwendung ethischer Theorie auf bestimmte kontroverse Themen als angewandte Ethik bezeichnet, und Forschungsethik kann als eine Form der angewandten Ethik angesehen werden, da ethische Theorie in realen Forschungsszenarien angewendet wird.
Forschungsethik ist als Konzept in der medizinischen Forschung am weitesten entwickelt; der bemerkenswerteste Kodex ist die Deklaration von Helsinki aus dem Jahr 1964.
Die Metaforschung befasst sich mit der Erkennung von Voreingenommenheit, methodischen Mängeln und anderen Fehlern und Ineffizienzen.
Wissenschaftler aus der Peripherie stehen vor den Herausforderungen von Ausgrenzung und Linguismus in der Forschung und wissenschaftlichen Veröffentlichung.
In vergleichenden Politikstudien sind westliche Länder in Einzelländerstudien überrepräsentiert, wobei der Schwerpunkt auf Westeuropa, Kanada, Australien und Neuseeland liegt.
Studien mit einem engen Umfang können zu mangelnder Generalisierbarkeit führen, was bedeutet, dass die Ergebnisse möglicherweise nicht auf andere Bevölkerungsgruppen oder Regionen anwendbar sind.
Beim Peer-Review-Verfahren werden in der Regel Experten aus dem gleichen Fachgebiet einbezogen, die von den Herausgebern konsultiert werden, um aus einer unvoreingenommenen und unparteiischen Sicht eine Bewertung der wissenschaftlichen Arbeiten eines ihrer Kollegen abzugeben, und dies geschieht in der Regel kostenlos.
Beispielsweise sind die meisten indigenen Gemeinschaften der Ansicht, dass der Zugang zu bestimmten, der Gruppe eigenen Informationen durch Beziehungen bestimmt werden sollte.
Das System variiert stark je nach Fachgebiet und verändert sich auch ständig, wenn auch oft langsam.
Diese Rechercheformen sind in Datenbanken explizit für Abschlussarbeiten und Dissertationen zu finden.
Die Arten von Veröffentlichungen, die als Wissens- oder Forschungsbeiträge akzeptiert werden, variieren stark zwischen den Fachgebieten, vom gedruckten bis zum elektronischen Format.
Geschäftsmodelle sind im elektronischen Umfeld unterschiedlich.
Viele leitende Forscher (z. B. Gruppenleiter) verbringen einen erheblichen Teil ihrer Zeit damit, sich um Zuschüsse für Forschungsgelder zu bewerben.
Die wissenschaftliche Methode ist eine empirische Methode zur Wissensgewinnung, die die Entwicklung der Wissenschaft seit mindestens dem 17. Jahrhundert prägt (mit namhaften Praktikern in früheren Jahrhunderten).
Dabei handelt es sich um Grundsätze der wissenschaftlichen Methode im Gegensatz zu einer endgültigen Reihe von Schritten, die für alle wissenschaftlichen Unternehmungen gelten.
Eine Hypothese ist eine Vermutung, die auf Erkenntnissen basiert, die bei der Suche nach Antworten auf die Frage gewonnen wurden.
Es gibt jedoch Schwierigkeiten bei einer formelhaften Aussage über die Methode.
Der Begriff „wissenschaftliche Methode“ entstand im 19. Jahrhundert, als eine bedeutende institutionelle Entwicklung der Wissenschaft stattfand und Terminologien auftauchten, die klare Grenzen zwischen Wissenschaft und Nichtwissenschaft festlegten, wie etwa „Wissenschaftler“ und „Pseudowissenschaft“.
Gauch 2003 und Tow 2010 widersprechen Feyerabends Behauptung; Problemlöser und Forscher müssen bei ihrer Untersuchung sparsam mit ihren Ressourcen umgehen.
Die Philosophen Robert Nola und Howard Sankey sagten in ihrem 2007 erschienenen Buch Theories of Scientific Method, dass die Debatten über wissenschaftliche Methoden weitergehen, und argumentierten, dass Feyerabend trotz des Titels „Gegen die Methode“ bestimmte Methodenregeln akzeptierte und versuchte, diese Regeln mit einem Meta zu rechtfertigen Methodik.
Das allgegenwärtige Element der wissenschaftlichen Methode ist der Empirismus.
Die wissenschaftliche Methode widerspricht Behauptungen, dass Offenbarung, politische oder religiöse Dogmen, Berufungen auf Traditionen, allgemein verbreitete Überzeugungen, den gesunden Menschenverstand oder aktuelle Theorien die einzig möglichen Mittel zur Demonstration der Wahrheit darstellen.
Ab dem 16. Jahrhundert wurden Experimente von Francis Bacon befürwortet und von Giambattista della Porta, Johannes Kepler und Galileo Galilei durchgeführt.
Wie in anderen Forschungsbereichen kann die Wissenschaft (durch die wissenschaftliche Methode) auf Vorkenntnissen aufbauen und im Laufe der Zeit ein differenzierteres Verständnis ihrer Forschungsthemen entwickeln.
Dieses Modell kann als Grundlage der wissenschaftlichen Revolution angesehen werden: „
Eine Vermutung könnte sein, dass ein neues Medikament die Krankheit bei einigen Menschen in dieser Bevölkerung heilen wird, wie in einer klinischen Studie mit dem Medikament.
Diese Vorhersagen sind Erwartungen an die Testergebnisse.
Der Unterschied zwischen erwartet und tatsächlich gibt an, welche Hypothese die resultierenden Daten aus dem Experiment besser erklärt.
Abhängig von der Komplexität des Experiments kann eine Iteration des Prozesses erforderlich sein, um genügend Beweise zu sammeln, um die Frage zuverlässig zu beantworten, oder um andere Antworten auf sehr spezifische Fragen zu finden, um eine einzelne umfassendere Frage zu beantworten.
Röntgenbeugungsmuster von DNA von Florence Bell in ihrer Doktorarbeit. These (1939) waren ähnlich (wenn auch nicht so gut) wie „Foto 51“, aber diese Forschung wurde durch die Ereignisse des Zweiten Weltkriegs unterbrochen.
Juni 1952 – Watson war es gelungen, Röntgenbilder von TMV zu erhalten, die ein Beugungsmuster zeigten, das mit der Transformation einer Helix übereinstimmte.
Diese Vorhersage war ein mathematisches Konstrukt, völlig unabhängig vom vorliegenden biologischen Problem.
DNA ist keine Helix.
Zum Beispiel die Anzahl der Stränge im Rückgrat der Helix (Crick vermutete zwei Stränge, ermahnte Watson jedoch, dies kritischer zu prüfen), die Position der Basenpaare (innerhalb des Rückgrats oder außerhalb des Rückgrats) usw.
Aber Wilkins stimmt dem erst nach Franklins Weggang zu.: „
Anschließend erstellten er und Crick ihr Modell und verwendeten diese Informationen zusammen mit den zuvor bekannten Informationen über die Zusammensetzung der DNA, insbesondere Chargaffs Regeln der Basenpaarung:
Bei signifikanten oder überraschenden Ergebnissen können auch andere Wissenschaftler versuchen, die Ergebnisse für sich selbst zu reproduzieren, insbesondere wenn diese Ergebnisse für ihre eigene Arbeit wichtig wären.
Das Peer-Review bestätigt nicht die Richtigkeit der Ergebnisse, sondern lediglich, dass die Experimente selbst nach Meinung des Gutachters solide waren (basierend auf der Beschreibung des Experimentators).
Diese methodischen Elemente und die Organisation der Verfahren sind tendenziell eher für experimentelle Wissenschaften als für Sozialwissenschaften charakteristisch.
Die oben genannten Elemente werden im Bildungssystem häufig als „wissenschaftliche Methode“ gelehrt.
Als Einstein beispielsweise die Spezielle und Allgemeine Relativitätstheorie entwickelte, widerlegte er Newtons Principia in keiner Weise oder lehnte sie ab.
Die systematische, sorgfältige Sammlung von Messungen oder Zählungen relevanter Größen ist oft der entscheidende Unterschied zwischen Pseudowissenschaften wie der Alchemie und Naturwissenschaften wie der Chemie oder Biologie.
Unsicherheiten können auch unter Berücksichtigung der Unsicherheiten der einzelnen zugrunde liegenden Größen berechnet werden.
Die operative Definition einer Sache beruht häufig auf Vergleichen mit Standards: Die operative Definition von „Masse“ beruht letztendlich auf der Verwendung eines Artefakts, beispielsweise eines bestimmten Kilogramms Platin-Iridium, das in einem Labor in Frankreich aufbewahrt wird.
Wissenschaftliche Größen werden häufig durch ihre Maßeinheiten charakterisiert, die später bei der Kommunikation der Arbeit in herkömmlichen physikalischen Einheiten beschrieben werden können.
Die Messungen der chaldäischen, indischen, persischen, griechischen, arabischen und europäischen Astronomen erforderten Jahrtausende, um die Bewegung des Planeten Erde vollständig zu erfassen.
Der beobachtete Unterschied zwischen Newtons Theorie und Beobachtung für die Präzession des Merkur war eines der Dinge, die Albert Einstein als möglichen frühen Test seiner Allgemeinen Relativitätstheorie einfielen.
Den Wissenschaftlern steht es frei, alle ihnen zur Verfügung stehenden Ressourcen – ihre eigene Kreativität, Ideen aus anderen Bereichen, induktives Denken, Bayes’sche Folgerung usw. – zu nutzen, um sich mögliche Erklärungen für ein untersuchtes Phänomen auszudenken.
Wissenschaftler verwenden diese Begriffe häufig, um eine Theorie zu bezeichnen, die den bekannten Fakten folgt, aber dennoch relativ einfach und leicht zu handhaben ist.
Es ist wichtig, dass das Ergebnis der Prüfung einer solchen Vorhersage derzeit nicht bekannt ist.
Wenn die Vorhersagen nicht durch Beobachtung oder Erfahrung zugänglich sind, ist die Hypothese noch nicht überprüfbar und bleibt daher im engeren Sinne unwissenschaftlich.
Dies implizierte, dass das Röntgenbeugungsmuster der DNA „x-förmig“ sein würde.
Manchmal werden die Experimente falsch durchgeführt oder sind im Vergleich zu einem entscheidenden Experiment nicht sehr gut konzipiert.
Diese Technik nutzt den Kontrast zwischen mehreren Stichproben, Beobachtungen oder Populationen unter unterschiedlichen Bedingungen, um zu sehen, was variiert oder was gleich bleibt.
Die Faktoranalyse ist eine Technik zur Ermittlung des wichtigen Faktors in einer Wirkung.
Sogar der Flug von New York nach Paris ist ein Experiment, das die aerodynamischen Hypothesen überprüft, die für den Bau des Flugzeugs verwendet wurden.
Franklin erkannte sofort die Mängel, die den Wassergehalt betrafen.
Gelingt es einem Wissenschaftler nicht, eine interessante Hypothese zu entwickeln, kann dies dazu führen, dass ein Wissenschaftler das betrachtete Thema neu definiert.
Andere Wissenschaftler können jederzeit mit ihrer eigenen Forschung beginnen und in den Prozess einsteigen.
Entscheidend ist, dass experimentelle und theoretische Ergebnisse von anderen innerhalb der wissenschaftlichen Gemeinschaft reproduziert werden müssen.
Je besser eine Erklärung Vorhersagen treffen kann, desto nützlicher kann sie häufig sein und desto wahrscheinlicher ist es, dass sie weiterhin eine Reihe von Beweisen besser erklärt als ihre Alternativen.
Wissenschaftliche Modelle unterscheiden sich in dem Umfang und der Dauer ihrer experimentellen Erprobung sowie in ihrer Akzeptanz in der wissenschaftlichen Gemeinschaft.
Wenn solche Beweise gefunden werden, kann eine neue Theorie vorgeschlagen werden, oder (häufiger) wird festgestellt, dass Modifikationen der vorherigen Theorie ausreichen, um die neuen Beweise zu erklären.
Beispielsweise erklärten Newtons Gesetze die jahrtausendelange wissenschaftliche Beobachtung der Planeten nahezu perfekt.
Da neue Theorien möglicherweise umfassender sind als ihre Vorgänger und daher in der Lage sind, mehr zu erklären als frühere, können Nachfolgetheorien möglicherweise einen höheren Standard erfüllen, indem sie eine größere Menge an Beobachtungen erklären als ihre Vorgänger.
Sobald sich ein strukturell vollständiges und geschlossenes System von Meinungen gebildet hat, das aus vielen Details und Beziehungen besteht, bietet es dauerhaften Widerstand gegen alles, was ihm widerspricht.“
Seine Erfolge können glänzen, sind aber meist nur vorübergehender Natur.
Die Methode des Apriori – die Konformität weniger brutal fördert, sondern Meinungen als so etwas wie Geschmäcker fördert, die in Gesprächen und Vergleichen von Perspektiven im Hinblick darauf entstehen, „was für die Vernunft akzeptabel ist“.
Das ist ein Ziel, das so weit oder nahe ist wie die Wahrheit selbst für Sie oder mich oder die gegebene endliche Gemeinschaft.
Von der Abduktion unterscheidet Peirce die Induktion durch die auf Tests basierende Ableitung des Wahrheitsanteils der Hypothese.
Selbst ein gut vorbereiteter Geist vermutet oft falsch.
Peirce, Charles S. (1902), Carnegie-Antrag, siehe MS L75.329330, aus Draft D von Memoir 27: „Folglich bedeutet entdecken einfach, ein Ereignis zu beschleunigen, das früher oder später eintreten würde, wenn wir uns nicht die Mühe gemacht hätten.“ Machen Sie die Entdeckung.
Folglich muss die Durchführung einer Entführung, die hauptsächlich eine Frage der Heuretik ist und die erste Frage der Heuretik ist, von wirtschaftlichen Erwägungen bestimmt werden.“
Da die Hypothese unsicher ist, muss sie praktische Implikationen haben, die zumindest zu mentalen Tests führen und sich in der Wissenschaft für wissenschaftliche Tests eignen.
Einstein, Albert (1936, 1956) Man könnte sagen: „Das ewige Geheimnis der Welt ist ihre Verständlichkeit.“
Diese Annahmen des methodologischen Naturalismus bilden eine Grundlage, auf der sich die Wissenschaft gründen kann.
Seine Beobachtungen der wissenschaftlichen Praxis sind im Wesentlichen soziologischer Natur und geben keinen Aufschluss darüber, wie Wissenschaft in anderen Zeiten und anderen Kulturen praktiziert wird oder praktiziert werden kann.
Er eröffnet Kapitel 1 mit einer Diskussion der Golgi-Körper und ihrer anfänglichen Ablehnung als Artefakt der Färbetechnik sowie einer Diskussion darüber, wie Brahe und Kepler die Morgendämmerung beobachteten und trotz des gleichen physiologischen Phänomens einen „anderen“ Sonnenaufgang sahen.
Im Wesentlichen sagt er, dass es für jede bestimmte Methode oder Norm der Wissenschaft eine historische Episode gibt, in der die Verletzung dieser Methode zum Fortschritt der Wissenschaft beigetragen hat.
Die postmoderne Wissenschaftskritik war selbst Gegenstand heftiger Kontroversen.
Sowohl in den Naturwissenschaften als auch in der Mathematik müssen Modelle in sich konsistent sein und außerdem falsifizierbar (widerlegbar) sein.
Beispielsweise entstand in der Wissenschaft das technische Konzept der Zeit, und Zeitlosigkeit war ein Kennzeichen eines mathematischen Themas.
Eugene Wigners Aufsatz „The Unreasonable Effectiveness of Mathematics in the Natural Sciences“ ist ein sehr bekannter Bericht eines mit dem Nobelpreis ausgezeichneten Physikers zu diesem Thema.
In Beweisen und Widerlegungen gab Lakatos mehrere Grundregeln für die Suche nach Beweisen und Gegenbeispielen für Vermutungen an.
Dies könnte erklären, warum Wissenschaftler so oft sagen, sie hätten Glück gehabt.
Mahwah, NJ: Lawrence Erlbaum Associates.
Das ist es, was Nassim Nicholas Taleb „Anti-Fragilität“ nennt; Während einige Untersuchungssysteme angesichts menschlicher Fehler, menschlicher Voreingenommenheit und Zufälligkeit fragil sind, ist die wissenschaftliche Methode mehr als resistent oder hart – sie profitiert tatsächlich in vielerlei Hinsicht von dieser Zufälligkeit (sie ist antifragil).
Diese unerwarteten Ergebnisse veranlassen Forscher dazu, zu versuchen, einen Fehler in ihrer Methode zu beheben.
Eine wissenschaftliche Theorie ist eine Erklärung eines Aspekts der natürlichen Welt und des Universums, die wiederholt gemäß der wissenschaftlichen Methode unter Verwendung anerkannter Protokolle zur Beobachtung, Messung und Bewertung der Ergebnisse getestet und verifiziert wurde.
Etablierte wissenschaftliche Theorien haben einer strengen Prüfung standgehalten und verkörpern wissenschaftliche Erkenntnisse.
Stephen Jay Gould schrieb: „...Fakten und Theorien sind verschiedene Dinge, keine Stufen in einer Hierarchie zunehmender Gewissheit.“
Die Bedeutung des Begriffs „wissenschaftliche Theorie“ (der Kürze halber oft verkürzt auf „Theorie“), wie er in den Wissenschaftsdisziplinen verwendet wird, unterscheidet sich erheblich von der üblichen umgangssprachlichen Verwendung von Theorie.
Im alltäglichen Sprachgebrauch kann Theorie eine Erklärung bedeuten, die eine unbegründete und spekulative Vermutung darstellt, während sie in der Wissenschaft eine Erklärung beschreibt, die getestet wurde und allgemein als gültig akzeptiert wird.
Einige Theorien sind so etabliert, dass sie wahrscheinlich nie grundlegend geändert werden (z. B. wissenschaftliche Theorien wie Evolution, heliozentrische Theorie, Zelltheorie, Theorie der Plattentektonik, Keimtheorie von Krankheiten usw.).
Wissenschaftliche Theorien sind überprüfbar und machen falsifizierbare Vorhersagen.
Das bestimmende Merkmal aller wissenschaftlichen Erkenntnisse, einschließlich Theorien, ist die Fähigkeit, falsifizierbare oder überprüfbare Vorhersagen zu treffen.
Es wird durch viele unabhängige Beweisstränge und nicht durch eine einzige Grundlage gut gestützt.
Die Theorie der biologischen Evolution ist mehr als „nur eine Theorie“.
Dies liefert Beweise für oder gegen die Hypothese.
Dies kann viele Jahre dauern, da es schwierig oder kompliziert sein kann, ausreichende Beweise zu sammeln.
Die Stärke der Beweise wird von der wissenschaftlichen Gemeinschaft bewertet und die wichtigsten Experimente werden von mehreren unabhängigen Gruppen wiederholt.
In der Chemie gibt es viele Säure-Base-Theorien, die sehr unterschiedliche Erklärungen für die zugrunde liegende Natur saurer und basischer Verbindungen liefern, aber sie sind sehr nützlich für die Vorhersage ihres chemischen Verhaltens.
Um eine Theorie zu akzeptieren, müssen nicht alle wichtigen Vorhersagen überprüft werden, wenn sie bereits durch ausreichend starke Beweise gestützt wird.
Lösungen erfordern möglicherweise geringfügige oder größere Änderungen an der Theorie oder gar keine, wenn eine zufriedenstellende Erklärung innerhalb des bestehenden Rahmens der Theorie gefunden wird.
Wenn Änderungen an der Theorie oder andere Erklärungen nicht ausreichen, um die neuen Ergebnisse zu erklären, ist möglicherweise eine neue Theorie erforderlich.
Dies liegt daran, dass es immer noch die beste verfügbare Erklärung für viele andere Phänomene ist, was durch seine Vorhersagekraft in anderen Kontexten bestätigt wird.
Nach den Änderungen wird die akzeptierte Theorie mehr Phänomene erklären und eine größere Vorhersagekraft haben (sonst würden die Änderungen nicht übernommen); Diese neue Erklärung kann dann weiter ersetzt oder geändert werden.
Beispielsweise weiß man heute, dass Elektrizität und Magnetismus zwei Aspekte desselben Phänomens sind, das als Elektromagnetismus bezeichnet wird.
Dies wurde durch die Entdeckung der Kernfusion, der Hauptenergiequelle der Sonne, gelöst.
Indem Einstein den leuchtenden Äther aus der Speziellen Relativitätstheorie wegließ, stellte er fest, dass die Zeitdilatation und Längenkontraktion, die an einem Objekt in relativer Bewegung gemessen werden, träge ist – das heißt, das Objekt weist eine konstante Geschwindigkeit auf, die Geschwindigkeit mit Richtung korrespondiert, wenn es von seinem Beobachter gemessen wird.
Einstein versuchte, das Invarianzprinzip auf alle Bezugssysteme zu übertragen, ob träge oder beschleunigend.
Sogar masselose Energie übt eine Gravitationsbewegung auf lokale Objekte aus, indem sie die geometrische „Oberfläche“ der 4D-Raumzeit „krümmt“.
Allerdings handelt es sich bei wissenschaftlichen Gesetzen um beschreibende Darstellungen darüber, wie sich die Natur unter bestimmten Bedingungen verhält.
Ein weit verbreitetes Missverständnis besteht darin, dass es sich bei wissenschaftlichen Theorien um rudimentäre Ideen handelt, die schließlich in wissenschaftliche Gesetze übergehen, wenn genügend Daten und Beweise gesammelt wurden.
Sowohl Theorien als auch Gesetze könnten potenziell durch Gegenbeweise verfälscht werden.
Ein Beispiel für eine formale Sprache ist die Logik erster Ordnung.
Die durch die Theorien erklärten Phänomene wurden als theoretische Konzepte behandelt, sofern sie nicht direkt mit den Sinnen beobachtet werden konnten (z. B. Atome und Radiowellen).
Zur Beschreibung dieses Ansatzes wird der Ausdruck „die rezipierte Sicht auf Theorien“ verwendet.
Man kann Sprache verwenden, um ein Modell zu beschreiben; Die Theorie ist jedoch das Modell (oder eine Sammlung ähnlicher Modelle) und nicht die Beschreibung des Modells.
Die Modellparameter, z. B. das Newtonsche Gravitationsgesetz, bestimmen, wie sich die Positionen und Geschwindigkeiten mit der Zeit ändern.
Das Wort „Semantik“ bezieht sich auf die Art und Weise, wie ein Modell die reale Welt darstellt.
In der Ingenieurspraxis wird zwischen „mathematischen Modellen“ und „physikalischen Modellen“ unterschieden; Die Kosten für die Herstellung eines physikalischen Modells können minimiert werden, indem zunächst ein mathematisches Modell mithilfe eines Computersoftwarepakets, beispielsweise eines computergestützten Designtools, erstellt wird.
Für alle empirischen Behauptungen sind bestimmte Annahmen notwendig (z. B. die Annahme, dass die Realität existiert).
Dies kann so einfach sein wie die Beobachtung, dass die Theorie genaue Vorhersagen macht, was ein Beweis dafür ist, dass alle zu Beginn getroffenen Annahmen unter den getesteten Bedingungen richtig oder annähernd richtig sind.
Die Theorie macht genaue Vorhersagen, wenn die Annahme gültig ist, und macht keine genauen Vorhersagen, wenn die Annahme nicht gültig ist.
Das Oxford English Dictionary (OED) und Online-Wiktionary geben seine lateinische Quelle als „annehmen, an sich nehmen, adoptieren, usurpieren“ an, was eine Konjunktion von ad- („zu, in Richtung, bei“) und sumere ( nehmen).
Der Begriff wurde ursprünglich in religiösen Kontexten verwendet, etwa für „in den Himmel aufsteigen“, insbesondere für „die Aufnahme der Jungfrau Maria in den Himmel, deren Körper vor Verderbnis bewahrt wurde“ (1297 n. Chr.), aber er wurde auch einfach verwendet, um sich auf „ in einen Verein aufnehmen“ oder „in eine Partnerschaft aufnehmen“.
Bestätigungen sollten nur dann zählen, wenn sie das Ergebnis riskanter Vorhersagen sind; Das heißt, wenn wir, ohne über die betreffende Theorie informiert zu sein, ein Ereignis erwartet hätten, das mit der Theorie unvereinbar wäre – ein Ereignis, das die Theorie widerlegt hätte.
Eine Theorie, die durch kein denkbares Ereignis widerlegbar ist, ist unwissenschaftlich.
Einige wirklich überprüfbare Theorien werden, wenn sie sich als falsch herausstellen, immer noch von ihren Bewunderern aufrechterhalten – zum Beispiel indem sie post hoc (nachträglich) eine Hilfshypothese oder Annahme einführen oder die Theorie post hoc so uminterpretieren, dass sie entgeht Widerlegung.
Popper fasste diese Aussagen zusammen, indem er sagte, dass das zentrale Kriterium für den wissenschaftlichen Status einer Theorie ihre „Falfizierbarkeit, oder Widerlegbarkeit, oder Prüfbarkeit“ sei.
Mehrere Philosophen und Wissenschaftshistoriker haben jedoch argumentiert, dass Poppers Definition der Theorie als eine Reihe falsifizierbarer Aussagen falsch ist, da, wie Philip Kitcher betont hat, Beobachtungen von Uranus bei einer strikt Popperschen Sichtweise der „Theorie“ zum ersten Mal beobachtet werden müssten 1781 entdeckt, hätte Newtons Himmelsmechanik „verfälscht“.
Fruchtbarkeit: „Eine große wissenschaftliche Theorie wie die von Newton eröffnet neue Forschungsbereiche …“
Es wirft jederzeit mehr Fragen auf, als es derzeit beantworten kann.
Wie andere Definitionen von Theorien, einschließlich der von Popper, macht Kitcher deutlich, dass eine Theorie Aussagen enthalten muss, die Beobachtungskonsequenzen haben.
Sie kann als Regelsystem zu Papier gebracht werden, und je wahrer eine Theorie ist, desto vollständiger kann sie in solchen Begriffen dargelegt werden.
Die spezifischen mathematischen Aspekte der klassischen elektromagnetischen Theorie werden als „Gesetze des Elektromagnetismus“ bezeichnet und spiegeln den Grad der konsistenten und reproduzierbaren Beweise wider, die sie stützen.
Ein Beispiel für Letzteres könnte die Strahlungsreaktionskraft sein.
Ein Wissenschaftler ist eine Person, die wissenschaftliche Forschung betreibt, um das Wissen in einem Interessengebiet zu erweitern.
Wissenschaftler verschiedener Epochen (und vor ihnen Naturphilosophen, Mathematiker, Naturhistoriker, Naturtheologen, Ingenieure und andere, die zur Entwicklung der Wissenschaft beigetragen haben) hatten sehr unterschiedliche Plätze in der Gesellschaft und in den sozialen Normen, ethischen Werten und der Erkenntnistheorie Auch die mit Wissenschaftlern verbundenen und von ihnen erwarteten Tugenden haben sich im Laufe der Zeit verändert.
Viele Protowissenschaftler aus dem islamischen Goldenen Zeitalter gelten als Universalgelehrte, was zum Teil daran liegt, dass es an modernen wissenschaftlichen Disziplinen mangelt.
Rein logisch gewonnene Aussagen sind in Bezug auf die Realität völlig leer.
Descartes war nicht nur ein Pionier der analytischen Geometrie, sondern formulierte auch eine Theorie der Mechanik und erweiterte Ideen über die Ursprünge der Bewegung und Wahrnehmung von Tieren.
Er lieferte eine umfassende Formulierung der klassischen Mechanik und untersuchte Licht und Optik.
Er entdeckte, dass eine auf das Rückenmark eines Frosches ausgeübte Ladung Muskelkrämpfe im gesamten Körper hervorrufen konnte.
Lazzaro Spallanzani ist eine der einflussreichsten Persönlichkeiten der experimentellen Physiologie und der Naturwissenschaften.
Es gibt jedoch kein formelles Verfahren zur Feststellung, wer ein Wissenschaftler ist und wer nicht.
Etwas mehr als die Hälfte der Befragten wollte eine Karriere im akademischen Bereich anstreben, während kleinere Anteile auf eine Arbeit in der Industrie, der Regierung und im gemeinnützigen Umfeld hofften.
Sie zeigen eine starke Neugier gegenüber der Realität.
Einige Wissenschaftler haben den Wunsch, wissenschaftliche Erkenntnisse zum Wohle der Gesundheit der Menschen, der Nationen, der Welt, der Natur oder der Industrie anzuwenden (akademische Wissenschaftler und Industriewissenschaftler).
Dazu gehören Kosmologie und Biologie, insbesondere Molekularbiologie und das Humangenomprojekt.
Darunter waren doppelt so viele Männer wie Frauen.
Zu den relevanten Phänomenen gehören Supernova-Explosionen, Gammastrahlenausbrüche, Quasare, Blazare, Pulsare und kosmische Mikrowellen-Hintergrundstrahlung.
Die Astronomie ist eine der ältesten Naturwissenschaften.
In der Vergangenheit umfasste die Astronomie so unterschiedliche Disziplinen wie Astrometrie, Himmelsnavigation, beobachtende Astronomie und die Erstellung von Kalendern.
Die beobachtende Astronomie konzentriert sich auf die Erfassung von Daten aus Beobachtungen astronomischer Objekte.
Diese beiden Bereiche ergänzen sich.
Basierend auf strengen Wörterbuchdefinitionen bezieht sich „Astronomie“ auf „die Untersuchung von Objekten und Materie außerhalb der Erdatmosphäre und ihrer physikalischen und chemischen Eigenschaften“, während sich „Astrophysik“ auf den Zweig der Astronomie bezieht, der sich mit „dem Verhalten, den physikalischen Eigenschaften, und dynamische Prozesse von Himmelsobjekten und -phänomenen“.
Einige Bereiche, wie zum Beispiel die Astrometrie, sind reine Astronomie und nicht auch Astrophysik.
Aus diesen Beobachtungen entstanden frühe Vorstellungen über die Bewegungen der Planeten und die Natur von Sonne, Mond und Erde im Universum wurde philosophisch erforscht.
Eine besonders wichtige frühe Entwicklung war der Beginn der mathematischen und wissenschaftlichen Astronomie, die bei den Babyloniern begann, die den Grundstein für die späteren astronomischen Traditionen legten, die sich in vielen anderen Zivilisationen entwickelten.
Die griechische Astronomie zeichnete sich von Anfang an dadurch aus, dass sie nach einer rationalen, physikalischen Erklärung für Himmelsphänomene suchte.
Hipparchos erstellte außerdem einen umfassenden Katalog mit 1020 Sternen, und die meisten Sternbilder der nördlichen Hemisphäre stammen aus der griechischen Astronomie.
Georg von Peuerbach (1423–1461) und Regiomontanus (1436–1476) trugen dazu bei, dass der astronomische Fortschritt Jahrzehnte später maßgeblich zur Entwicklung des heliozentrischen Modells durch Kopernikus beitrug.
Im Jahr 964 wurde die Andromedagalaxie, die größte Galaxie der Lokalen Gruppe, vom persisch-muslimischen Astronomen Abd al-Rahman al-Sufi in seinem Buch der Fixsterne beschrieben.
Astronomen führten damals viele arabische Namen ein, die heute für einzelne Sterne verwendet werden.
Der Songhai-Historiker Mahmud Kati dokumentierte im August einen Meteoritenschauer
Kepler war der erste, der ein System entwickelte, das die Einzelheiten der Bewegung der Planeten um die Sonne korrekt beschrieb.
Der englische Astronom John Flamsteed katalogisierte über 3000 Sterne. Umfangreichere Sternkataloge wurden von Nicolas Louis de Lacaille erstellt.
Diese Arbeit wurde von Joseph-Louis Lagrange und Pierre Simon Laplace weiter verfeinert, sodass die Massen der Planeten und Monde aus ihren Störungen geschätzt werden konnten.
Es wurde nachgewiesen, dass Sterne der erdeigenen Sonne ähneln, jedoch eine große Bandbreite an Temperaturen, Massen und Größen aufweisen.
Die theoretische Astronomie führte zu Spekulationen über die Existenz von Objekten wie Schwarzen Löchern und Neutronensternen, die zur Erklärung beobachteter Phänomene wie Quasare, Pulsare, Blazare und Radiogalaxien herangezogen wurden.
Beobachtungsastronomie kann nach dem entsprechenden Bereich des elektromagnetischen Spektrums kategorisiert werden, in dem die Beobachtungen gemacht werden.
Obwohl einige Radiowellen direkt von astronomischen Objekten ausgesendet werden, ein Produkt thermischer Emission, ist der größte Teil der beobachteten Radioemission das Ergebnis von Synchrotronstrahlung, die entsteht, wenn Elektronen Magnetfelder umkreisen.
Beobachtungen mit dem Wide-Field-Infrarot-Survey-Explorer (WISE) waren besonders effektiv bei der Entdeckung zahlreicher galaktischer Protosterne und ihrer Wirtssternhaufen.
Bilder von Beobachtungen wurden ursprünglich von Hand gezeichnet.
Die Ultraviolettastronomie eignet sich am besten für die Untersuchung der Wärmestrahlung und der spektralen Emissionslinien heißer blauer Sterne (OB-Sterne), die in diesem Wellenband sehr hell sind.
Gammastrahlen können direkt von Satelliten wie dem Compton Gamma Ray Observatory oder von speziellen Teleskopen, den sogenannten atmosphärischen Cherenkov-Teleskopen, beobachtet werden.
Die Gravitationswellenastronomie ist ein aufstrebendes Gebiet der Astronomie, das Gravitationswellendetektoren einsetzt, um Beobachtungsdaten über entfernte massive Objekte zu sammeln.
Historisch gesehen war die genaue Kenntnis der Positionen von Sonne, Mond, Planeten und Sternen für die Himmelsnavigation (die Verwendung von Himmelsobjekten als Navigationshilfe) und für die Erstellung von Kalendern von entscheidender Bedeutung.
Die Messung der Sternparallaxe nahegelegener Sterne liefert eine grundlegende Basislinie in der kosmischen Entfernungsleiter, die zur Messung der Größe des Universums verwendet wird.
Analytische Modelle eines Prozesses eignen sich besser, um umfassendere Einblicke in den Kern des Geschehens zu gewähren.
Die Beobachtung eines von einem Modell vorhergesagten Phänomens ermöglicht es Astronomen, zwischen mehreren alternativen oder widersprüchlichen Modellen dasjenige auszuwählen, das das Phänomen am besten beschreiben kann.
In einigen Fällen kann eine große Menge inkonsistenter Daten im Laufe der Zeit dazu führen, dass ein Modell vollständig aufgegeben wird.
Da es sich bei der Astrophysik um ein sehr breites Fachgebiet handelt, wenden Astrophysiker typischerweise viele Disziplinen der Physik an, darunter Mechanik, Elektromagnetismus, statistische Mechanik, Thermodynamik, Quantenmechanik, Relativitätstheorie, Kern- und Teilchenphysik sowie Atom- und Molekularphysik.
Das Wort „Astrochemie“ kann sowohl auf das Sonnensystem als auch auf das interstellare Medium angewendet werden.
Der Begriff Exobiologie ist ähnlich.
Beobachtungen der großräumigen Struktur des Universums, ein Zweig, der als physikalische Kosmologie bekannt ist, haben zu einem tiefen Verständnis der Entstehung und Entwicklung des Kosmos geführt.
Aus winzigen Variationen der Massendichte des Raumes begann sich eine hierarchische Struktur der Materie zu bilden.
Gravitationsaggregate gruppierten sich zu Filamenten und hinterließen Hohlräume in den Lücken.
Verschiedene Bereiche der Physik sind für die Erforschung des Universums von entscheidender Bedeutung.
Letzteres ist schließlich wichtig für das Verständnis der großräumigen Struktur des Kosmos.
Wie der Name schon sagt, hat eine elliptische Galaxie die Querschnittsform einer Ellipse.
Elliptische Galaxien kommen häufiger im Kern von Galaxienhaufen vor und könnten durch Verschmelzungen großer Galaxien entstanden sein.
Spiralgalaxien sind typischerweise von einem Halo aus älteren Sternen umgeben.
Etwa ein Viertel aller Galaxien sind unregelmäßig, und die eigentümlichen Formen solcher Galaxien könnten das Ergebnis der Gravitationswechselwirkung sein.
Eine Radiogalaxie ist eine aktive Galaxie, die im Radiobereich des Spektrums sehr leuchtend ist und riesige Gasfahnen oder -keulen aussendet.
Die großräumige Struktur des Kosmos wird durch Galaxiengruppen und -haufen repräsentiert.
Im Zentrum der Milchstraße befindet sich der Kern, eine stabförmige Ausbuchtung, in deren Mitte sich vermutlich ein supermassereiches Schwarzes Loch befindet.
Die Scheibe ist von einem kugelförmigen Halo aus älteren Sternen der Population II sowie von relativ dichten Konzentrationen von Sternen umgeben, die als Kugelsternhaufen bekannt sind.
Diese beginnen als kompakter prästellarer Kern oder dunkle Nebel, die sich konzentrieren und kollabieren (in Volumina, die durch die Jeans-Länge bestimmt werden), um kompakte Protosterne zu bilden.
Diese Cluster zerstreuen sich allmählich und die Sterne schließen sich der Population der Milchstraße an.
7–18 Die Sternentstehung findet in dichten Regionen aus Staub und Gas statt, die als riesige Molekülwolken bekannt sind.
Fast alle Elemente, die schwerer als Wasserstoff und Helium sind, entstanden im Inneren von Sternen.
Mit der Zeit wird dieser Wasserstoffbrennstoff vollständig in Helium umgewandelt und der Stern beginnt sich zu entwickeln.
Durch den Auswurf der äußeren Schichten entsteht ein planetarischer Nebel.
Dies ist eine 11-jährige Schwankung der Sonnenfleckenzahl.
Auch die Sonne hat periodische Veränderungen der Leuchtkraft erfahren, die erhebliche Auswirkungen auf die Erde haben können.
Über dieser Schicht befindet sich ein dünner Bereich, der als Chromosphäre bekannt ist.
Oberhalb des Kerns befindet sich die Strahlungszone, in der das Plasma mittels Strahlung den Energiefluss transportiert.
Ein Sonnenwind aus Plasmateilchen strömt ständig von der Sonne nach außen, bis er an der äußersten Grenze des Sonnensystems die Heliopause erreicht.
Die Planeten entstanden vor 4,6 Milliarden Jahren in der protoplanetaren Scheibe, die die frühe Sonne umgab.
Die Planeten fegten oder schleuderten die verbleibende Materie während einer Phase intensiven Bombardements weiter mit, wie die vielen Einschlagskrater auf dem Mond belegen.
Durch diesen Prozess kann ein steiniger oder metallischer Kern entstehen, der von einem Mantel und einer äußeren Kruste umgeben ist.
Einige Planeten und Monde speichern genug Wärme, um geologische Prozesse wie Vulkanismus und Tektonik voranzutreiben.
Astrostatistik ist die Anwendung von Statistiken auf die Astrophysik zur Analyse einer großen Menge astrophysikalischer Beobachtungsdaten.
Unter Kosmochemie versteht man die Untersuchung der im Sonnensystem vorkommenden Chemikalien, einschließlich der Herkunft der Elemente und Variationen in den Isotopenverhältnissen.
Astronomieclubs gibt es auf der ganzen Welt und viele verfügen über Programme, die ihren Mitgliedern bei der Einrichtung und Durchführung von Beobachtungsprogrammen helfen, darunter solche zur Beobachtung aller Objekte im Messier- (110 Objekte) oder Herschel 400-Katalog mit interessanten Punkten am Nachthimmel.
Die meisten Amateure arbeiten bei sichtbaren Wellenlängen, aber eine kleine Minderheit experimentiert mit Wellenlängen außerhalb des sichtbaren Spektrums.
Eine Reihe von Amateurastronomen verwenden entweder selbstgebaute Teleskope oder Radioteleskope, die ursprünglich für die astronomische Forschung gebaut wurden, jetzt aber auch für Amateure erhältlich sind (z. B. das One-Mile-Teleskop).
Antworten auf diese Fragen erfordern möglicherweise den Bau neuer boden- und weltraumgestützter Instrumente und möglicherweise neue Entwicklungen in der theoretischen und experimentellen Physik.
Ein tieferes Verständnis der Entstehung von Sternen und Planeten ist erforderlich.
Wenn ja, was ist die Erklärung für das Fermi-Paradoxon?
Was ist die Natur von dunkler Materie und dunkler Energie?
Wie entstanden die ersten Galaxien?
Die Astrobiologie, früher als Exobiologie bekannt, ist ein interdisziplinäres wissenschaftliches Gebiet, das den Ursprung, die frühe Entwicklung, die Verbreitung und die Zukunft des Lebens im Universum untersucht.
Der Ursprung und die frühe Entwicklung des Lebens sind ein untrennbarer Bestandteil der Disziplin Astrobiologie.
Die Biochemie könnte kurz nach dem Urknall vor 13,8 Milliarden Jahren begonnen haben, in einer bewohnbaren Epoche, als das Universum erst 10–17 Millionen Jahre alt war.
Dennoch ist die Erde der einzige Ort im Universum, von dem Menschen wissen, dass er Leben beherbergt.
Der Begriff Exobiologie wurde vom Molekularbiologen und Nobelpreisträger Joshua Lederberg geprägt.
Der Begriff Xenobiologie wird heute in einem spezielleren Sinne verwendet und bedeutet „Biologie, die auf fremder Chemie basiert“, sei es außerirdischen oder terrestrischen (möglicherweise synthetischen) Ursprungs.
Obwohl die Astrobiologie einst außerhalb des Mainstreams der wissenschaftlichen Forschung betrachtet wurde, ist sie zu einem formalisierten Studienfach geworden.
1959 finanzierte die NASA ihr erstes Exobiologieprojekt und 1960 gründete die NASA ein Exobiologieprogramm, das heute eines von vier Hauptelementen des aktuellen Astrobiologieprogramms der NASA ist.
Fortschritte in den Bereichen Astrobiologie, beobachtende Astronomie und die Entdeckung einer großen Vielfalt von Extremophilen mit außergewöhnlicher Fähigkeit, in den rauesten Umgebungen der Erde zu gedeihen, haben zu Spekulationen geführt, dass möglicherweise auf vielen außerirdischen Körpern im Universum Leben gedeiht.
Missionen, die speziell auf die Suche nach aktuellem Leben auf dem Mars ausgerichtet waren, waren das Viking-Programm und die Sonden Beagle 2.
Ende 2008 untersuchte der Phoenix-Lander die Umwelt auf frühere und gegenwärtige planetarische Bewohnbarkeit von mikrobiellem Leben auf dem Mars und erforschte die Geschichte des Wassers dort.
Im November 2011 startete die NASA die Mission Mars Science Laboratory mit dem Rover Curiosity, der im August 2012 am Gale-Krater auf dem Mars landete.
Eine davon ist die fundierte Annahme, dass die überwiegende Mehrheit der Lebensformen in unserer Galaxie, wie alle Lebensformen auf der Erde, auf der Kohlenstoffchemie basieren.
Die Tatsache, dass Kohlenstoffatome sich leicht an andere Kohlenstoffatome binden, ermöglicht den Aufbau extrem langer und komplexer Moleküle.
Eine dritte Annahme besteht darin, sich auf Planeten zu konzentrieren, die sonnenähnliche Sterne umkreisen, um die Wahrscheinlichkeit der Bewohnbarkeit von Planeten zu erhöhen.
Zu diesem Zweck wurde eine Reihe von Instrumenten zur Erkennung erdgroßer Exoplaneten in Betracht gezogen, insbesondere der Terrestrial Planet Finder (TPF) der NASA und die Darwin-Programme der ESA, die beide eingestellt wurden.
Drake formulierte die Gleichung ursprünglich lediglich als Agenda für die Diskussion auf der Green-Bank-Konferenz, einige Anwendungen der Formel wurden jedoch wörtlich genommen und auf vereinfachende oder pseudowissenschaftliche Argumente bezogen.
Die Entdeckung von Extremophilen, Organismen, die in extremen Umgebungen überleben können, wurde zu einem zentralen Forschungselement für Astrobiologen, da sie wichtig sind, um vier Bereiche innerhalb der Grenzen des Lebens im planetaren Kontext zu verstehen: das Potenzial für Panspermie, Vorwärtskontamination aufgrund menschlicher Erkundungsvorhaben , die Besiedlung des Planeten durch Menschen und die Erforschung ausgestorbenen und noch existierenden außerirdischen Lebens.
Es wurde angenommen, dass sogar das Leben in den Tiefen des Ozeans, wo das Sonnenlicht nicht hinkommt, seine Nahrung entweder durch den Verzehr von organischem Abfall, der von den Oberflächengewässern herabregnet, oder durch den Verzehr von Tieren erhält, die dies tun.
Diese Chemosynthese revolutionierte das Studium der Biologie und Astrobiologie, indem sie enthüllte, dass das Leben nicht unbedingt von der Sonne abhängig sein muss; Es benötigt nur Wasser und einen Energiegradienten, um zu existieren.
Zehn robuste Organismen, die von Amir Alexander für das LIFE-Projekt ausgewählt wurden: Deinococcus radiodurans, Bacillus subtilis, die Hefe Saccharomyces cerevisiae, Samen von Arabidopsis thaliana („Mausohrkresse“) sowie das wirbellose Tier Bärtierchen.
Der Jupitermond Europa und der Saturnmond Enceladus gelten heute aufgrund ihrer unterirdischen Wasserozeane, in denen radiogene Erwärmung und Gezeitenerwärmung die Existenz von flüssigem Wasser ermöglichen, als wahrscheinlichste Standorte für noch vorhandenes außerirdisches Leben im Sonnensystem.
Der kosmische Staub, der das Universum durchdringt, enthält komplexe organische Verbindungen („amorphe organische Feststoffe mit gemischter aromatisch-aliphatischer Struktur“), die auf natürliche Weise und schnell von Sternen erzeugt werden könnten.
PAKs scheinen kurz nach dem Urknall entstanden zu sein, sind im gesamten Universum verbreitet und werden mit neuen Sternen und Exoplaneten in Verbindung gebracht.
Die experimentelle Astroökologie untersucht Ressourcen in Planetenböden und nutzt dazu tatsächliches Weltraummaterial in Meteoriten.
Im weitesten Sinne befasst sich die Kosmoökologie mit dem Leben im Universum über kosmologische Zeiträume hinweg.
Zu den Spezialisierungen gehören Kosmochemie, Biochemie und organische Geochemie.
Einige Regionen auf der Erde, wie etwa die Pilbara-Region in Westaustralien und die McMurdo-Trockentäler in der Antarktis, gelten ebenfalls als geologische Analogien zu Regionen auf dem Mars und könnten als solche Hinweise darauf geben, wie man dort nach vergangenem Leben suchen kann Mars.
Tatsächlich ist es wahrscheinlich, dass die Grundbausteine des Lebens überall denen auf der Erde ähneln, wenn auch nicht im Detail, im Großen und Ganzen.
Es ist bekannt, dass nur zwei der natürlichen Atome, Kohlenstoff und Silizium, als Rückgrat von Molekülen dienen, die groß genug sind, um biologische Informationen zu transportieren.
Die vier wahrscheinlichsten Kandidaten für Leben im Sonnensystem sind der Planet Mars, der Jupitermond Europa und die Saturnmonde Titan und Enceladus.
Bei niedrigen Temperaturen und niedrigem Druck auf dem Mars ist flüssiges Wasser wahrscheinlich stark salzhaltig.
Am 11. Dezember 2013 meldete die NASA die Entdeckung „tonartiger Mineralien“ (insbesondere Schichtsilikate), die häufig mit organischen Materialien in Verbindung stehen, auf der eisigen Kruste Europas.
Einige Wissenschaftler halten es für möglich, dass diese flüssigen Kohlenwasserstoffe in anderen lebenden Zellen als denen auf der Erde den Platz von Wasser einnehmen könnten.
Es sind keine abiotischen Prozesse auf dem Planeten bekannt, die seine Präsenz verursachen könnten.
Yamato 000593, der zweitgrößte Meteorit vom Mars, wurde im Jahr 2000 auf der Erde gefunden.
Am 5. März 2011 spekulierte Richard B. Hoover, ein Wissenschaftler am Marshall Space Flight Center, über den Fund angeblicher Mikrofossilien, die Cyanobakterien in kohlenstoffhaltigen CI1-Meteoriten ähneln, im Randjournal Journal of Cosmology, eine Geschichte, über die in den Mainstream-Medien ausführlich berichtet wurde.
Im gesamten Sonnensystem und insbesondere auf dem Mars wurden Hinweise auf Perchlorate gefunden.
Verbesserte Entdeckungsmethoden und eine längere Beobachtungszeit werden zweifellos mehr Planetensysteme entdecken, und möglicherweise noch mehr wie unseres.
Ziel ist es, Organismen aufzuspüren, die in der Lage sind, die Bedingungen der Raumfahrt zu überleben und ihre Vermehrungskapazität aufrechtzuerhalten.
Diese Stressreaktionen könnten es ihnen auch ermöglichen, unter rauen Weltraumbedingungen zu überleben, obwohl die Evolution ihrer Verwendung als Analoga zu außerirdischem Leben auch einige Einschränkungen auferlegt.
Die Bildung von Sporen ermöglicht es ihm, extreme Umgebungen zu überstehen und gleichzeitig das Zellwachstum wieder in Gang zu bringen.
Da die beiden Lander identisch waren, wurden die gleichen Tests an zwei Orten auf der Marsoberfläche durchgeführt; Viking 1 in der Nähe des Äquators und Viking 2 weiter nördlich.
In der Astronomie ist Extinktion die Absorption und Streuung elektromagnetischer Strahlung durch Staub und Gas zwischen einem emittierenden astronomischen Objekt und dem Beobachter.
Bei Sternen, die in der Nähe der Ebene der Milchstraße liegen und nur wenige tausend Parsec von der Erde entfernt sind, beträgt die Auslöschung im visuellen Frequenzband (photometrisches System) etwa 1,8 Magnituden pro Kiloparsec.
Die Rötung entsteht durch die Lichtstreuung an Staub und anderen Stoffen im interstellaren Medium.
In den meisten photometrischen Systemen werden Filter (Durchlassbänder) verwendet, mit deren Hilfe die Lichtstärke unter Berücksichtigung der Breitengrade und der Luftfeuchtigkeit unter den terrestrischen Faktoren ermittelt werden kann.
Im Großen und Ganzen ist die interstellare Extinktion bei kurzen Wellenlängen am stärksten, was im Allgemeinen mit spektroskopischen Techniken beobachtet wird.
Das Ausmaß der Auslöschung kann in bestimmten Richtungen deutlich höher sein.
Daher kann es bei der Berechnung kosmischer Entfernungen von Vorteil sein, auf Sterndaten aus dem nahen Infrarotbereich umzusteigen (dessen Filter oder Durchlassband Ks ganz normal ist), wo die Schwankungen und das Ausmaß der Extinktion deutlich geringer sind und ähnliche Verhältnisse bestehen R(Ks): 0,49 ± 0,02 bzw. 0,528 ± 0,015 wurden von unabhängigen Gruppen gefunden.
Dieses Merkmal wurde erstmals in den 1960er Jahren beobachtet, sein Ursprung ist jedoch immer noch nicht genau geklärt.
In der SMC sind extremere Schwankungen zu beobachten, mit keinem 2175 Å und einer sehr starken Fern-UV-Extinktion im Sternentstehungsbalken und einer ziemlich normalen Ultraviolett-Extinktion im ruhigeren Flügel.
Das Auffinden von Extinktionskurven sowohl im LMC als auch im SMC, die denen in der Milchstraße ähneln, und das Auffinden von Extinktionskurven in der Milchstraße, die eher denen in der LMC2-Superschale des LMC und im SMC-Balken ähneln, hat zu einem Ergebnis geführt neue Interpretation.
Dieses Aussterben besteht aus drei Hauptkomponenten: Rayleigh-Streuung durch Luftmoleküle, Streuung durch Partikel und molekulare Absorption.
Das Ausmaß dieser Auslöschung ist im Zenit des Beobachters am niedrigsten und in der Nähe des Horizonts am höchsten.
Die Drake-Gleichung spekuliert über die Existenz intelligenten Lebens anderswo im Universum.
Dies umfasst eine Suche nach aktuellem und historischem außerirdischem Leben und eine engere Suche nach außerirdischem intelligentem Leben.
Im Laufe der Jahre vermittelte Science-Fiction wissenschaftliche Ideen, stellte sich ein breites Spektrum an Möglichkeiten vor und beeinflusste das öffentliche Interesse und die Perspektiven des außerirdischen Lebens.
Nach diesem Argument von Wissenschaftlern wie Carl Sagan und Stephen Hawking sowie namhaften Persönlichkeiten wie Winston Churchill wäre es unwahrscheinlich, dass Leben nicht irgendwo anders als auf der Erde existiert.
Das Leben könnte an vielen Orten im Universum unabhängig voneinander entstanden sein.
Auf jeder Ebene des Organismus gibt es Mechanismen, um Konflikte zu beseitigen, die Zusammenarbeit aufrechtzuerhalten und die Funktionsfähigkeit des Organismus aufrechtzuerhalten.
Als Alternative wurde Leben auf der Basis von Ammoniak (anstelle von Wasser) vorgeschlagen, obwohl dieses Lösungsmittel weniger geeignet zu sein scheint als Wasser.
Etwa 95 % der lebenden Materie bestehen aus nur sechs Elementen: Kohlenstoff, Wasserstoff, Stickstoff, Sauerstoff, Phosphor und Schwefel.
Das Kohlenstoffatom hat die einzigartige Fähigkeit, vier starke chemische Bindungen mit anderen Atomen, einschließlich anderen Kohlenstoffatomen, einzugehen.
Laut der Astrobiologie-Strategie der NASA aus dem Jahr 2015 „besteht das Leben auf anderen Welten höchstwahrscheinlich aus Mikroben, und jedes komplexe lebende System anderswo ist wahrscheinlich aus mikrobiellem Leben entstanden und basiert darauf.“
Rick Colwell, ein Mitglied des Deep Carbon Observatory-Teams der Oregon State University, sagte gegenüber der BBC: „Ich denke, es ist wahrscheinlich vernünftig anzunehmen, dass der Untergrund anderer Planeten und ihrer Monde bewohnbar ist, insbesondere weil wir das hier auf der Erde gesehen haben.“ Organismen können fernab des Sonnenlichts funktionieren, indem sie die Energie nutzen, die direkt aus den Gesteinen tief unter der Erde bereitgestellt wird.“
Die Panspermie-Hypothese legt nahe, dass Leben anderswo im Sonnensystem einen gemeinsamen Ursprung haben könnte.
Im 19. Jahrhundert wurde es in moderner Form von mehreren Wissenschaftlern wiederbelebt, darunter Jöns Jacob Berzelius (1834), Kelvin (1871), Hermann von Helmholtz (1879) und etwas später von Svante Arrhenius (1903).
Eine der ersten wissenschaftlichen Untersuchungen zu diesem Thema erschien 1878 in einem Artikel des Scientific American mit dem Titel „Ist der Mond bewohnt?“
Warme und unter Druck stehende Regionen im Mondinneren könnten noch flüssiges Wasser enthalten.
Es gibt Hinweise darauf, dass der Mars eine wärmere und feuchtere Vergangenheit hatte: Es wurden ausgetrocknete Flussbetten, Polkappen, Vulkane und Mineralien gefunden, die sich in der Gegenwart von Wasser bilden.
Der Dampf könnte durch Eisvulkane oder durch die Sublimierung (Umwandlung von fest in gasförmig) von Eis nahe der Oberfläche entstanden sein.
Es ist auch möglich, dass Europa die aerobe Makrofauna mithilfe von Sauerstoff unterstützen könnte, der durch kosmische Strahlung erzeugt wird, die auf das Oberflächeneis trifft.
Am 11. Dezember 2013 meldete die NASA die Entdeckung „tonartiger Mineralien“ (insbesondere Schichtsilikate), die häufig mit organischen Materialien in Verbindung stehen, auf der eisigen Kruste Europas.
Einige behaupten, Beweise dafür gefunden zu haben, dass auf dem Mars mikrobielles Leben existiert hat.
Im Jahr 1996 hieß es in einem kontroversen Bericht, dass Strukturen, die Nanobakterien ähneln, in einem Meteoriten, ALH84001, entdeckt wurden, der aus vom Mars ausgeworfenem Gestein gebildet wurde.
NASA-Beamte distanzierten die NASA bald von den Behauptungen der Wissenschaftler, und Stoker selbst wich von ihren ursprünglichen Behauptungen ab.
Ziel ist es, mithilfe verschiedener wissenschaftlicher Instrumente die frühere und gegenwärtige Bewohnbarkeit des Mars zu beurteilen.
Allerdings sind erhebliche Fortschritte bei der Fähigkeit erforderlich, Licht von kleineren Gesteinswelten in der Nähe ihrer Sterne zu finden und aufzulösen, bevor solche spektroskopischen Methoden zur Analyse extrasolarer Planeten eingesetzt werden können.
Im August 2011 legten Ergebnisse der NASA, die auf Untersuchungen von auf der Erde gefundenen Meteoriten basieren, nahe, dass DNA- und RNA-Komponenten (Adenin, Guanin und verwandte organische Moleküle), Bausteine für das Leben, wie wir es kennen, außerirdisch im Weltraum gebildet werden könnten.
Im August 2012 berichteten Astronomen der Universität Kopenhagen erstmals weltweit über die Entdeckung eines bestimmten Zuckermoleküls, Glykolaldehyd, in einem entfernten Sternensystem.
Das Kepler-Weltraumteleskop hat auch einige tausend Kandidatenplaneten entdeckt, von denen etwa 11 % falsch positive Ergebnisse sein könnten.
Der massereichste Planet, der im Exoplanetenarchiv der NASA aufgeführt ist, ist DENIS-P J082303.1-491201 b, etwa 29-mal so groß wie Jupiter, obwohl er nach den meisten Definitionen eines Planeten zu massereich ist, um ein Planet zu sein, und möglicherweise ein Planet ist stattdessen Brauner Zwerg.
Ein Zeichen dafür, dass ein Planet wahrscheinlich bereits Leben enthält, ist das Vorhandensein einer Atmosphäre mit erheblichen Mengen an Sauerstoff, da dieses Gas sehr reaktiv ist und ohne ständigen Nachschub im Allgemeinen nicht lange überleben würde.
Selbst wenn man davon ausgeht, dass nur einer von einer Milliarde dieser Sterne Planeten hat, auf denen Leben möglich ist, gäbe es im beobachtbaren Universum etwa 6,25 Milliarden lebenserhaltende Planetensysteme.
Die früheste aufgezeichnete Behauptung außerirdischen menschlichen Lebens findet sich in alten Schriften des Jainismus.
Mittelalterliche muslimische Schriftsteller wie Fakhr al-Din al-Razi und Muhammad al-Baqir unterstützten den kosmischen Pluralismus auf der Grundlage des Korans.
Als klar wurde, dass die Erde lediglich ein Planet unter unzähligen Körpern im Universum war, begann die Theorie des außerirdischen Lebens zu einem Thema in der wissenschaftlichen Gemeinschaft zu werden.
Die Möglichkeit von Außerirdischen blieb eine weit verbreitete Spekulation, da die wissenschaftlichen Entdeckungen immer schneller vorankamen.
Die Idee von Leben auf dem Mars veranlasste den britischen Schriftsteller H. G. Wells 1897, den Roman „Der Krieg der Welten“ zu schreiben, in dem er von einer Invasion außerirdischer Wesen vom Mars erzählte, die vor der Austrocknung des Planeten flohen.
Der Glaube an außerirdische Wesen wird weiterhin in Pseudowissenschaften, Verschwörungstheorien und populärer Folklore, insbesondere „Area 51“ und Legenden, geäußert.
Ward und Brownlee sind offen für die Idee einer Evolution auf anderen Planeten, die nicht auf wesentlichen erdähnlichen Eigenschaften (wie DNA und Kohlenstoff) basiert.
Wenn Außerirdische uns besuchen würden, wäre das Ergebnis ähnlich wie bei der Landung von Kolumbus in Amerika, was für die amerikanischen Ureinwohner nicht gut ausging“, sagte er.
COSPAR stellt auch Richtlinien für den Schutz des Planeten bereit.
Außerdem gebe es laut Antwort „keine glaubwürdigen Informationen, die darauf hindeuten, dass irgendwelche Beweise vor der Öffentlichkeit verborgen bleiben“.
Oben: Lichtquellen unterschiedlicher Stärke.
Komet Borrelly, die Farben zeigen seine Helligkeit im Bereich von drei Größenordnungen (rechts).
Die Skala ist logarithmisch und so definiert, dass jeder Schritt einer Größenordnung die Helligkeit um den Faktor der fünften Wurzel von 100 oder etwa 2,512 ändert.
Astronomen verwenden zwei unterschiedliche Definitionen der Helligkeit: scheinbare Helligkeit und absolute Helligkeit.
Die absolute Helligkeit beschreibt die intrinsische Leuchtkraft, die ein Objekt aussendet, und ist definiert als gleich der scheinbaren Helligkeit, die das Objekt hätte, wenn es sich in einer bestimmten Entfernung von der Erde befinden würde, 10 Parsec für Sterne.
Die Entwicklung des Teleskops zeigte, dass diese großen Größen illusorisch waren – Sterne erschienen durch das Teleskop viel kleiner.
Je negativer der Wert, desto heller ist das Objekt.
Sterne mit einer Helligkeit zwischen 1,5 und 2,5 werden als Sterne zweiter Größe bezeichnet. Es gibt etwa 20 Sterne, die heller als 1,5 sind und Sterne erster Größe sind (siehe Liste der hellsten Sterne).
Absolute Helligkeiten für Objekte im Sonnensystem werden häufig auf der Grundlage einer Entfernung von 1 AE angegeben.
Die einfachste Form der Technologie ist die Entwicklung und Nutzung grundlegender Werkzeuge.
Es hat zur Entwicklung fortgeschrittenerer Volkswirtschaften (einschließlich der heutigen Weltwirtschaft) beigetragen und den Aufstieg einer Freizeitschicht ermöglicht.
Beispiele hierfür sind der Aufstieg des Begriffs der Effizienz im Hinblick auf die menschliche Produktivität und die Herausforderungen der Bioethik.
Die Bedeutung des Begriffs änderte sich im frühen 20. Jahrhundert, als amerikanische Sozialwissenschaftler, beginnend mit Thorstein Veblen, Ideen aus dem deutschen Konzept der Technik in „Technologie“ übersetzten.
Im Jahr 1937 schrieb der amerikanische Soziologe Read Bain: „Technologie umfasst alle Werkzeuge, Maschinen, Utensilien, Waffen, Instrumente, Wohnräume, Kleidung, Kommunikations- und Transportgeräte sowie die Fähigkeiten, mit denen wir sie herstellen und nutzen.“
In jüngerer Zeit haben Wissenschaftler Anleihen bei europäischen Philosophen der „Technik“ genommen, um die Bedeutung der Technologie auf verschiedene Formen der instrumentellen Vernunft auszudehnen, wie in Foucaults Werk über Technologien des Selbst (techniques de soi).
nützliche Dinge erfinden oder Probleme lösen“ und „eine Maschine, ein Gerät, eine Methode usw.“,
Der Begriff wird häufig verwendet, um ein bestimmtes Technologiefeld zu bezeichnen oder um sich auf Hochtechnologie oder nur Unterhaltungselektronik und nicht auf die Technologie als Ganzes zu beziehen.
In diesem Sinne bezieht sich Technologie auf Werkzeuge und Maschinen, die zur Lösung realer Probleme eingesetzt werden können.
W. Brian Arthur definiert Technologie ähnlich weit gefasst als „ein Mittel zur Erfüllung eines menschlichen Zwecks“.
In Kombination mit einem anderen Begriff, etwa „Medizintechnik“ oder „Weltraumtechnik“, bezieht es sich auf den Wissens- und Werkzeugstand des jeweiligen Fachgebiets. "
Darüber hinaus ist Technologie die Anwendung von Mathematik, Naturwissenschaften und Künsten zum Wohle des sogenannten Lebens.
Ingenieurwesen ist der zielorientierte Prozess des Entwurfs und der Herstellung von Werkzeugen und Systemen zur Nutzung natürlicher Phänomene für praktische menschliche Zwecke, wobei häufig (aber nicht immer) Ergebnisse und Techniken aus der Wissenschaft genutzt werden.
Beispielsweise könnte die Wissenschaft den Elektronenfluss in elektrischen Leitern mithilfe bereits vorhandener Werkzeuge und Kenntnisse untersuchen.
Insbesondere die genauen Beziehungen zwischen Wissenschaft und Technologie wurden im späten 20. Jahrhundert von Wissenschaftlern, Historikern und politischen Entscheidungsträgern diskutiert, auch weil die Debatte Einfluss auf die Finanzierung der Grundlagen- und angewandten Wissenschaft haben kann.
Der frühe Mensch entwickelte sich aus einer Art futtersuchender Hominiden, die bereits zweibeinig waren und eine Gehirnmasse von etwa einem Drittel der des modernen Menschen aufwiesen.
Die Erfindung polierter Steinäxte war ein großer Fortschritt, der die großflächige Rodung von Wäldern zur Schaffung von Bauernhöfen ermöglichte.
Die früheste bekannte Nutzung der Windkraft ist das Segelschiff; Die früheste Erwähnung eines Schiffes unter Segeln stammt aus dem 8. Jahrtausend v. Chr. und stammt von einem Nilboot.
Archäologen zufolge wurde das Rad um 4000 v. Chr. vermutlich unabhängig voneinander und nahezu gleichzeitig in Mesopotamien (im heutigen Irak), im Nordkaukasus (Maykop-Kultur) und in Mitteleuropa erfunden.
Vor kurzem wurde das älteste bekannte Holzrad der Welt in den Sümpfen von Ljubljana in Slowenien gefunden.
Die alten Sumerer nutzten die Töpferscheibe und haben sie möglicherweise erfunden.
Die ersten zweirädrigen Karren wurden von Travois abgeleitet und erstmals um 3000 v. Chr. in Mesopotamien und im Iran eingesetzt.
Im Palast von Knossos wurde eine Badewanne ausgegraben, die mit modernen praktisch identisch ist.
Der wichtigste Abwasserkanal in Rom war die Cloaca Maxima; Der Bau begann im sechsten Jahrhundert v. Chr. und es wird noch heute genutzt.
In der mittelalterlichen Technologie wurden einfache Maschinen (wie der Hebel, die Schraube und die Riemenscheibe) zu komplizierteren Werkzeugen wie der Schubkarre, Windmühlen und Uhren kombiniert, und ein System von Universitäten entwickelte und verbreitete wissenschaftliche Ideen und Praktiken .
Die Industrielle Revolution begann im Vereinigten Königreich im 18. Jahrhundert und war eine Zeit großer technologischer Entdeckungen, insbesondere in den Bereichen Landwirtschaft, Fertigung, Bergbau, Metallurgie und Transportwesen, vorangetrieben durch die Entdeckung der Dampfkraft und die weitverbreitete Anwendung der Dampfkraft Fabriksystem.
Der Fortschritt der Technologie hat zu Wolkenkratzern und weitläufigen Stadtgebieten geführt, deren Bewohner auf Motoren angewiesen sind, um sich und ihre Lebensmittelvorräte zu transportieren.
Das 20. Jahrhundert brachte eine Vielzahl von Innovationen.
Die Informationstechnologie führte in den 1980er Jahren zur Geburt des Internets, das das heutige Informationszeitalter einläutete.
Um einige der neueren Technologien herzustellen und zu warten, sind komplexe Fertigungs- und Konstruktionstechniken und -organisationen erforderlich, und ganze Industrien sind entstanden, um nachfolgende Generationen immer komplexerer Werkzeuge zu unterstützen und zu entwickeln.
Transhumanisten glauben im Allgemeinen, dass der Zweck der Technologie darin besteht, Barrieren zu überwinden, und dass das, was wir gemeinhin als die menschliche Verfassung bezeichnen, nur eine weitere Barriere ist, die es zu überwinden gilt.
Sie weisen darauf hin, dass das unvermeidliche Ergebnis einer solchen Gesellschaft darin besteht, dass sie auf Kosten der Freiheit und der psychischen Gesundheit immer technologischer wird.
Er hofft, das Wesen der Technologie auf eine Weise zu enthüllen, die „uns keineswegs in den verdummten Zwang einschränkt, blind mit der Technologie weiterzumachen oder, was auf das Gleiche hinausläuft, hilflos dagegen zu rebellieren“.
Einige der ergreifendsten Kritikpunkte an der Technologie finden sich in heute als dystopisch geltenden Literaturklassikern wie Aldous Huxleys „Brave New World“, Anthony Burgess‘ „A Clockwork Orange“ und George Orwells „1984“.
Der verstorbene Kulturkritiker Neil Postman unterschied Werkzeug nutzende Gesellschaften von technologischen Gesellschaften und von dem, was er „Technopole“ nannte, Gesellschaften, die von der Ideologie des technologischen und wissenschaftlichen Fortschritts dominiert werden und andere kulturelle Praktiken, Werte und Weltkulturen ausschließen oder schädigen. Ansichten.
Nikolas Kompridis hat auch über die Gefahren neuer Technologien wie Gentechnik, Nanotechnologie, synthetische Biologie und Robotik geschrieben.
Ein weiterer prominenter Technologiekritiker ist Hubert Dreyfus, der Bücher wie „On the Internet“ und „What Computers Still Can't Do“ veröffentlicht hat.
In seinem Artikel stellt Jared Bernstein, Senior Fellow am Center on Budget and Policy Priorities, die weit verbreitete Vorstellung in Frage, dass die Automatisierung und im weiteren Sinne der technologische Fortschritt hauptsächlich zu diesem wachsenden Arbeitsmarktproblem beigetragen haben.
Zur Verteidigung seines Standpunkts führt er zwei Hauptargumente an.
In der Tat bedroht die Automatisierung sich wiederholende Tätigkeiten, aber hochwertigere Tätigkeiten sind weiterhin notwendig, da sie die Technologie ergänzen und manuelle Tätigkeiten, die „Flexibilität, Urteilsvermögen und gesunden Menschenverstand erfordern“, nach wie vor schwer durch Maschinen zu ersetzen sind.
Technologie wird oft zu eng betrachtet; Laut Hughes ist „Technologie ein kreativer Prozess, der menschlichen Einfallsreichtum erfordert“.
Sie haben oft angenommen, dass Technologie leicht kontrollierbar sei, und diese Annahme muss gründlich in Frage gestellt werden.
Solutionismus ist die Ideologie, dass jedes gesellschaftliche Problem dank Technologie und insbesondere dank des Internets gelöst werden kann.
Auch Benjamin R. Cohen und Gwen Ottinger diskutierten die multivalenten Auswirkungen von Technologie.
Der Einsatz grundlegender Technologie ist neben dem Menschen auch ein Merkmal anderer Tierarten.
Die Fähigkeit, Werkzeuge herzustellen und zu verwenden, galt einst als charakteristisches Merkmal der Gattung Homo.
Im Jahr 2005 sagte der Zukunftsforscher Ray Kurzweil voraus, dass die Zukunft der Technologie hauptsächlich aus einer sich überschneidenden „GNR-Revolution“ aus Genetik, Nanotechnologie und Robotik bestehen würde, wobei die Robotik die wichtigste der drei sein würde.
Die Menschheit hat bereits einige der ersten Schritte zur Verwirklichung der GNR-Revolution gemacht.
Einige glauben, dass die Zukunft der Robotik eine „größere als die menschliche nichtbiologische Intelligenz“ beinhalten wird.
Diese Zukunft hat viele Ähnlichkeiten mit dem Konzept der geplanten Obsoleszenz, allerdings wird geplante Obsoleszenz als „finstere Geschäftsstrategie“ angesehen.
Auch die Genetik wurde erforscht, und der Mensch versteht die Gentechnik bis zu einem gewissen Grad.
Andere glauben, dass die Gentechnik eingesetzt werden soll, um den Menschen resistenter oder völlig immun gegen bestimmte Krankheiten zu machen.
Zukunftsforscher gehen davon aus, dass die Nanobot-Technologie es Menschen ermöglichen wird, „Materie auf molekularer und atomarer Ebene zu manipulieren“.
In diesem inzwischen veralteten Zusammenhang bezog sich ein „Motor“ auf eine militärische Maschine, d. h. ein mechanisches Gerät, das im Krieg eingesetzt wurde (z. B. ein Katapult).
Die sechs klassischen einfachen Maschinen waren im alten Nahen Osten bekannt.
Der Hebelmechanismus tauchte erstmals vor etwa 5.000 Jahren im Nahen Osten auf, wo er in einer einfachen Waage und in der altägyptischen Technologie zum Bewegen großer Objekte verwendet wurde.
Die Schraube, die letzte der einfachen Maschinen, die erfunden wurde, tauchte erstmals in Mesopotamien während der neuassyrischen Zeit (911-609) v. Chr. auf.
Als einer der Beamten des Pharaos Djosèr entwarf und überwachte er wahrscheinlich den Bau der Djoser-Pyramide (Stufenpyramide) in Sakkara in Ägypten um 2630–2611 v. Chr.
Kuschitische Vorfahren bauten während der Bronzezeit zwischen 3700 und 3250 v. Chr. Speos. Im 7. Jahrhundert v. Chr. wurden in Kusch auch Bäckereien und Hochöfen errichtet.
Einige der Erfindungen von Archimedes sowie der Antikythera-Mechanismus erforderten ausgefeilte Kenntnisse über Differentialgetriebe oder Umlaufrädergetriebe, zwei Schlüsselprinzipien der Maschinentheorie, die bei der Konstruktion der Räderwerke der industriellen Revolution halfen und auch heute noch in verschiedenen Bereichen wie der Robotik weit verbreitet sind und Automobiltechnik.
Das Spinnrad war auch ein Vorläufer der Spinning Jenny, einer Schlüsselentwicklung während der frühen industriellen Revolution im 18. Jahrhundert.
Er beschrieb vier Automatenmusiker, darunter Schlagzeuger, die von einer programmierbaren Trommelmaschine gesteuert wurden und mit der sie unterschiedliche Rhythmen und unterschiedliche Trommelmuster spielen konnten.
Abgesehen von diesen Berufen wurde angenommen, dass Universitäten keine große praktische Bedeutung für die Technologie hatten.
Der Kanalbau war in den frühen Phasen der Industriellen Revolution eine wichtige Ingenieursarbeit.
Er war außerdem ein fähiger Maschinenbauingenieur und ein bedeutender Physiker.
Smeaton nahm auch mechanische Verbesserungen an der Newcomen-Dampfmaschine vor.
Samuel Morland, ein Mathematiker und Erfinder, der sich mit Pumpen beschäftigte, hinterließ im Vauxhall Ordinance Office Notizen zu einem Dampfpumpenentwurf, den Thomas Savery las.
Es war nicht bekannt, dass der Eisenhändler Thomas Newcomen, der 1712 die erste kommerzielle Kolbendampfmaschine baute, über eine wissenschaftliche Ausbildung verfügte.
Diese Innovationen senkten die Kosten für Eisen und machten Pferdeeisenbahnen und Eisenbrücken praktisch.
Mit der Entwicklung der Hochdruckdampfmaschine ermöglichte das Leistungsgewicht der Dampfmaschinen praktische Dampfschiffe und Lokomotiven.
Die industrielle Revolution führte zu einer Nachfrage nach Maschinen mit Metallteilen, was zur Entwicklung mehrerer Werkzeugmaschinen führte.
Präzisionsbearbeitungstechniken wurden in der ersten Hälfte des 19. Jahrhunderts entwickelt.
Bei der US-Volkszählung von 1850 wurde der Beruf „Ingenieur“ erstmals mit 2.000 Personen aufgeführt.
Im Jahr 1890 gab es 6.000 Ingenieure in den Bereichen Bauwesen, Bergbau, Maschinenbau und Elektrotechnik.
Zu den Grundlagen der Elektrotechnik im 19. Jahrhundert gehörten die Experimente von Alessandro Volta, Michael Faraday, Georg Ohm und anderen sowie die Erfindung des elektrischen Telegraphen im Jahr 1816 und des Elektromotors im Jahr 1872.
Die Luftfahrttechnik befasst sich mit der Prozessgestaltung des Flugzeugentwurfs, während Luft- und Raumfahrttechnik ein modernerer Begriff ist, der die Reichweite der Disziplin durch die Einbeziehung des Entwurfs von Raumfahrzeugen erweitert.
Historisch gesehen waren Schiffstechnik und Bergbautechnik wichtige Zweige.
Aus diesem Grund erlernen viele Ingenieure im Laufe ihrer Karriere immer wieder neues Material.
Um ein technisch erfolgreiches Produkt zu bauen, reicht es in der Regel nicht aus, vielmehr muss es noch weitere Anforderungen erfüllen.
Nachdem Genrich Altshuller Statistiken über eine große Anzahl von Patenten gesammelt hatte, kam er zu dem Schluss, dass Kompromisse das Herzstück von Konstruktionsentwürfen auf „niedriger Ebene“ seien, während auf einer höheren Ebene der beste Entwurf derjenige sei, der den Kernwiderspruch, der das Problem verursacht, beseitigt.
Durch Tests wird sichergestellt, dass die Produkte die erwartete Leistung erbringen.
Neben der typischen betriebswirtschaftlichen Anwendungssoftware gibt es eine Reihe computergestützter Anwendungen (computergestützte Technologien) speziell für das Ingenieurwesen.
Damit können Ingenieure 3D-Modelle, 2D-Zeichnungen und Schaltpläne ihrer Entwürfe erstellen.
Der Zugriff und die Verteilung all dieser Informationen wird in der Regel mithilfe einer Produktdatenmanagementsoftware organisiert.
Es liegt in der Natur der Technik, dass sie mit der Gesellschaft, der Kultur und dem menschlichen Verhalten verknüpft ist.
Ingenieurprojekte können kontrovers diskutiert werden.
Ingenieurwesen ist ein zentraler Treiber für Innovation und menschliche Entwicklung.
Dies kann viele negative wirtschaftliche und politische Probleme sowie ethische Probleme verursachen.
Wissenschaftler müssen möglicherweise auch technische Aufgaben erledigen, beispielsweise Versuchsgeräte entwerfen oder Prototypen bauen.
Erstens geht es oft um Bereiche, in denen die grundlegende Physik oder Chemie gut verstanden ist, die Probleme selbst jedoch zu komplex sind, um sie exakt lösen zu können.
Ersteres setzt ein Verständnis in ein mathematisches Prinzip um, während letzteres die beteiligten Variablen misst und Technologie schafft.
Ein Physiker benötigt normalerweise eine zusätzliche und relevante Ausbildung.
Ein Beispiel hierfür ist die Verwendung numerischer Näherungen an die Navier-Stokes-Gleichungen zur Beschreibung der aerodynamischen Strömung über einem Flugzeug oder die Verwendung der Finite-Elemente-Methode zur Berechnung der Spannungen in komplexen Bauteilen.
Ingenieure legen Wert auf Innovation und Erfindung.
Da ein Entwurf realistisch und funktional sein muss, müssen seine Geometrie-, Abmessungen- und Merkmalsdaten definiert sein.
So studierten sie Mathematik, Physik, Chemie, Biologie und Mechanik.
Die moderne Medizin kann mehrere Körperfunktionen durch den Einsatz künstlicher Organe ersetzen und die Funktion des menschlichen Körpers durch künstliche Geräte wie beispielsweise Gehirnimplantate und Herzschrittmacher erheblich verändern.
Beide Bereiche bieten Lösungen für reale Probleme.
Ingenieurmanagement oder „Management Engineering“ ist ein spezialisiertes Managementgebiet, das sich mit der Ingenieurpraxis oder dem Ingenieursektor befasst.
Auf Change Management spezialisierte Ingenieure müssen über fundierte Kenntnisse in der Anwendung arbeits- und organisationspsychologischer Prinzipien und Methoden verfügen.
Künstliche Intelligenz (KI) ist die von Maschinen demonstrierte Intelligenz im Gegensatz zur natürlichen Intelligenz von Menschen oder Tieren.
Die KI-Forschung hat im Laufe ihres Lebens viele verschiedene Ansätze ausprobiert und wieder verworfen, darunter die Simulation des Gehirns, die Modellierung menschlicher Problemlösungen, formale Logik, große Wissensdatenbanken und die Nachahmung tierischen Verhaltens.
Zu den traditionellen Zielen der KI-Forschung gehören Argumentation, Wissensrepräsentation, Planung, Lernen, Verarbeitung natürlicher Sprache, Wahrnehmung und die Fähigkeit, Objekte zu bewegen und zu manipulieren.
KI stützt sich auch auf Informatik, Psychologie, Linguistik, Philosophie und viele andere Bereiche.
Das Studium des mechanischen oder „formalen“ Denkens begann bei Philosophen und Mathematikern in der Antike.
Die Church-Turing-These sowie gleichzeitige Entdeckungen in der Neurobiologie, Informationstheorie und Kybernetik führten dazu, dass Forscher über die Möglichkeit des Aufbaus eines elektronischen Gehirns nachdachten.
Die Teilnehmer Allen Newell (CMU), Herbert Simon (CMU), John McCarthy (MIT), Marvin Minsky (MIT) und Arthur Samuel (IBM) wurden zu Begründern und Leitern der KI-Forschung.
Die Gründer von AI blickten optimistisch in die Zukunft: Herbert Simon sagte voraus, dass „Maschinen innerhalb von zwanzig Jahren in der Lage sein werden, jede Arbeit zu erledigen, die ein Mensch tun kann“.
Der Fortschritt verlangsamte sich und 1974 stellten sowohl die US-amerikanische als auch die britische Regierung als Reaktion auf die Kritik von Sir James Lighthill und den anhaltenden Druck des US-Kongresses, produktivere Projekte zu finanzieren, die Sondierungsforschung im Bereich KI ein.
Bis 1985 hatte der Markt für KI über eine Milliarde Dollar erreicht.
Schnellere Computer, algorithmische Verbesserungen und der Zugriff auf große Datenmengen ermöglichten Fortschritte beim maschinellen Lernen und der Wahrnehmung; Datenhungrige Deep-Learning-Methoden dominierten etwa 2012 die Genauigkeitsbenchmarks.
Die KI-Forschung gliederte sich in konkurrierende Teilbereiche, die oft nicht miteinander kommunizieren konnten.
Die Forschung konzentrierte sich auf drei Institutionen: Carnegie Mellon University, Stanford und MIT, und wie unten beschrieben, entwickelte jede ihren eigenen Forschungsstil.
Sie gaben ihrer Arbeit mehrere Namen: z.B. verkörpert, situiert, verhaltensbasiert oder entwicklungsbezogen.
Die gemeinsame mathematische Sprache ermöglichte ein hohes Maß an Zusammenarbeit mit etablierteren Bereichen (wie Mathematik, Wirtschaftswissenschaften oder Operations Research).
Heutzutage sind Ergebnisse von Experimenten oft genau messbar und manchmal (mit Schwierigkeiten) reproduzierbar.
Diese Algorithmen erwiesen sich als unzureichend für die Lösung großer Denkprobleme, da sie eine „kombinatorische Explosion“ erlebten: Sie wurden exponentiell langsamer, je größer die Probleme wurden.
Zu den Dingen, die eine umfassende, allgemeinverständliche Wissensdatenbank enthalten würde, gehören: Objekte, Eigenschaften, Kategorien und Beziehungen zwischen Objekten; Situationen, Ereignisse, Zustände und Zeit; Ursachen und Wirkungen; Wissen über Wissen (was wir über das wissen, was andere Leute wissen); und viele andere, weniger gut erforschte Bereiche.
Wenn zum Beispiel ein Vogel im Gespräch auftaucht, stellen sich die Leute normalerweise ein faustgroßes Tier vor, das singt und fliegt.
Fast nichts ist einfach so wahr oder falsch, wie es die abstrakte Logik erfordert.
Forschungsprojekte, die versuchen, eine vollständige Wissensbasis aus allgemeinverständlichem Wissen aufzubauen (z. B. Cyc), erfordern enorme Mengen an mühsamer ontologischer Technik – sie müssen von Hand ein kompliziertes Konzept nach dem anderen erstellen.
Sie brauchen eine Möglichkeit, sich die Zukunft vorzustellen – eine Darstellung des Zustands der Welt und in der Lage zu sein, Vorhersagen darüber zu treffen, wie ihre Handlungen sie verändern werden – und in der Lage zu sein, Entscheidungen zu treffen, die den Nutzen (oder „Wert“) der verfügbaren Entscheidungen maximieren .
Dies erfordert einen Agenten, der nicht nur seine Umgebung beurteilen und Vorhersagen treffen kann, sondern auch seine Vorhersagen bewerten und sich auf der Grundlage seiner Einschätzung anpassen kann.
Die Klassifizierung wird verwendet, um zu bestimmen, zu welcher Kategorie etwas gehört. Sie erfolgt, nachdem ein Programm eine Reihe von Beispielen für Dinge aus mehreren Kategorien gesehen hat.
Die Theorie des computergestützten Lernens kann Lernende anhand der Rechenkomplexität, der Stichprobenkomplexität (wie viele Daten erforderlich sind) oder anhand anderer Optimierungsvorstellungen bewerten.
Viele aktuelle Ansätze nutzen Häufigkeiten des gleichzeitigen Vorkommens von Wörtern, um syntaktische Darstellungen von Texten zu konstruieren. "
Moderne statistische NLP-Ansätze können alle diese und andere Strategien kombinieren und erzielen häufig eine akzeptable Genauigkeit auf Seiten- oder Absatzebene.
Ein moderner mobiler Roboter kann in einer kleinen, statischen und sichtbaren Umgebung leicht seinen Standort bestimmen und seine Umgebung kartieren; Allerdings stellen dynamische Umgebungen, wie (in der Endoskopie) das Innere des Atemkörpers eines Patienten, eine größere Herausforderung dar.
Einige virtuelle Assistenten sind beispielsweise so programmiert, dass sie im Gespräch sprechen oder sogar humorvoll scherzen; Dadurch scheinen sie sensibler auf die emotionale Dynamik der menschlichen Interaktion zu reagieren oder die Interaktion zwischen Mensch und Computer auf andere Weise zu erleichtern.
Lässt sich intelligentes Verhalten mit einfachen, eleganten Prinzipien (wie Logik oder Optimierung) beschreiben?
Oder verwenden wir Algorithmen, die uns nur eine „vernünftige“ Lösung liefern können (z. B. probabilistische Methoden), aber möglicherweise denselben unergründlichen Fehlern zum Opfer fallen, die die menschliche Intuition macht?
Stuart Russell und Peter Norvig stellen fest, dass die meisten KI-Forscher „sich nicht um die starke KI-Hypothese kümmern – solange das Programm funktioniert, ist es ihnen egal, ob man es eine Simulation von Intelligenz oder echte Intelligenz nennt.“
Die neue Intelligenz könnte somit exponentiell zunehmen und die des Menschen dramatisch übertreffen.
Der Zusammenhang zwischen Automatisierung und Beschäftigung ist kompliziert.
Die subjektiven Einschätzungen des Risikos variieren stark; Beispielsweise schätzen Michael Osborne und Carl Benedikt Frey, dass 47 % der US-Arbeitsplätze einem „hohen Risiko“ einer möglichen Automatisierung ausgesetzt sind, während ein OECD-Bericht nur 9 % der US-Arbeitsplätze als „hohes Risiko“ einstuft.
Langfristig schlagen die Wissenschaftler vor, die Funktion weiter zu optimieren und gleichzeitig mögliche Sicherheitsrisiken, die mit neuen Technologien einhergehen, zu minimieren.
In seinem Buch Superintelligence argumentiert der Philosoph Nick Bostrom, dass künstliche Intelligenz eine Bedrohung für die Menschheit darstellen wird.
Bostrom betont auch die Schwierigkeit, einer fortschrittlichen KI die Werte der Menschheit vollständig zu vermitteln.
In seinem Buch „Human Compatible“ greift der KI-Forscher Stuart J. Russell einige von Bostroms Bedenken auf und schlägt gleichzeitig einen Ansatz zur Entwicklung nachweislich nützlicher Maschinen vor, der sich auf Unsicherheit und Respekt gegenüber Menschen konzentriert und möglicherweise inverses Verstärkungslernen beinhaltet.
Die Meinung von Experten auf dem Gebiet der künstlichen Intelligenz ist gemischt, wobei ein beträchtlicher Teil sowohl besorgt als auch unbesorgt über das Risiko einer möglicherweise übermenschlichen KI ist.
Mark Zuckerberg, CEO von Facebook, glaubt, dass KI „eine Menge positiver Dinge freisetzen“ wird, etwa die Heilung von Krankheiten und die Erhöhung der Sicherheit autonomer Autos.
Musk finanziert auch Unternehmen, die künstliche Intelligenz entwickeln, wie DeepMind und Vicarious, um „einfach im Auge zu behalten, was mit künstlicher Intelligenz los ist“.
Die Forschung in diesem Bereich umfasst Maschinenethik, künstliche moralische Agenten und freundliche KI, und es wird auch über die Schaffung eines Menschenrechtsrahmens diskutiert.
Es ist an der Zeit, zumindest einigen Maschinen eine ethische Dimension hinzuzufügen.
Die Erforschung der Maschinenethik ist der Schlüssel zur Zerstreuung von Bedenken hinsichtlich autonomer Systeme – man könnte argumentieren, dass die Vorstellung autonomer Maschinen ohne eine solche Dimension die Wurzel aller Ängste vor maschineller Intelligenz ist.
Menschen sollten nicht davon ausgehen, dass Maschinen oder Roboter uns positiv behandeln würden, da es keinen a priori Grund zu der Annahme gibt, dass sie mit unserem Moralsystem einverstanden wären, das sich zusammen mit unserer besonderen Biologie entwickelt hat (die KIs nicht teilen würden).
Ein Vorschlag zur Bewältigung dieses Problems besteht darin, sicherzustellen, dass die erste allgemein intelligente KI eine „freundliche KI“ ist und in der Lage sein wird, später entwickelte KIs zu kontrollieren.
Ich denke, die Sorge rührt von einem grundlegenden Fehler her, der darin besteht, den Unterschied zwischen den sehr realen jüngsten Fortschritten in einem bestimmten Aspekt der KI und der Ungeheuerlichkeit und Komplexität des Aufbaus empfindungsfähiger Willensintelligenz nicht zu unterscheiden.“
Regulierung wird als notwendig erachtet, um sowohl KI zu fördern als auch die damit verbundenen Risiken zu bewältigen.
Ein gemeinsames Motiv in diesen Werken begann mit Mary Shelleys Frankenstein, wo eine menschliche Schöpfung zu einer Bedrohung für ihre Herren wird.
Isaac Asimov stellte die drei Gesetze der Robotik in vielen Büchern und Geschichten vor, insbesondere in der „Multivac“-Reihe über einen superintelligenten Computer mit demselben Namen.
In den 1980er Jahren wurde in Japan die Serie „Sexy Robots“ des Künstlers Hajime Sorayama gemalt und veröffentlicht, die die tatsächliche organische menschliche Form mit lebensechten, muskulösen Metallhäuten darstellte. Später folgte das Buch „The Gynoids“, das von Filmemachern wie George Lucas und anderen Kreativen verwendet oder beeinflusst wurde.
Biotechnologie ist ein weites Gebiet der Biologie, bei dem lebende Systeme und Organismen zur Entwicklung oder Herstellung von Produkten genutzt werden.
Die American Chemical Society definiert Biotechnologie als die Anwendung biologischer Organismen, Systeme oder Prozesse durch verschiedene Industrien, um mehr über die Wissenschaft des Lebens zu erfahren und den Wert von Materialien und Organismen wie Pharmazeutika, Nutzpflanzen und Nutztieren zu steigern.
Unter Bioingenieurwesen versteht man die Anwendung ingenieur- und naturwissenschaftlicher Prinzipien auf Gewebe, Zellen und Moleküle.
Durch die frühe Biotechnologie wählten und züchteten die ersten Landwirte die am besten geeigneten Pflanzen mit den höchsten Erträgen, um genügend Nahrungsmittel für die Ernährung einer wachsenden Bevölkerung zu produzieren.
Diese Prozesse wurden auch in die frühe Gärung von Bier einbezogen.
Bei diesem Prozess werden die Kohlenhydrate in den Körnern in Alkohole wie Ethanol zerlegt.
Obwohl der Prozess der Fermentation erst durch die Arbeit von Louis Pasteur im Jahr 1857 vollständig verstanden wurde, handelt es sich dennoch um den ersten Einsatz der Biotechnologie zur Umwandlung einer Nahrungsquelle in eine andere Form.
Diese Berichte trugen zu Darwins Theorie der natürlichen Auslese bei.
Im Jahr 1928 entdeckte Alexander Fleming den Schimmelpilz Penicillium.
Der MOSFET (Metalloxid-Halbleiter-Feldeffekttransistor) wurde 1959 von Mohamed M. Atalla und Dawon Kahng erfunden.
Der erste BioFET war der ionenempfindliche Feldeffekttransistor (ISFET), der 1970 von Piet Bergveld erfunden wurde.
Bis Mitte der 1980er Jahre wurden weitere BioFETs entwickelt, darunter der Gassensor-FET (GASFET), der Drucksensor-FET (PRESSFET), der chemische Feldeffekttransistor (ChemFET), der Referenz-ISFET (REFET) und der enzymmodifizierte FET (ENFET). und immunologisch modifizierter FET (IMFET).
Es wird erwartet, dass die steigende Nachfrage nach Biokraftstoffen eine gute Nachricht für den Biotechnologiesektor ist, da das Energieministerium schätzt, dass der Einsatz von Ethanol den Verbrauch von aus Erdöl gewonnenen Kraftstoffen in den USA bis 2030 um bis zu 30 % senken könnte.
TCE: The Chemical Engineer, (816), 26–31.
Ein weiteres Beispiel ist die Entwicklung transgener Pflanzen, die in bestimmten Umgebungen in Gegenwart (oder Abwesenheit) von Chemikalien wachsen sollen.
Andererseits werden bei einigen Anwendungen der grünen Biotechnologie Mikroorganismen eingesetzt, um Abfall zu reinigen und zu reduzieren.
Sowie die Entwicklung von Hormonen, Stammzellen, Antikörpern, siRNA und diagnostischen Tests.
Eine Anwendung ist die Schaffung von verbessertem Saatgut, das den extremen Umweltbedingungen in Trockengebieten standhält, was mit der Innovation, der Entwicklung landwirtschaftlicher Techniken und dem Ressourcenmanagement zusammenhängt.
Der Zweck der Pharmakogenomik besteht darin, rationale Methoden zur Optimierung der Arzneimitteltherapie im Hinblick auf den Genotyp des Patienten zu entwickeln, um maximale Wirksamkeit bei minimalen Nebenwirkungen sicherzustellen.
Mit moderner Biotechnologie lassen sich bestehende Medikamente relativ einfach und kostengünstig herstellen.
Gentests ermöglichen die genetische Diagnose von Anfälligkeiten für Erbkrankheiten und können auch zur Bestimmung der Abstammung eines Kindes (genetische Mutter und Vater) oder allgemein der Abstammung einer Person eingesetzt werden.
In den meisten Fällen werden Tests eingesetzt, um Veränderungen zu finden, die mit Erbkrankheiten in Zusammenhang stehen.
Biotechnologieunternehmen können zur künftigen Ernährungssicherheit beitragen, indem sie die Ernährung und Rentabilität der städtischen Landwirtschaft verbessern.
Im Jahr 2010 wurden 10 % der weltweiten Ackerflächen mit gentechnisch veränderten Pflanzen bepflanzt.
Diese Techniken haben die Einführung neuer Pflanzenmerkmale sowie eine weitaus größere Kontrolle über die genetische Struktur eines Lebensmittels ermöglicht, als dies zuvor durch Methoden wie selektive Züchtung und Mutationszüchtung möglich war.
Diese wurden im Hinblick auf Resistenz gegen Krankheitserreger und Herbizide sowie bessere Nährstoffprofile entwickelt.
Dennoch ist die Wahrscheinlichkeit, dass die Öffentlichkeit gentechnisch veränderte Lebensmittel als sicher einschätzt, weitaus geringer als bei Wissenschaftlern.
Gegner haben jedoch aus mehreren Gründen Einwände gegen gentechnisch veränderte Pflanzen an sich erhoben, darunter Bedenken hinsichtlich der Umwelt, der Frage, ob aus gentechnisch veränderten Pflanzen hergestellte Lebensmittel sicher sind, der Frage, ob gentechnisch veränderte Pflanzen benötigt werden, um den weltweiten Nahrungsmittelbedarf zu decken, und wirtschaftlicher Bedenken, die sich aus der Tatsache ergeben, dass diese Organismen betroffen sind zum Recht des geistigen Eigentums.
Es gibt Unterschiede in der Regulierung von GVO zwischen den Ländern, wobei einige der deutlichsten Unterschiede zwischen den USA und Europa bestehen.
Die Europäische Union unterscheidet zwischen der Zulassung zum Anbau innerhalb der EU und der Zulassung zur Einfuhr und Verarbeitung.
Jeder erfolgreiche Antrag wird in der Regel für fünf Jahre gefördert und muss dann wettbewerblich verlängert werden.
Beim Klonen werden einzelne Organismen mit entweder identischer oder nahezu identischer DNA auf natürliche oder künstliche Weise erzeugt.
Es wird in einer Vielzahl biologischer Experimente und praktischer Anwendungen eingesetzt, die vom genetischen Fingerabdruck bis zur Proteinproduktion im großen Maßstab reichen.
Zunächst muss die gewünschte DNA isoliert werden, um ein DNA-Segment geeigneter Größe bereitzustellen.
Nach der Ligation wird der Vektor mit dem interessierenden Insert in Zellen transfiziert.
Eine nützliche Gewebekulturtechnik zum Klonen verschiedener Zelllinienlinien umfasst die Verwendung von Klonringen (Zylindern).
Dieser Vorgang wird auch „Forschungsklonen“ oder „therapeutisches Klonen“ genannt.
Therapeutisches Klonen wird durch die Schaffung embryonaler Stammzellen erreicht, mit der Hoffnung, Krankheiten wie Diabetes und Alzheimer zu behandeln.
Der Grund, warum SCNT zum Klonen verwendet wird, liegt darin, dass somatische Zellen im Labor leicht gewonnen und kultiviert werden können.
Die Eizelle reagiert auf den somatischen Zellkern genauso wie auf den Kern einer Samenzelle.
Die somatischen Zellen könnten sofort verwendet oder zur späteren Verwendung im Labor gelagert werden.
Dadurch entsteht ein einzelliger Embryo.
Die erfolgreich entwickelten Embryonen werden dann in Ersatzempfänger, beispielsweise eine Kuh oder ein Schaf im Falle von Nutztieren, eingesetzt.
Ein weiterer Vorteil besteht darin, dass SCNT als Lösung zum Klonen gefährdeter Arten angesehen wird, die vom Aussterben bedroht sind.
Nur drei dieser Embryonen überlebten bis zur Geburt und nur einer überlebte das Erwachsenenalter.
Im Jahr 2014 meldeten Forscher jedoch Erfolgsraten beim Klonen von sieben bis acht von zehn, und im Jahr 2016 wurde berichtet, dass ein koreanisches Unternehmen, Sooam Biotech, täglich 500 geklonte Embryonen produziert.
Die ungeschlechtliche Fortpflanzung ist ein natürliches Phänomen bei vielen Arten, darunter den meisten Pflanzen und einigen Insekten.
Beispielsweise handelt es sich bei einigen europäischen Rebsorten um Klone, die seit über zwei Jahrtausenden vermehrt werden.
Viele Bäume, Sträucher, Weinreben, Farne und andere Stauden bilden auf natürliche Weise klonale Kolonien.
Bei Pflanzen bedeutet Parthenogenese die Entwicklung eines Embryos aus einer unbefruchteten Eizelle und ist ein Teilprozess der Apomixis.
Solche Klone sind nicht völlig identisch, da die somatischen Zellen Mutationen in ihrer Kern-DNA enthalten können.
Künstliche Embryonenspaltung oder Embryo-Twinning, eine Technik, bei der aus einem einzigen Embryo monozygote Zwillinge entstehen, wird nicht in gleicher Weise wie andere Klonmethoden betrachtet.
Dollys Embryo entstand, indem man die Zelle nahm und in eine Schafei einführte.
Sie wurde am Roslin Institute in Schottland von den britischen Wissenschaftlern Sir Ian Wilmut und Keith Campbell geklont und lebte dort von ihrer Geburt im Jahr 1996 bis zu ihrem Tod im Jahr 2003, als sie sechs Jahre alt war.
Dolly war von öffentlicher Bedeutung, weil die Bemühungen zeigten, dass genetisches Material aus einer bestimmten erwachsenen Zelle, das nur eine bestimmte Teilmenge seiner Gene exprimieren soll, umgestaltet werden kann, um einen völlig neuen Organismus hervorzubringen.
Das erste Klonen von Säugetieren (das zum Schaf Dolly führte) hatte eine Erfolgsquote von 29 Embryonen pro 277 befruchteten Eiern, die bei der Geburt drei Lämmer hervorbrachten, von denen eines überlebte.
Obwohl es sich bei den ersten Klonen um Frösche handelte, wurde bemerkenswerterweise noch kein erwachsener geklonter Frosch aus einer somatischen erwachsenen Kernspenderzelle erzeugt.
Andere Forscher, darunter Ian Wilmut, der das Team leitete, das Dolly erfolgreich geklont hat, argumentieren jedoch, dass Dollys früher Tod aufgrund einer Atemwegsinfektion nichts mit Problemen beim Klonvorgang zu tun hatte.
Die sowjetischen Wissenschaftler Chaylakhyan, Veprencev, Sviridova und Nikitin ließen die Maus „Mascha“ klonen.
Eher eine künstliche Zwillingsbildung.
Hund: Snuppy, ein männlicher Afghanischer Windhund, war der erste geklonte Hund (2005).
Wasserbüffel: Samrupa war der erste geklonte Wasserbüffel.
Kamel: (2009) Injaz ist das erste geklonte Kamel.
Ziege: (2001) Wissenschaftler der Northwest A&F University haben erfolgreich die erste Ziege geklont, die die erwachsene weibliche Zelle nutzt.
2017 in China durchgeführt und im Januar 2018 veröffentlicht.
Schwarzfußfrettchen: (2020) Im Jahr 2020 klonte ein Team von Wissenschaftlern ein Weibchen namens Willa, das Mitte der 1980er Jahre starb und keine lebenden Nachkommen hinterließ.
Es bezieht sich nicht auf die natürliche Empfängnis und Entbindung eineiiger Zwillinge.
Derzeit haben Wissenschaftler nicht die Absicht, zu versuchen, Menschen zu klonen, und sie glauben, dass ihre Ergebnisse eine breitere Diskussion über die Gesetze und Vorschriften auslösen sollten, die die Welt zur Regulierung des Klonens benötigt.
Während viele dieser Ansichten religiösen Ursprungs sind, werden die durch das Klonen aufgeworfenen Fragen auch aus säkularen Perspektiven betrachtet.
Gegner des Klonens haben Bedenken, dass die Technologie noch nicht sicher genug entwickelt ist und dass sie anfällig für Missbrauch sein könnte (was zur Generation von Menschen führen könnte, von denen Organe und Gewebe entnommen würden), sowie Bedenken hinsichtlich der Integration geklonter Individuen mit Familien und mit der Gesellschaft insgesamt.
Dies wird auch als „konservierendes Klonen“ bezeichnet.
Diese Erfolge gaben Anlass zur Hoffnung, dass ähnliche Techniken (unter Verwendung von Leihmüttern einer anderen Art) zum Klonen ausgestorbener Arten eingesetzt werden könnten.
Im Jahr 2002 gaben Genetiker des Australian Museum bekannt, dass sie mithilfe der Polymerase-Kettenreaktion die DNA des Beutelwolfs (Tasmanischer Tiger) repliziert hatten, der damals seit etwa 65 Jahren ausgestorben war.
Im Jahr 2003 wurde im Zentrum für Lebensmitteltechnologie und -forschung von Aragon zum ersten Mal ein ausgestorbenes Tier, der oben erwähnte Pyrenäensteinbock, geklont, wobei der konservierte gefrorene Zellkern der Hautproben aus dem Jahr 2001 und Eizellen von Hausziegen verwendet wurden.
„Когда вернутся маMONты“ („Wenn die Mammuts zurückkehren“), 5. Februar 2015 (abgerufen am 6. September 2015) Ein weiteres Problem ist das Überleben des rekonstruierten Mammuts: Wiederkäuer sind für die Verdauung auf eine Symbiose mit bestimmten Mikrobiota in ihrem Magen angewiesen.
Aus diesem Grund vermuteten einige, dass sie möglicherweise schneller gealtert sei als andere natürlich geborene Tiere, da sie im Alter von sechs Jahren für ein Schaf relativ früh starb.
Allerdings sind Frühschwangerschaftsverluste und Neugeborenenverluste beim Klonen immer noch größer als bei natürlicher Empfängnis oder assistierter Reproduktion (IVF).
Das Konzept des Klonens, insbesondere des Klonens von Menschen, ist Gegenstand einer Vielzahl von Science-Fiction-Werken.
Viele Werke stellen die künstliche Erschaffung des Menschen durch eine Methode dar, bei der Zellen aus einer Gewebe- oder DNA-Probe gezüchtet werden; Die Replikation kann augenblicklich erfolgen oder durch langsames Wachstum menschlicher Embryonen in künstlichen Gebärmuttern erfolgen.
In Science-Fiction-Filmen wie „Matrix“ und „Star Wars: Episode II – Angriff der Klonkrieger“ wurden Szenen gezeigt, in denen menschliche Föten im industriellen Maßstab in mechanischen Tanks kultiviert werden.
„A Number“ wurde von Caryl Churchill in einer Koproduktion zwischen der BBC und HBO Films für das Fernsehen adaptiert.
Als Kind zweifelte sie immer an der Liebe ihrer Mutter, die überhaupt nicht wie sie aussah und neun Jahre zuvor gestorben war.
In dem Roman „Die Jungen aus Brasilien“ von Ira Levin aus dem Jahr 1976 und seiner Verfilmung aus dem Jahr 1978 nutzt Josef Mengele das Klonen, um Kopien von Adolf Hitler zu erstellen.
In Doctor Who wurde 1973 in der Serie „The Time Warrior“ eine außerirdische Rasse aus gepanzerten, kriegerischen Wesen namens Sontarans vorgestellt.
Das Konzept, dass geklonte Soldaten für den Kampf gezüchtet werden, wurde in „The Doctor's Daughter“ (2008) erneut aufgegriffen, als die DNA des Doktors verwendet wird, um eine Kriegerin namens Jenny zu erschaffen.
Der Roman „Never Let Me Go“ von Kazuo Ishiguro aus dem Jahr 2005 und die Verfilmung aus dem Jahr 2010 spielen in einer alternativen Geschichte, in der geklonte Menschen nur zu dem Zweck erschaffen werden, auf natürlichem Weg geborene Menschen mit Organspenden zu versorgen, obwohl sie vollkommen empfindungsfähig und selbstbewusst sind. bewusst.
In dem futuristischen Roman „Cloud Atlas“ und dem darauffolgenden Film konzentriert sich einer der Handlungsstränge auf einen gentechnisch veränderten Fabrikklon namens Sonmi~451, einen von Millionen, die in einem künstlichen „Mutterleib“ aufgezogen werden und dazu bestimmt sind, von Geburt an zu dienen.
Im Film „Us“ erstellt die US-Regierung irgendwann vor den 1980er-Jahren Klone von jedem US-Bürger mit der Absicht, sie zur Kontrolle ihrer ursprünglichen Gegenstücke zu nutzen, ähnlich wie Voodoo-Puppen.
In der heutigen Zeit starten die Klone einen Überraschungsangriff und schaffen es, einen Massenvölkermord an ihren ahnungslosen Gegenstücken zu begehen.
Gene wurden innerhalb derselben Art, zwischen Arten (wodurch transgene Organismen entstanden) und sogar zwischen Königreichen übertragen.
Gentechniker müssen das Gen, das sie in den Wirtsorganismus einfügen möchten, isolieren und es mit anderen genetischen Elementen kombinieren, einschließlich einer Promotor- und Terminatorregion und häufig einem selektierbaren Marker.
Herbert Boyer und Stanley Cohen stellten 1973 den ersten gentechnisch veränderten Organismus her, ein Bakterium, das gegen das Antibiotikum Kanamycin resistent war.
Das erste gentechnisch veränderte Tier, das kommerzialisiert wurde, war der GloFish (2003) und das erste gentechnisch veränderte Tier, das für den Lebensmittelgebrauch zugelassen wurde, war der AquAdvantage-Lachs im Jahr 2015.
Pilze wurden mit weitgehend denselben Zielen entwickelt.
Es gibt Vorschläge, die virulenten Gene von Viren zu entfernen, um Impfstoffe herzustellen.
Die meisten sind auf Herbizidtoleranz oder Insektenresistenz ausgelegt.
Tiere sind im Allgemeinen viel schwieriger zu transformieren und die überwiegende Mehrheit befindet sich noch im Forschungsstadium.
Der Viehbestand wird mit der Absicht verändert, wirtschaftlich wichtige Merkmale wie Wachstumsrate, Fleischqualität, Milchzusammensetzung, Krankheitsresistenz und Überleben zu verbessern.
Obwohl die Gentherapie beim Menschen noch relativ neu ist, wird sie bereits zur Behandlung genetischer Störungen wie schwerer kombinierter Immunschwäche und der Leberschen angeborenen Amaurose eingesetzt.
Weitere Bedenken betreffen die Objektivität und Strenge der Regulierungsbehörden, die Kontamination nicht gentechnisch veränderter Lebensmittel, die Kontrolle der Lebensmittelversorgung, die Patentierung von Leben und die Nutzung geistiger Eigentumsrechte.
Die Länder haben Regulierungsmaßnahmen ergriffen, um diesen Bedenken Rechnung zu tragen.
Eine weit gefasste Definition der Gentechnik umfasst auch selektive Züchtung und andere Mittel der künstlichen Selektion.“,
Beispielsweise wurde die Getreidepflanze Triticale 1930 in einem Labor vollständig entwickelt und dabei verschiedene Techniken zur Veränderung ihres Genoms eingesetzt.
Moderne Biotechnologie wird weiter definiert als „In-vitro-Nukleinsäuretechniken, einschließlich rekombinanter Desoxyribonukleinsäure (DNA) und direkter Injektion von Nukleinsäure in Zellen oder Organellen oder Fusion von Zellen außerhalb der taxonomischen Familie.“
Die Definitionen konzentrieren sich mehr auf den Prozess als auf das Produkt, was bedeutet, dass es GVO und Nicht-GVO mit sehr ähnlichen Genotypen und Phänotypen geben kann.
Es wirft auch Probleme auf, wenn neue Prozesse entwickelt werden.
Gentechniker müssen das Gen isolieren, das sie in den Wirtsorganismus einfügen möchten.
Das Gen wird dann mit anderen genetischen Elementen kombiniert, einschließlich einer Promotor- und Terminatorregion und einem selektierbaren Marker.
DNA wird im Allgemeinen mittels Mikroinjektion in tierische Zellen eingebracht, wo sie durch die Kernhülle der Zelle direkt in den Zellkern injiziert werden kann, oder durch die Verwendung viraler Vektoren.
Bei Pflanzen geschieht dies durch Gewebekultur.
Traditionell wurde das neue genetische Material zufällig in das Wirtsgenom eingefügt.
Es gibt vier Familien manipulierter Nukleasen: Meganukleasen, Zinkfinger-Nukleasen, Transkriptionsaktivator-ähnliche Effektor-Nukleasen (TALENs) und das Cas9-guideRNA-System (adaptiert von CRISPR).
1972 schuf Paul Berg das erste rekombinante DNA-Molekül, als er die DNA eines Affenvirus mit der des Lambda-Virus kombinierte.
Die Bakterien, die das Plasmid erfolgreich eingebaut hatten, konnten dann in Gegenwart von Kanamycin überleben.
1974 schuf Rudolf Jaenisch eine transgene Maus, indem er fremde DNA in ihren Embryo einführte und sie damit zum ersten transgenen Tier der Welt machte.
Mäuse mit entfernten Genen (sogenannte Knockout-Mäuse) wurden 1989 geschaffen.
1983 wurde die erste gentechnisch veränderte Pflanze von Michael W. Bevan, Richard B. Flavell und Mary-Dell Chilton entwickelt.
Im Jahr 2000 wurde mit Vitamin A angereicherter goldener Reis als erste Pflanze mit erhöhtem Nährwert entwickelt.
Das von Bakterien produzierte Insulin namens Humulin wurde 1982 von der Food and Drug Administration zur Freigabe zugelassen.
1994 erhielt Calgene die Genehmigung zur kommerziellen Vermarktung der Flavr-Savr-Tomate, dem ersten gentechnisch veränderten Lebensmittel.
Im Jahr 2010 gaben Wissenschaftler des J. Craig Venter Institute bekannt, dass sie das erste synthetische Bakteriengenom geschaffen hatten.
Es wurde 2003 auf den US-Markt gebracht.
Gene und andere genetische Informationen aus einer Vielzahl von Organismen können einem Plasmid hinzugefügt und zur Speicherung und Modifikation in Bakterien eingefügt werden.
Eine große Anzahl maßgeschneiderter Plasmide macht die Manipulation von aus Bakterien extrahierter DNA relativ einfach.
Wissenschaftler können Gene innerhalb der Bakterien leicht manipulieren und kombinieren, um neue oder gestörte Proteine zu erzeugen, und die Auswirkungen beobachten, die dies auf verschiedene molekulare Systeme hat.
Bakterien werden seit langem bei der Herstellung von Lebensmitteln eingesetzt und für diese Arbeit im industriellen Maßstab wurden spezielle Stämme entwickelt und ausgewählt.
Bei den meisten lebensmittelproduzierenden Bakterien handelt es sich um Milchsäurebakterien, und hierher hat sich auch der Großteil der Forschung im Bereich gentechnisch veränderter lebensmittelproduzierender Bakterien entwickelt.
Der Großteil wird in den USA hergestellt und obwohl es Vorschriften gibt, die eine Produktion in Europa ermöglichen, sind dort seit 2015 derzeit keine aus Bakterien gewonnenen Lebensmittelprodukte erhältlich.
Anschließend werden die Bakterien geerntet und daraus das gewünschte Protein gereinigt.
Viele dieser Proteine sind auf natürlichem Wege nicht oder nur schwer zu gewinnen und sie sind weniger wahrscheinlich mit Krankheitserregern kontaminiert, was sie sicherer macht.
Außerhalb der Medizin wurden sie zur Herstellung von Biokraftstoffen verwendet.
Zu den Ideen gehört es, Darmbakterien so zu verändern, dass sie schädliche Bakterien zerstören, oder Bakterien zu verwenden, um mangelhafte Enzyme oder Proteine zu ersetzen oder zu erhöhen.
Den Bakterien die Möglichkeit zu geben, eine Kolonie zu bilden, könnte eine längerfristige Lösung darstellen, könnte aber auch Sicherheitsbedenken aufwerfen, da die Wechselwirkungen zwischen Bakterien und dem menschlichen Körper weniger gut verstanden sind als bei herkömmlichen Medikamenten.
Seit über einem Jahrhundert werden Bakterien in der Landwirtschaft eingesetzt.
Mit Fortschritten in der Gentechnik wurden diese Bakterien manipuliert, um ihre Effizienz zu steigern und das Wirtsspektrum zu erweitern.
Pseudomonas-Bakterienstämme verursachen Frostschäden, indem sie Wasser zu Eiskristallen um sich herum bilden.
Zu den weiteren Einsatzmöglichkeiten gentechnisch veränderter Bakterien gehört die biologische Sanierung, bei der die Bakterien dazu dienen, Schadstoffe in eine weniger toxische Form umzuwandeln.
In den 1980er Jahren wandelten der Künstler Jon Davis und die Genetikerin Dana Boyd das germanische Symbol für Weiblichkeit (ᛉ) in einen Binärcode und dann in eine DNA-Sequenz um, die dann in Escherichia coli exprimiert wurde.
Forscher können damit verschiedene Faktoren kontrollieren; einschließlich der Zielposition, der Insertgröße und der Dauer der Genexpression.
Obwohl sich die Gentherapie größtenteils noch im Versuchsstadium befindet, wurden bereits einige Erfolge erzielt, um defekte Gene zu ersetzen.
Seit 2018 laufen zahlreiche klinische Studien, darunter Behandlungen für Hämophilie, Glioblastom, chronische granulomatöse Erkrankung, Mukoviszidose und verschiedene Krebsarten.
Herpes-simplex-Viren sind vielversprechende Vektoren mit einer Tragfähigkeit von über 30 kb und ermöglichen eine langfristige Expression, obwohl sie bei der Genübertragung weniger effizient sind als andere Vektoren.
Andere Viren, die als Vektoren verwendet wurden, umfassen Alphaviren, Flaviviren, Masernviren, Rhabdoviren, das Newcastle-Virus, Pockenviren und Picornaviren.
Dies hat keinen Einfluss auf die Infektiosität der Viren, löst eine natürliche Immunantwort aus und es besteht keine Chance, dass sie ihre Virulenzfunktion wiedererlangen, was bei einigen anderen Impfstoffen der Fall sein kann.
Der wirksamste Impfstoff gegen Tuberkulose, der Bacillus Calmette-Guérin (BCG)-Impfstoff, bietet nur einen teilweisen Schutz.
Weitere vektorbasierte Impfstoffe sind bereits zugelassen und viele weitere befinden sich in der Entwicklung.
Im Jahr 2004 berichteten Forscher, dass ein genetisch verändertes Virus, das das egoistische Verhalten von Krebszellen ausnutzt, eine alternative Möglichkeit zur Abtötung von Tumoren darstellen könnte.
Das Virus wurde in Orangenbäume injiziert, um die Citrus-Greening-Krankheit zu bekämpfen, die seit 2005 die Orangenproduktion um 70 % reduziert hatte.
Im Labor wurden sowohl gentechnisch veränderte Viren erzeugt, die die Zieltiere durch Immunkontrazeption unfruchtbar machen, als auch andere, die auf das Entwicklungsstadium des Tieres abzielen.
Es wurde eine genetische Veränderung des Myxomavirus vorgeschlagen, um europäische Wildkaninchen auf der Iberischen Halbinsel zu schützen und zur Regulierung ihrer Verbreitung in Australien beizutragen.
Es ist möglich, Bakteriophagen so zu manipulieren, dass sie veränderte Proteine auf ihrer Oberfläche exprimieren und sie in bestimmten Mustern zusammenfügen (eine Technik, die Phagendisplay genannt wird).
Für industrielle Anwendungen kombinieren Hefen die bakteriellen Vorteile eines einzelligen Organismus, der leicht zu manipulieren und zu züchten ist, mit den fortschrittlichen Proteinmodifikationen, die in Eukaryoten vorkommen.
Eines davon erhöht die Effizienz der malolaktischen Fermentation, während das andere die Bildung gefährlicher Ethylcarbamatverbindungen während der Fermentation verhindert.
Im Gegensatz zu Bakterien und Viren haben sie den Vorteil, dass sie die Insekten allein durch Kontakt infizieren, obwohl ihnen chemische Pestizide in ihrer Effizienz überlegen sind.
Ein attraktives Ziel für die biologische Bekämpfung sind Mücken, Überträger einer Reihe tödlicher Krankheiten, darunter Malaria, Gelbfieber und Dengue-Fieber.
Eine andere Strategie besteht darin, den Pilzen Proteine hinzuzufügen, die die Übertragung von Malaria blockieren oder das Plasmodium ganz entfernen.
Viele Pflanzen sind pluripotent, was bedeutet, dass eine einzelne Zelle einer reifen Pflanze geerntet werden kann und sich unter den richtigen Bedingungen zu einer neuen Pflanze entwickeln kann.
Große Fortschritte in der Gewebekultur und den zellulären Mechanismen von Pflanzen für eine Vielzahl von Pflanzen sind auf Systeme zurückzuführen, die im Tabak entwickelt wurden.
Ein weiterer wichtiger Modellorganismus mit Relevanz für die Gentechnik ist Arabidopsis thaliana.
In der Forschung werden Pflanzen so manipuliert, dass sie dabei helfen, die Funktionen bestimmter Gene zu entdecken.
Im Gegensatz zur Mutagenese ermöglicht die Gentechnik eine gezielte Entfernung, ohne andere Gene im Organismus zu zerstören.
Andere Strategien bestehen darin, das Gen an einen starken Promotor zu binden und zu sehen, was passiert, wenn es überexprimiert wird, wodurch ein Gen gezwungen wird, an einer anderen Stelle oder in anderen Entwicklungsstadien exprimiert zu werden.
Die ersten gentechnisch veränderten Zierpflanzen brachten veränderte Farben auf den Markt.
Weitere gentechnisch veränderte Zierpflanzen sind Chrysanthemen und Petunien.
Das Papaya-Ringspot-Virus zerstörte im 20. Jahrhundert Papayabäume auf Hawaii, bis transgenen Papayapflanzen eine Resistenz gegen Krankheitserreger verliehen wurde.
Ziel der zweiten Kulturpflanzengeneration war eine Verbesserung der Qualität, häufig durch Veränderung des Nährstoffprofils.
GV-Pflanzen tragen dazu bei, die Ernten zu verbessern, indem sie den Insektendruck verringern, den Nährwert erhöhen und verschiedene abiotische Belastungen tolerieren.
Die meisten gentechnisch veränderten Nutzpflanzen wurden so modifiziert, dass sie gegen ausgewählte Herbizide, meist auf Glyphosat- oder Glufosinatbasis, resistent sind.
Einige nutzen die Gene, die für vegetative insektizide Proteine kodieren.
Weniger als ein Prozent der gentechnisch veränderten Pflanzen enthielten andere Eigenschaften, darunter die Bereitstellung von Virusresistenz, die Verzögerung der Seneszenz und die Veränderung der Pflanzenzusammensetzung.
Pflanzen und Pflanzenzellen wurden für die Produktion von Biopharmazeutika in Bioreaktoren gentechnisch verändert, ein Prozess, der als Pharming bekannt ist.
Viele Medikamente enthalten auch natürliche pflanzliche Inhaltsstoffe und die Wege, die zu ihrer Herstellung führen, wurden genetisch verändert oder auf andere Pflanzenarten übertragen, um größere Mengen zu produzieren.
Sie bergen auch ein geringeres Risiko einer Kontamination.
Die Herstellung, der Transport und die Verabreichung von Impfstoffen sind teuer. Ein System, das sie vor Ort produzieren könnte, würde daher einen besseren Zugang zu ärmeren und sich entwickelnden Gebieten ermöglichen.
Durch die Lagerung in Pflanzen werden die langfristigen Kosten gesenkt, da sie ohne Kühllagerung verbreitet werden können, nicht gereinigt werden müssen und langfristig stabil sind.
Bis 2018 wurden nur drei gentechnisch veränderte Tiere zugelassen, alle in den USA.
Kanada: Geistesblitz Die ersten transgenen Säugetiere wurden durch die Injektion viraler DNA in Embryonen und die anschließende Implantation der Embryonen in Weibchen erzeugt.
Die Entwicklung des CRISPR-Cas9-Geneditierungssystems als kostengünstige und schnelle Möglichkeit zur direkten Modifikation von Keimzellen, wodurch sich die Zeit, die für die Entwicklung genetisch veränderter Säugetiere benötigt wird, effektiv halbiert.
Genetisch veränderte Mäuse sind die am häufigsten in der biomedizinischen Forschung verwendeten Säugetiere, da sie kostengünstig und leicht zu manipulieren sind.
Im Jahr 2009 gaben Wissenschaftler bekannt, dass es ihnen erstmals gelungen sei, ein Gen auf eine Primatenart (Weißbüschelaffen) zu übertragen.
Eine stabile Expression wurde bei Schafen, Schweinen, Ratten und anderen Tieren erreicht.
Humanes Alpha-1-Antitrypsin ist ein weiteres Protein, das aus Ziegen gewonnen wird und zur Behandlung von Menschen mit diesem Mangel eingesetzt wird.
Schweinelungen von gentechnisch veränderten Schweinen werden für eine Transplantation in den Menschen in Betracht gezogen.
Tiere wurden entwickelt, um schneller zu wachsen, gesünder zu sein und Krankheiten zu widerstehen.
Es wurde ein gentechnisch verändertes Schwein namens Enviropig geschaffen, das pflanzlichen Phosphor effizienter verdauen kann als herkömmliche Schweine.
Dies könnte möglicherweise Müttern zugute kommen, die keine Muttermilch produzieren können, aber möchten, dass ihre Kinder Muttermilch statt Milchnahrung bekommen.
Es gab Vorschläge, dass Gentechnik eingesetzt werden könnte, um Tiere vor dem Aussterben zu retten.
Es wurde zur Behandlung genetischer Störungen wie schwerer kombinierter Immunschwäche und Leber-angeborener Amaurose eingesetzt.
Die Keimbahn-Gentherapie führt dazu, dass jede Veränderung vererbbar ist, was in der wissenschaftlichen Gemeinschaft zu Bedenken geführt hat.
Die Aquakultur ist ein wachsender Wirtschaftszweig, der derzeit über die Hälfte des weltweit konsumierten Fischs liefert.
Mehrere Gruppen haben Zebrafische entwickelt, um Umweltverschmutzung zu erkennen, indem sie fluoreszierende Proteine an Gene anhängen, die durch das Vorhandensein von Schadstoffen aktiviert werden.
Es wurde ursprünglich von einer der Gruppen zur Erkennung von Umweltverschmutzung entwickelt, ist heute jedoch Teil des Zierfischhandels und war das erste gentechnisch veränderte Tier, das als Haustier öffentlich verfügbar wurde, als es 2003 in den USA zum Verkauf angeboten wurde.
Zebrafische sind Modellorganismen für Entwicklungsprozesse, Regeneration, Genetik, Verhalten, Krankheitsmechanismen und Toxizitätstests.
Es wurden gentechnisch veränderte Fische entwickelt, deren Befürworter eine Überproduktion von Wachstumshormonen zur Verwendung in der Aquakulturindustrie vorantreiben, um die Entwicklungsgeschwindigkeit zu erhöhen und möglicherweise den Fischereidruck auf Wildbestände zu verringern.
Es erhielt 2015 die behördliche Zulassung und war das erste gentechnisch veränderte Lebensmittel, das auf den Markt kam.
Drosophila wurden zur Untersuchung von Genetik und Vererbung, Embryonalentwicklung, Lernen, Verhalten und Altern eingesetzt.
Malariaresistente Mücken wurden im Labor entwickelt, indem ein Gen eingefügt wurde, das die Entwicklung des Malariaparasiten reduziert, und dann mithilfe von Homing-Endonukleasen dieses Gen schnell in der männlichen Bevölkerung verbreitet wurde (bekannt als Gene Drive).
Ein anderer Ansatz besteht darin, eine sterile Insektentechnik zu verwenden, bei der gentechnisch so veränderte Männchen, dass sie steril sind, mit lebensfähigen Männchen konkurrieren, um die Populationszahl zu reduzieren.
Der Ansatz ähnelt der an Mücken getesteten sterilen Technik, bei der Männchen mit einem Gen transformiert werden, das verhindert, dass geborene Weibchen erwachsen werden.
In diesem Fall wurde ein durch Strahlung sterilisierter Stamm rosa Kapselwürmer gentechnisch verändert, um ein rot fluoreszierendes Protein zu exprimieren, was es den Forschern erleichtert, sie zu überwachen.
Es besteht auch das Potenzial, die Seidenproduktionsmaschinen zur Herstellung anderer wertvoller Proteine zu nutzen.
Ein gentechnisch verändertes Huhn, das in seinem Ei das Medikament Kanuma produziert, ein Enzym zur Behandlung einer seltenen Erkrankung, erhielt 2015 die behördliche Zulassung in den USA.
Es gibt Vorschläge, die Aga-Kröte in Australien mithilfe von Gentechnik zu bekämpfen.
Es ist auch relativ einfach, stabile transgene Nematoden zu produzieren, und diese sind zusammen mit RNAi die wichtigsten Werkzeuge zur Untersuchung ihrer Gene.
Transgene Nematoden wurden zur Untersuchung von Viren, Toxikologie und Krankheiten sowie zur Erkennung von Umweltschadstoffen eingesetzt.
Plattwürmer haben die Fähigkeit, sich aus einer einzelnen Zelle zu regenerieren.
Der Borstenwurm, ein mariner Ringelwurm, wurde modifiziert.
Die Entwicklung eines Regulierungsrahmens für die Gentechnik begann 1975 in Asilomar, Kalifornien.
Dabei handelt es sich um einen internationalen Vertrag, der die Weitergabe, den Umgang und die Nutzung gentechnisch veränderter Organismen regelt.
Viele Experimente benötigen außerdem die Genehmigung einer nationalen Regulierungsgruppe oder Gesetzgebung.
Es gibt ein nahezu universelles System zur Bewertung der relativen Risiken, die mit GVO und anderen Wirkstoffen für das Laborpersonal und die Gemeinschaft verbunden sind.
Verschiedene Länder verwenden unterschiedliche Nomenklaturen zur Beschreibung der Ebenen und können unterschiedliche Anforderungen an das haben, was auf den einzelnen Ebenen getan werden kann.
Beispielsweise wird eine Kulturpflanze, die nicht für den Lebensmittelgebrauch bestimmt ist, im Allgemeinen nicht von den für Lebensmittelsicherheit zuständigen Behörden überprüft.
Die meisten Länder, die den GVO-Anbau nicht zulassen, erlauben die Forschung mit GVO.
Während in der EU nur wenige GVO für den Anbau zugelassen sind, ist eine Reihe von GVO für den Import und die Verarbeitung zugelassen.
Die US-Politik konzentriert sich nicht so sehr auf den Prozess wie andere Länder, sondern achtet auf nachweisbare wissenschaftliche Risiken und verwendet das Konzept der substanziellen Äquivalenz.
Eine der zentralen Fragen der Regulierungsbehörden ist die Frage, ob gentechnisch veränderte Produkte gekennzeichnet werden sollten.
An dem Streit sind Verbraucher, Produzenten, Biotechnologieunternehmen, staatliche Regulierungsbehörden, Nichtregierungsorganisationen und Wissenschaftler beteiligt.
Die meisten Bedenken beziehen sich auf die gesundheitlichen und ökologischen Auswirkungen von GVO.
Dennoch ist die Wahrscheinlichkeit, dass die Öffentlichkeit gentechnisch veränderte Lebensmittel als sicher einschätzt, weitaus geringer als bei Wissenschaftlern.
Der Genfluss zwischen gentechnisch veränderten Nutzpflanzen und kompatiblen Pflanzen sowie der verstärkte Einsatz von Breitbandherbiziden können das Risiko herbizidresistenter Unkrautpopulationen erhöhen.
Um einige dieser Bedenken auszuräumen, wurden einige GVO mit Merkmalen entwickelt, die dabei helfen, ihre Ausbreitung zu kontrollieren.
Zu weiteren ökologischen und agronomischen Bedenken zählen ein Rückgang der Artenvielfalt, eine Zunahme sekundärer Schädlinge (nicht gezielte Schädlinge) und die Entwicklung resistenter Insektenschädlinge.
Die Auswirkungen von Bt-Pflanzen auf nützliche Nichtzielorganismen wurden zu einem öffentlichen Thema, nachdem in einer Veröffentlichung aus dem Jahr 1999 darauf hingewiesen wurde, dass sie für Monarchfalter giftig sein könnten.
Angesichts der nun möglichen Möglichkeit, Menschen gentechnisch zu verändern, gibt es ethische Bedenken darüber, wie weit diese Technologie gehen soll oder ob sie überhaupt eingesetzt werden sollte.
Oktober 2006 die Strenge des Regulierungsprozesses, die Konsolidierung der Kontrolle der Lebensmittelversorgung in Unternehmen, die GVO herstellen und verkaufen, die Übertreibung der Vorteile genetischer Veränderungen oder Bedenken hinsichtlich der Verwendung von Herbiziden mit Glyphosat.
GVO kamen auf den Plan, als das Vertrauen der Öffentlichkeit in die Lebensmittelsicherheit aufgrund jüngster Lebensmittelskandale wie der bovinen spongiformen Enzephalopathie und anderer Skandale im Zusammenhang mit der staatlichen Produktregulierung in Europa gering war.
Gentechnik, auch genetische Veränderung oder Genmanipulation genannt, ist die direkte Manipulation der Gene eines Organismus mittels Biotechnologie.
Normalerweise wird ein Konstrukt erstellt und verwendet, um diese DNA in den Wirtsorganismus einzufügen.
Die neue DNA kann zufällig eingefügt werden oder gezielt auf einen bestimmten Teil des Genoms abzielen.
Rudolf Jaenisch schuf 1974 das erste gentechnisch veränderte Tier, als er einer Maus fremde DNA einführte.
Gentechnisch veränderte Lebensmittel werden seit 1994 mit der Einführung der Flavr-Savr-Tomate verkauft.
Im Jahr 2016 wurden mit einem Wachstumshormon modifizierte Lachse verkauft.
Durch das Ausschalten von Genen, die für bestimmte Erkrankungen verantwortlich sind, ist es möglich, tierische Modellorganismen für menschliche Krankheiten zu schaffen.
Der Aufstieg kommerzialisierter gentechnisch veränderter Pflanzen hat den Landwirten in vielen verschiedenen Ländern wirtschaftliche Vorteile gebracht, war aber auch die Quelle der meisten Kontroversen rund um diese Technologie.
Als potenzielle Probleme wurden auch der Genfluss, die Auswirkungen auf Nichtzielorganismen, die Kontrolle der Nahrungsmittelversorgung und die Rechte an geistigem Eigentum genannt.
Dies ist viel schneller, kann zum Einfügen beliebiger Gene aus jedem Organismus (auch aus unterschiedlichen Domänen) verwendet werden und verhindert, dass auch andere unerwünschte Gene hinzugefügt werden.
Medikamente, Impfstoffe und andere Produkte wurden aus Organismen gewonnen, die zu ihrer Herstellung entwickelt wurden.
Die synthetische Biologie ist eine aufstrebende Disziplin, die die Gentechnik einen Schritt weiter treibt, indem sie künstlich synthetisiertes Material in einen Organismus einbringt.
Wenn dem Wirt genetisches Material einer anderen Art hinzugefügt wird, wird der resultierende Organismus als transgen bezeichnet.
1973 schufen Herbert Boyer und Stanley Cohen den ersten transgenen Organismus, indem sie Antibiotikaresistenzgene in das Plasmid eines Escherichia coli-Bakteriums einfügten.
1976 wurde Genentech, das erste Gentechnikunternehmen, von Herbert Boyer und Robert Swanson gegründet und ein Jahr später produzierte das Unternehmen ein menschliches Protein (Somatostatin) in E. coli.
Das von Bakterien produzierte Insulin wurde 1982 von der Food and Drug Administration (FDA) zur Freisetzung zugelassen.
Die Volksrepublik China war das erste Land, das transgene Pflanzen kommerzialisierte und 1992 einen virusresistenten Tabak einführte.
Im Jahr 1995 wurde die Bt-Kartoffel von der Environmental Protection Agency als sicher eingestuft, nachdem sie bereits von der FDA zugelassen worden war. Damit war sie die erste Pestizid produzierende Nutzpflanze, die in den USA zugelassen wurde.
Genetische Screenings können durchgeführt werden, um potenzielle Gene zu bestimmen, und weitere Tests können dann verwendet werden, um die besten Kandidaten zu identifizieren.
Diese Segmente können dann durch Gelelektrophorese extrahiert werden.
Nach der Isolierung wird das Gen in ein Plasmid ligiert, das dann in ein Bakterium eingefügt wird.
Dazu gehören eine Promotor- und eine Terminatorregion, die die Transkription einleiten und beenden.
Diese Fähigkeit kann bei anderen Bakterien durch Stress (z. B. thermischer oder elektrischer Schock) induziert werden, wodurch die Durchlässigkeit der Zellmembran für DNA erhöht wird; Die aufgenommene DNA kann sich entweder in das Genom integrieren oder als extrachromosomale DNA vorliegen.
In Pflanzen wird die DNA häufig mithilfe einer Agrobacterium-vermittelten Transformation eingefügt, wobei die T-DNA-Sequenz des Agrobacteriums ausgenutzt wird, die die natürliche Einfügung von genetischem Material in Pflanzenzellen ermöglicht.
Bei Pflanzen wird dies durch den Einsatz von Gewebekulturen erreicht.
Selektierbare Marker werden verwendet, um transformierte von nicht transformierten Zellen leicht zu unterscheiden.
Diese Tests können auch die chromosomale Position und Kopienzahl des eingefügten Gens bestätigen.
Das neue genetische Material kann zufällig in das Wirtsgenom eingefügt oder gezielt an eine bestimmte Stelle gebracht werden.
Die Häufigkeit des Gen-Targetings kann durch Genombearbeitung erheblich gesteigert werden.
TALEN und CRISPR sind die beiden am häufigsten verwendeten und jede hat ihre eigenen Vorteile.
Bei den meisten kommerzialisierten GVO handelt es sich um insektenresistente oder herbizidtolerante Nutzpflanzen.
Maus-Hybridome, Zellen, die zur Bildung monoklonaler Antikörper verschmolzen sind, wurden durch Gentechnik angepasst, um menschliche monoklonale Antikörper zu erzeugen.
Gentechnik wird auch zur Erstellung von Tiermodellen für menschliche Krankheiten eingesetzt.
Potenzielle Heilmittel können anhand dieser Mausmodelle getestet werden.
Im Jahr 2015 wurde ein Virus verwendet, um ein gesundes Gen in die Hautzellen eines Jungen einzufügen, der an einer seltenen Hautkrankheit, der Epidermolysis bullosa, leidet, um dort zu wachsen, und dann gesunde Haut auf 80 Prozent des Körpers des Jungen zu transplantieren, der von der Epidermolysis bullosa betroffen war Erkrankung.
Es gibt auch Bedenken, dass die Technologie nicht nur zur Behandlung, sondern auch zur Verbesserung, Modifikation oder Veränderung des Aussehens, der Anpassungsfähigkeit, der Intelligenz, des Charakters oder des Verhaltens eines Menschen eingesetzt werden könnte.
Er sagte, dass die Zwillingsmädchen Lulu und Nana einige Wochen zuvor geboren worden seien.
Derzeit ist die Keimbahnveränderung in 40 Ländern verboten.
Bakterien sind günstig, leicht zu züchten, klonal, vermehren sich schnell, lassen sich relativ leicht umwandeln und können bei -80 °C nahezu unbegrenzt gelagert werden.
Dies könnte die Auswirkung auf den Phänotyp des Organismus sein, wo das Gen exprimiert wird oder mit welchen anderen Genen es interagiert.
Durch einen einfachen Knockout wurde eine Kopie des gewünschten Gens so verändert, dass es nicht mehr funktionsfähig ist.
Dadurch kann der Experimentator die durch diese Mutation verursachten Defekte analysieren und so die Rolle bestimmter Gene bestimmen.
Die einfachste und erste angewandte Methode ist das „Alanin-Scanning“, bei dem jede Position der Reihe nach zur unreaktiven Aminosäure Alanin mutiert wird.
Der Prozess ist weitgehend derselbe wie beim Knockout-Engineering, mit der Ausnahme, dass das Konstrukt darauf ausgelegt ist, die Funktion des Gens zu erhöhen, in der Regel durch die Bereitstellung zusätzlicher Kopien des Gens oder die häufigere Induktion der Proteinsynthese.
Eine Möglichkeit, dies zu erreichen, besteht darin, das Wildtyp-Gen durch ein „Fusions“-Gen zu ersetzen, bei dem es sich um eine Gegenüberstellung des Wildtyp-Gens mit einem Berichtselement wie dem grün fluoreszierenden Protein (GFP) handelt, das eine einfache Visualisierung der Produkte ermöglicht der genetischen Veränderung.
Expressionsstudien zielen darauf ab, herauszufinden, wo und wann bestimmte Proteine produziert werden.
Einige Gene funktionieren in Bakterien nicht gut, daher können auch Hefe-, Insekten- oder Säugetierzellen verwendet werden.
Bestimmte gentechnisch veränderte Mikroben können aufgrund ihrer Fähigkeit, Schwermetalle aus ihrer Umgebung zu extrahieren und sie in Verbindungen einzubauen, die leichter rückgewinnbar sind, auch beim Bioabbau und der Bioremediation eingesetzt werden.
Auch pilz- und virusresistente Pflanzen wurden entwickelt oder befinden sich in der Entwicklung.
Im Jahr 2016 wurden Lachse mit Wachstumshormonen gentechnisch verändert, um viel schneller die normale Erwachsenengröße zu erreichen.
Sojabohnen und Raps wurden gentechnisch verändert, um gesündere Öle herzustellen.
Der Gentransfer durch virale Vektoren wurde als Mittel zur Bekämpfung invasiver Arten sowie zur Impfung bedrohter Tiere vor Krankheiten vorgeschlagen.
Anwendungen der Gentechnik im Naturschutz sind bisher meist theoretisch und müssen noch in die Praxis umgesetzt werden.
Auf dem Asilomar-Treffen wurde eine Reihe freiwilliger Richtlinien für den Einsatz rekombinanter Technologie empfohlen.
157 Länder sind Mitglieder des Protokolls und viele nutzen es als Bezugspunkt für ihre eigenen Vorschriften.
Die meisten Länder, die den GVO-Anbau nicht zulassen, erlauben die Forschung.
Emily Marden, Risiko und Regulierung: U.S. Regulatory Policy on Genetically Modified Food and Agriculture, 44 v. Chr. Rev. 733 (2003) Im Gegensatz dazu verfügt die Europäische Union möglicherweise über die strengsten GVO-Vorschriften der Welt.
Eine der zentralen Fragen der Regulierungsbehörden ist die Frage, ob gentechnisch veränderte Produkte gekennzeichnet werden sollten.
Diese Kontroversen haben zu Rechtsstreitigkeiten, internationalen Handelsstreitigkeiten und Protesten sowie in einigen Ländern zu einer restriktiven Regulierung kommerzieller Produkte geführt.
Obwohl Zweifel geäußert wurden, haben die meisten Studien ergeben, dass der Anbau gentechnisch veränderter Pflanzen wirtschaftlich vorteilhaft für die Landwirte ist.
Es kann viele Jahre dauern, bis viele der Umweltauswirkungen gentechnisch veränderter Pflanzen verstanden werden, und sie sind auch in konventionellen landwirtschaftlichen Praktiken erkennbar.
Nur wenige Filme haben das Publikum über Gentechnik informiert, mit Ausnahme von „The Boys from Brazil“ von 1978 und „Jurassic Park“ von 1993, die beide eine Lektion, eine Demonstration und einen wissenschaftlichen Filmausschnitt nutzten.
Unter Nanotechnologie, auch Nanotechnologie genannt, versteht man die Nutzung von Materie auf atomarer, molekularer und supramolekularer Ebene für industrielle Zwecke.
Diese Definition spiegelt die Tatsache wider, dass quantenmechanische Effekte auf dieser Quantenbereichsskala wichtig sind, und so verlagerte sich die Definition von einem bestimmten technologischen Ziel zu einer Forschungskategorie, die alle Arten von Forschung und Technologien umfasst, die sich mit den auftretenden besonderen Eigenschaften der Materie befassen unterhalb der angegebenen Größenschwelle liegen.
Die damit verbundenen Forschungen und Anwendungen sind ebenso vielfältig und reichen von Erweiterungen der konventionellen Gerätephysik bis hin zu völlig neuen Ansätzen, die auf molekularer Selbstorganisation basieren, von der Entwicklung neuer Materialien mit Dimensionen im Nanomaßstab bis hin zur direkten Kontrolle von Materie auf atomarer Skala.
Der Begriff „Nanotechnologie“ wurde erstmals 1974 von Norio Taniguchi verwendet, war jedoch nicht allgemein bekannt.
Die Entstehung der Nanotechnologie als Fachgebiet in den 1980er Jahren erfolgte durch die Konvergenz von Drexlers theoretischen und öffentlichen Arbeiten, die einen konzeptionellen Rahmen für die Nanotechnologie entwickelten und populär machten, und öffentlichkeitswirksamen experimentellen Fortschritten, die zusätzliche breite Aufmerksamkeit auf die Aussichten der atomaren Kontrolle lenkten Gegenstand.
Die Mikroskopentwickler Gerd Binnig und Heinrich Rohrer vom IBM Forschungslabor Zürich erhielten 1986 den Nobelpreis für Physik.
C60 wurde ursprünglich nicht als Nanotechnologie beschrieben; Der Begriff wurde für spätere Arbeiten mit verwandten Kohlenstoffnanoröhren (manchmal auch Graphenröhren oder Bucky-Röhren genannt) verwendet, die mögliche Anwendungen für nanoskalige Elektronik und Geräte nahelegten.
Jahrzehnte später ermöglichten Fortschritte in der Multi-Gate-Technologie die Skalierung von Metall-Oxid-Halbleiter-Feldeffekttransistoren (MOSFETs) bis hin zu Nanomaßstäben mit einer Gate-Länge von weniger als 20 nm, beginnend mit dem FinFET (Fin-Feldeffekttransistor). , ein dreidimensionaler, nichtplanarer Doppelgate-MOSFET.
Es kam zu Kontroversen hinsichtlich der Definitionen und möglichen Auswirkungen von Nanotechnologien, wie beispielsweise der Bericht der Royal Society über Nanotechnologie zeigt.
Diese Produkte sind auf Massenanwendungen von Nanomaterialien beschränkt und erfordern keine atomare Kontrolle der Materie.
Es basierte auf der Gate-Allround-FinFET-Technologie (GAA).
Dies umfasst sowohl aktuelle Arbeiten als auch fortgeschrittenere Konzepte.
Die untere Grenze wird durch die Größe der Atome bestimmt (Wasserstoff hat die kleinsten Atome, die einen kinetischen Durchmesser von etwa einem Viertel Nanometer haben), da die Nanotechnologie ihre Geräte aus Atomen und Molekülen aufbauen muss.
Um diese Skala in einen anderen Kontext zu stellen: Die Vergleichsgröße eines Nanometers zu einem Meter ist die gleiche wie die einer Murmel zur Größe der Erde.
Beim „Bottom-up“-Ansatz werden Materialien und Geräte aus molekularen Komponenten aufgebaut, die sich durch Prinzipien der molekularen Erkennung chemisch zusammensetzen.
Ein Beispiel ist die Vergrößerung des Verhältnisses von Oberfläche zu Volumen, die die mechanischen, thermischen und katalytischen Eigenschaften von Materialien verändert.
Die katalytische Aktivität von Nanomaterialien birgt auch potenzielle Risiken in ihrer Wechselwirkung mit Biomaterialien.
Besonders wichtig ist das Konzept der molekularen Erkennung: Moleküle können so gestaltet werden, dass eine bestimmte Konfiguration oder Anordnung aufgrund nichtkovalenter zwischenmolekularer Kräfte begünstigt wird.
Solche Bottom-up-Ansätze sollten in der Lage sein, Geräte parallel zu produzieren und viel kostengünstiger als Top-down-Methoden zu sein, könnten jedoch möglicherweise überfordert sein, wenn die Größe und Komplexität der gewünschten Baugruppe zunimmt.
Die Herstellung im Kontext produktiver Nanosysteme steht in keinem Zusammenhang mit den herkömmlichen Technologien zur Herstellung von Nanomaterialien wie Kohlenstoffnanoröhren und Nanopartikeln und sollte klar von diesen unterschieden werden.
Es besteht die Hoffnung, dass die Entwicklungen in der Nanotechnologie ihre Konstruktion auf andere Weise, möglicherweise unter Verwendung biomimetischer Prinzipien, ermöglichen werden.
Im Allgemeinen ist es sehr schwierig, Geräte auf atomarer Ebene zusammenzubauen, da man Atome auf anderen Atomen vergleichbarer Größe und Klebrigkeit positionieren muss.
Dies führte 2003 zu einem Briefwechsel in der ACS-Publikation Chemical & Engineering News.
Sie haben mindestens drei verschiedene molekulare Geräte konstruiert, deren Bewegung vom Schreibtisch aus mit wechselnder Spannung gesteuert wird: einen Nanoröhren-Nanomotor, einen molekularen Aktuator und einen nanoelektromechanischen Relaxationsoszillator.
Nanomaterialien mit schnellem Ionentransport hängen auch mit der Nanoionik und Nanoelektronik zusammen.
Nanoskalige Materialien wie Nanosäulen werden manchmal in Solarzellen verwendet, was die Kosten herkömmlicher Siliziumsolarzellen senkt.
Im Allgemeinen versucht die molekulare Selbstorganisation, Konzepte der supramolekularen Chemie und insbesondere der molekularen Erkennung zu nutzen, um einzelne Molekülkomponenten dazu zu bringen, sich automatisch in einer nützlichen Konformation anzuordnen.
Zu dieser Beschreibung passen bereits auf dem Markt erhältliche riesige Magnetowiderstands-Festplatten, ebenso wie ALD-Techniken (Atomic Layer Deposition).
Fokussierte Ionenstrahlen können Material direkt abtragen oder bei gleichzeitiger Anwendung geeigneter Vorläufergase sogar Material abscheiden.
Diese könnten dann als Einzelmolekülkomponenten in einem nanoelektronischen Gerät verwendet werden.
Die molekulare Nanotechnologie ist ein vorgeschlagener Ansatz, bei dem einzelne Moleküle auf fein kontrollierte, deterministische Weise manipuliert werden.
Es gibt Hoffnungen auf den Einsatz von Nanorobotern in der Medizin.
Aufgrund der diskreten (d. h. atomaren) Natur der Materie und der Möglichkeit eines exponentiellen Wachstums wird dieses Stadium als Grundlage einer weiteren industriellen Revolution angesehen.
Mit der Abnahme der Dimensionalität wird eine Zunahme des Oberflächen-Volumen-Verhältnisses beobachtet.
Obwohl sie konzeptionell dem von Marvin Minsky 1961 entwickelten konfokalen Rastermikroskop und dem in den 1970er Jahren von Calvin Quate und Mitarbeitern entwickelten akustischen Rastermikroskop (SAM) ähneln, haben neuere Rastersondenmikroskope eine viel höhere Auflösung, da sie nicht durch die Wellenlänge begrenzt sind Ton oder Licht.
Aufgrund der geringen Scangeschwindigkeit des Mikroskops ist dies jedoch immer noch ein langsamer Prozess.
Zu einer weiteren Gruppe nanotechnologischer Techniken gehören jene, die zur Herstellung von Nanoröhren und Nanodrähten verwendet werden, jene, die in der Halbleiterfertigung verwendet werden, wie z. B. Tief-Ultraviolett-Lithographie, Elektronenstrahl-Lithographie, fokussierte Ionenstrahlbearbeitung, Nanopräge-Lithographie, Atomlagenabscheidung und molekulare Gasphasenabscheidung und weitere molekulare Selbstorganisationstechniken, beispielsweise solche, die Diblockcopolymere verwenden.
Die Rastersondenmikroskopie ist eine wichtige Technik sowohl zur Charakterisierung als auch zur Synthese von Nanomaterialien.
Durch die Verwendung beispielsweise eines merkmalsorientierten Scan-Ansatzes können Atome oder Moleküle mithilfe von Rastersondenmikroskopietechniken auf einer Oberfläche bewegt werden.
Zu diesen Techniken gehören chemische Synthese, Selbstorganisation und Positionsanordnung.
Forscher der Bell Telephone Laboratories wie John R. Arthur.
Mit MBE können Wissenschaftler atomar präzise Schichten von Atomen ablegen und dabei komplexe Strukturen aufbauen.
Um Schnittwunden schneller heilen zu lassen, werden Bandagen mit Silber-Nanopartikeln angereichert.
Die Nanotechnologie könnte dazu führen, dass bestehende medizinische Anwendungen kostengünstiger und einfacher einsetzbar werden, etwa in der Hausarztpraxis oder zu Hause.
In diesen Motoren wird derzeit Platin als Dieselkatalysator eingesetzt.
Anschließend oxidiert der Oxidationskatalysator die Kohlenwasserstoffe und Kohlenmonoxid zu Kohlendioxid und Wasser.
Das dänische Unternehmen InnovationsFonden investierte 15 Millionen DKK in die Suche nach neuen Katalysatorersatzstoffen mithilfe der Nanotechnologie.
Wenn die Oberfläche des Katalysators, die den Abgasen ausgesetzt ist, maximiert wird, wird die Effizienz des Katalysators maximiert.
Somit wird die Herstellung dieser Nanopartikel die Wirksamkeit des resultierenden Katalysators für Dieselmotoren erhöhen – was wiederum zu saubereren Abgasen führt – und die Kosten senken.
Beim Entwurf von Gerüsten versuchen Forscher, die nanoskaligen Merkmale der Mikroumgebung einer Zelle nachzuahmen, um deren Differenzierung in eine geeignete Abstammungslinie zu lenken.
TSMC begann 2017 mit der Produktion eines 7-nm-Prozesses und Samsung begann 2018 mit der Produktion eines 5-nm-Prozesses.
Aus diesen Gründen befürworten einige Gruppen eine staatliche Regulierung der Nanotechnologie.
Einige Nanopartikelprodukte können unbeabsichtigte Folgen haben.
Das Einatmen von Nanopartikeln und Nanofasern in der Luft kann zu einer Reihe von Lungenerkrankungen führen, z. Fibrose.
Eine kürzlich in Nature Nanotechnology veröffentlichte große Studie legt nahe, dass einige Formen von Kohlenstoffnanoröhren – ein Aushängeschild der „Nanotechnologie-Revolution“ – genauso schädlich sein könnten wie Asbest, wenn sie in ausreichenden Mengen eingeatmet werden.
Davies (2008) hat einen Regulierungsfahrplan vorgeschlagen, der Schritte zur Behebung dieser Mängel beschreibt.
Aus diesem Grund haben einige Wissenschaftler eine strengere Anwendung des Vorsorgeprinzips gefordert, mit verzögerter Marktzulassung, verbesserter Kennzeichnung und zusätzlichen Anforderungen an die Entwicklung von Sicherheitsdaten in Bezug auf bestimmte Formen der Nanotechnologie.
Nukleartechnik ist eine Technologie, die Kernreaktionen von Atomkernen beinhaltet.
Er, Pierre Curie und Marie Curie begannen, das Phänomen zu untersuchen.
Einige dieser Arten von Strahlung könnten gewöhnliche Materie durchdringen und alle könnten in großen Mengen schädlich sein.
Nach und nach erkannte man, dass es sich bei der durch den radioaktiven Zerfall erzeugten Strahlung um ionisierende Strahlung handelte und dass selbst Mengen, die zum Verbrennen zu klein waren, langfristig eine ernsthafte Gefahr darstellen konnten.
Je besser man das Atom verstand, desto klarer wurde die Natur der Radioaktivität.
Beim Alpha-Zerfall setzt ein Kern ein Alpha-Teilchen frei, das aus zwei Protonen und zwei Neutronen besteht, was einem Heliumkern entspricht.
Diese Art von Strahlung ist die gefährlichste und am schwierigsten zu blockierende.
Die durchschnittliche Anzahl der pro Kern freigesetzten Neutronen, die anschließend einen anderen Kern spalten, wird als k bezeichnet. Werte von k größer als 1 bedeuten, dass die Spaltreaktion mehr Neutronen freisetzt als absorbiert und daher als selbsterhaltende Kettenreaktion bezeichnet wird.
Wenn es genügend unmittelbare Zerfälle gibt, um die Kettenreaktion fortzusetzen, wird die Masse als sofort kritisch bezeichnet und die Energiefreisetzung wird schnell und unkontrolliert ansteigen, was normalerweise zu einer Explosion führt.
Im Rahmen des Projekts wurden auch die ersten Kernspaltungsreaktoren entwickelt, die jedoch hauptsächlich der Waffenherstellung dienten und keinen Strom erzeugten.
Ist die Masse jedoch nur bei Einbeziehung der verzögerten Neutronen kritisch, kann die Reaktion kontrolliert werden, beispielsweise durch das Einbringen oder Entfernen von Neutronenabsorbern.
Wenn der resultierende Kern leichter als der von Eisen ist, wird normalerweise Energie freigesetzt; Wenn der Kern schwerer ist als der von Eisen, wird im Allgemeinen Energie absorbiert.
Der verbleibende Reichtum an schweren Elementen, von Nickel bis Uran und darüber hinaus, ist auf die Supernova-Nukleosynthese, den R-Prozess, zurückzuführen.
Wasserstoffbomben erhalten ihre enorme Zerstörungskraft durch Fusion, ihre Energie ist jedoch nicht kontrollierbar.
Allerdings arbeiten beide Geräte mit einem Nettoenergieverlust.
Die Kernfusion wurde während des Zweiten Weltkriegs zunächst nur in theoretischen Phasen verfolgt, als Wissenschaftler des Manhattan-Projekts (unter der Leitung von Edward Teller) sie als Methode zum Bau einer Bombe untersuchten.
Selbst kleine Atombomben können eine Stadt durch Explosion, Feuer und Strahlung zerstören.
Eine solche Waffe muss eine oder mehrere unterkritische spaltbare Massen für den Einsatz stabil halten und dann für die Detonation Kritikalität induzieren (eine kritische Masse erzeugen).
Ein Uranisotop, nämlich Uran-235, kommt natürlich vor und ist ausreichend instabil, es kommt jedoch immer gemischt mit dem stabileren Isotop Uran-238 vor.
Alternativ besitzt das Element Plutonium ein Isotop, das ausreichend instabil ist, um diesen Prozess nutzen zu können.
Am 16. Juli 1945 zündeten sie in einem Test mit dem Codenamen „Trinity“ in der Nähe von Alamogordo, New Mexico, die erste Atomwaffe.
Nach beispiellosen Zerstörungen und Verlusten durch eine einzige Waffe kapitulierte die japanische Regierung bald und beendete den Zweiten Weltkrieg.
Etwas mehr als vier Jahre später, am 29. August 1949, zündete die Sowjetunion ihre erste Spaltwaffe.
Eine radiologische Waffe ist eine Art Atomwaffe, die dazu bestimmt ist, gefährliches Nuklearmaterial in feindlichen Gebieten zu verteilen.
Obwohl eine solche Waffe von einem konventionellen Militär als nutzlos angesehen wird, gibt sie Anlass zur Sorge hinsichtlich des nuklearen Terrorismus.
Der Vertrag erlaubte unterirdische Atomtests.
Nach der Unterzeichnung des Vertrags über das umfassende Verbot von Atomtests im Jahr 1996 (der 2011 noch nicht in Kraft getreten war) haben sich alle diese Staaten verpflichtet, alle Atomtests einzustellen.
Während des Kalten Krieges verfügten die gegnerischen Mächte über riesige Atomwaffenarsenale, die ausreichten, um Hunderte Millionen Menschen zu töten.
Derzeit liefert die Kernenergie etwa 15,7 % des weltweiten Stroms (im Jahr 2004) und wird zum Antrieb von Flugzeugträgern, Eisbrechern und U-Booten verwendet (bisher haben wirtschaftliche Gründe und Ängste in einigen Häfen den Einsatz von Atomkraft in Transportschiffen verhindert).
Medizinische und zahnmedizinische Röntgenbildgeräte verwenden Kobalt-60 oder andere Röntgenquellen.
Beide enthalten eine kleine Quelle von 241 Am, die einen kleinen Konstantstrom erzeugt.
Eine weitere Anwendung bei der Insektenbekämpfung ist die Technik der sterilen Insekten, bei der männliche Insekten durch Strahlung sterilisiert und freigelassen werden, sodass sie keine Nachkommen haben, um die Population zu reduzieren.
Zu den verwendeten Strahlungsquellen gehören Radioisotop-Gammastrahlenquellen, Röntgengeneratoren und Elektronenbeschleuniger.
Daher wird es auch für Non-Food-Artikel wie medizinische Hardware, Kunststoffe, Rohre für Gasleitungen, Schläuche für Fußbodenheizungen, Schrumpffolien für Lebensmittelverpackungen, Autoteile, Drähte und Kabel (Isolierung), Reifen usw. verwendet. und sogar Edelsteine.
Mikroorganismen können sich nicht mehr vermehren und ihre bösartigen oder pathogenen Aktivitäten fortsetzen.
Pflanzen können den natürlichen Reife- oder Alterungsprozess nicht fortsetzen.
Das Besondere an der Verarbeitung von Lebensmitteln durch ionisierende Strahlung ist die Tatsache, dass die Energiedichte pro Atomübergang sehr hoch ist. Dadurch können Moleküle gespalten und eine Ionisierung induziert werden (daher der Name), die durch bloßes Erhitzen nicht erreicht werden kann.
Allerdings ist die Verwendung des Begriffs Kaltpasteurisierung zur Beschreibung bestrahlter Lebensmittel umstritten, da es sich bei Pasteurisierung und Bestrahlung um grundsätzlich unterschiedliche Verfahren handelt, obwohl die angestrebten Endergebnisse in manchen Fällen ähnlich sein können.
Marie Curie starb an aplastischer Anämie, die auf ihre hohe Exposition zurückzuführen war.
Ungefähr die Hälfte der Todesfälle in Hiroshima und Nagasaki starben zwei bis fünf Jahre später an den Folgen der Strahlenexposition.
Unter einer Kernschmelze versteht man die größere Gefahr, dass Kernmaterial in die Umgebung freigesetzt wird.
Militärreaktoren, bei denen es zu ähnlichen Unfällen kam, waren Windscale im Vereinigten Königreich und SL-1 in den Vereinigten Staaten.
Ein weiteres Thema der transhumanistischen Forschung ist der Schutz der Menschheit vor existenziellen Risiken wie einem Atomkrieg oder einem Asteroidenzusammenstoß.
Diese Behauptung sollte den intellektuellen Grundstein dafür legen, dass der britische Philosoph Max More 1990 damit begann, die Prinzipien des Transhumanismus als futuristische Philosophie zu artikulieren und in Kalifornien eine Denkschule zu organisieren, die sich seitdem zur weltweiten transhumanistischen Bewegung entwickelt hat.
Im Diskurs stellte sich Descartes eine neue Art von Medizin vor, die sowohl körperliche Unsterblichkeit als auch einen stärkeren Geist verleihen könnte.
St. Leon diente möglicherweise als Inspiration für den Roman Frankenstein seiner Tochter Mary Shelley.
Sein besonderes Interesse galt der Entwicklung der Wissenschaft der Eugenik, der Ektogenese (Erschaffung und Erhaltung von Leben in einer künstlichen Umgebung) und der Anwendung der Genetik zur Verbesserung menschlicher Eigenschaften wie Gesundheit und Intelligenz.
Diese Ideen sind seitdem gängige transhumanistische Themen.
Im Abschnitt „Material und Mensch“ des Manifests schlägt Noboru Kawazoe Folgendes vor: „Nach einigen Jahrzehnten wird mit dem rasanten Fortschritt der Kommunikationstechnologie jeder einen „Gehirnwellenempfänger“ im Ohr haben, der direkt und genau übermittelt, was andere Menschen denken über ihn und umgekehrt.
Im Jahr 1966 begann FM-2030 (ehemals F. M. Esfandiary), ein Futurist, der „neue Konzepte des Menschen“ an der New School in New York City lehrte, Menschen, die Technologien, Lebensstile und Weltanschauungen im Übergang zur Posthumanität übernehmen, als „ transhuman".
FM-2030 und Vita-More begannen bald damit, Treffen für Transhumanisten in Los Angeles abzuhalten, zu denen auch Studenten der FM-2030-Kurse und Publikum der künstlerischen Produktionen von Vita-More gehörten.
Ein besonderes Anliegen ist der gleichberechtigte Zugang zu Human-Enhancement-Technologien über Klassen und Grenzen hinweg.
Damit war die World Transhumanist Association die führende internationale transhumanistische Organisation.
Die Mormon Transhumanist Association wurde 2006 gegründet.
Der Transhumanismus betont die evolutionäre Perspektive, einschließlich manchmal der Schaffung einer hochintelligenten Tierart durch kognitive Verbesserung (d. h. biologischen Aufstieg), hält aber an einer „posthumanen Zukunft“ als Endziel der teilnehmenden Evolution fest.
Während ein solcher „kultureller Posthumanismus“ Ressourcen für ein Neudenken der Beziehungen zwischen Menschen und immer ausgefeilteren Maschinen bieten würde, geben Transhumanismus und ähnliche Posthumanismen aus dieser Sicht nicht veraltete Konzepte des „autonomen liberalen Subjekts“ auf, sondern erweitern dessen „Vorrechte“. in den Bereich des Posthumanen.
Andere Progressive haben jedoch argumentiert, dass der Posthumanismus, sei es in seinen philosophischen oder aktivistischen Formen, einer Abkehr von Sorgen um soziale Gerechtigkeit, von der Reform menschlicher Institutionen und anderen Anliegen der Aufklärung hin zu narzisstischen Sehnsüchten nach einer Transzendenz des Menschlichen gleichkommt Körper auf der Suche nach exquisiteren Seinsweisen.
Viele Transhumanisten prüfen aktiv das Potenzial zukünftiger Technologien und innovativer sozialer Systeme zur Verbesserung der Lebensqualität allen Lebens und versuchen gleichzeitig, die materielle Realität der menschlichen Existenz durch die Beseitigung angeborener geistiger und körperlicher Barrieren dem Versprechen rechtlicher und politischer Gleichheit zu erfüllen.
Einige Theoretiker wie Ray Kurzweil glauben, dass sich das Tempo der technologischen Innovation beschleunigt und dass die nächsten 50 Jahre nicht nur radikale technologische Fortschritte, sondern möglicherweise auch eine technologische Singularität hervorbringen könnten, die die Natur des Menschen grundlegend verändern könnte.
Bostrom hat beispielsweise ausführlich über existenzielle Risiken für das zukünftige Wohlergehen der Menschheit geschrieben, einschließlich solcher, die durch neue Technologien entstehen könnten.
Um dem entgegenzuwirken, betont Hawking entweder die Selbstgestaltung des menschlichen Genoms oder mechanische Verbesserungen (z. B. Gehirn-Computer-Schnittstelle), um die menschliche Intelligenz zu verbessern und Aggression zu reduzieren. Ohne diese impliziert er, dass die menschliche Zivilisation insgesamt möglicherweise zu dumm ist, um in einem zunehmend instabilen System zu überleben , was zum Zusammenbruch der Gesellschaft führte.
Diese Denker argumentieren, dass die Fähigkeit, auf Falsifikationsbasis zu diskutieren, eine nicht willkürliche Schwelle darstellt, ab der es einem Individuum möglich wird, für sich selbst auf eine Weise zu sprechen, die nicht von äußeren Annahmen abhängig ist.
Dementsprechend bezeichnen viele prominente transhumanistische Befürworter, wie etwa Dan Agin, die Kritiker des Transhumanismus, sowohl auf der politischen Rechten als auch auf der Linken, als „Biokonservative“ oder „Bioludditen“, wobei letzterer Begriff auf die soziale Bewegung gegen die Industrialisierung des 19. Jahrhunderts anspielt die sich gegen die Ersetzung menschlicher Arbeiter durch Maschinen aussprach.
Das gleiche Szenario passiert, wenn Menschen bestimmte Nervenimplantate erhalten, die ihnen am Arbeitsplatz und in Bildungsaspekten einen Vorteil verschaffen.
Immortalismus, eine moralische Ideologie, die auf der Überzeugung basiert, dass radikale Lebensverlängerung und technologische Unsterblichkeit möglich und wünschenswert sind, und die sich für Forschung und Entwicklung einsetzt, um ihre Verwirklichung sicherzustellen.
Mathematik (aus dem Griechischen: ) umfasst das Studium von Themen wie Quantität (Zahlentheorie), Struktur (Algebra), Raum (Geometrie) und Veränderung (Analyse).
Wenn mathematische Strukturen gute Modelle realer Phänomene sind, können mathematische Überlegungen genutzt werden, um Erkenntnisse oder Vorhersagen über die Natur zu liefern.
Die zur Lösung mathematischer Probleme erforderliche Forschung kann Jahre oder sogar Jahrhunderte dauernder Forschung erfordern.
Die Mathematik entwickelte sich bis zur Renaissance relativ langsam, als mathematische Innovationen im Zusammenspiel mit neuen wissenschaftlichen Entdeckungen zu einem rasanten Anstieg der mathematischen Entdeckungsrate führten, der bis heute anhält.
Wie Zählungen auf Knochen belegen, wussten prähistorische Völker möglicherweise nicht nur, wie man physische Objekte zählt, sondern auch, wie man abstrakte Größen wie Zeit – Tage, Jahreszeiten oder Jahre – zählt.
Beginnend im 6. Jahrhundert v. Chr. begannen die alten Griechen mit den Pythagoräern und der griechischen Mathematik mit einem systematischen Studium der Mathematik als eigenständigem Fach.
Der größte Mathematiker der Antike wird oft als Archimedes (ca. 287–212 v. Chr.) von Syrakus angesehen.
Das hindu-arabische Zahlensystem und die Regeln für die Verwendung seiner Operationen, die heute auf der ganzen Welt verwendet werden, entwickelten sich im Laufe des ersten Jahrtausends n. Chr. in Indien und wurden über die islamische Mathematik in die westliche Welt übertragen.
Die bemerkenswerteste Errungenschaft der islamischen Mathematik war die Entwicklung der Algebra.
In der frühen Neuzeit begann sich die Mathematik in Westeuropa immer schneller zu entwickeln.
Der vielleicht bedeutendste Mathematiker des 19. Jahrhunderts war der deutsche Mathematiker Carl Friedrich Gauß, der zahlreiche Beiträge zu Gebieten wie Algebra, Analysis, Differentialgeometrie, Matrixtheorie, Zahlentheorie und Statistik leistete.
Auch heute noch werden mathematische Entdeckungen gemacht.
Insbesondere bedeutete mathēmatikḗ tékhnē „die mathematische Kunst“.
Im Englischen nimmt das Substantiv „mathematik“ ein Verb im Singular an.
Allerdings wies Aristoteles auch darauf hin, dass die Konzentration auf die Quantität allein die Mathematik möglicherweise nicht von Wissenschaften wie der Physik unterscheidet; Seiner Ansicht nach zeichnet sich die Mathematik durch die Abstraktion und das Studium der Quantität als einer Eigenschaft aus, die „in Gedanken von realen Instanzen trennbar“ ist.
Eine Besonderheit des Intuitionismus besteht darin, dass er einige mathematische Ideen ablehnt, die nach anderen Definitionen als gültig gelten.
Haskell Curry definierte Mathematik einfach als „die Wissenschaft der formalen Systeme“.
Popper bemerkte auch: „Ich werde ein System sicherlich nur dann als empirisch oder wissenschaftlich anerkennen, wenn es durch Erfahrung überprüft werden kann.“
Auch Intuition und Experimentierfreude spielen bei der Formulierung von Vermutungen sowohl in der Mathematik als auch in den (anderen) Naturwissenschaften eine Rolle.
Beispielsweise erfand der Physiker Richard Feynman die Pfadintegralformulierung der Quantenmechanik mithilfe einer Kombination aus mathematischen Überlegungen und physikalischen Erkenntnissen, und die heutige Stringtheorie, eine sich noch in der Entwicklung befindliche wissenschaftliche Theorie, die versucht, die vier Grundkräfte der Natur zu vereinen, inspiriert weiterhin Neue Mathematik.
Oft wird zwischen reiner Mathematik und angewandter Mathematik unterschieden.
Wie in den meisten Studienbereichen hat die Wissensexplosion im wissenschaftlichen Zeitalter zu einer Spezialisierung geführt: Mittlerweile gibt es in der Mathematik Hunderte von Spezialgebieten, und die neueste Fachklassifikation der Mathematik umfasst 46 Seiten.
Viele Mathematiker sprechen von der Eleganz der Mathematik, ihrer inneren Ästhetik und inneren Schönheit.
G. H. Hardy brachte in „A Mathematician's Apology“ die Überzeugung zum Ausdruck, dass diese ästhetischen Überlegungen an sich ausreichen, um das Studium der reinen Mathematik zu rechtfertigen.
Ein Satz, der als Charakterisierung des Objekts anhand dieser Merkmale ausgedrückt wird, ist der Preis.
Euler (1707–1783) war für viele der heute gebräuchlichen Notationen verantwortlich.
Im Gegensatz zur natürlichen Sprache, in der Menschen ein Wort (z. B. Kuh) oft mit dem physischen Objekt, dem es entspricht, gleichsetzen können, sind mathematische Symbole abstrakt und weisen kein physisches Analogon auf.
Zur mathematischen Sprache gehören auch viele Fachbegriffe wie Homöomorphismus und Integrierbarkeit, die außerhalb der Mathematik keine Bedeutung haben.
Mathematiker bezeichnen diese Präzision von Sprache und Logik als „Strenge“.
Damit sollen falsche „Theoreme“ vermieden werden, die auf fehlbaren Intuitionen beruhen und in der Geschichte des Fachs häufig vorkommen.
Ein Missverständnis der Strenge ist eine Ursache für einige der häufigsten Missverständnisse der Mathematik.
Andererseits ermöglichen Beweisassistenten die Überprüfung aller Details, die in einem handschriftlichen Beweis nicht angegeben werden können, und geben Gewissheit über die Richtigkeit langer Beweise wie des Feit-Thompson-Theorems.
Zusätzlich zu diesen Hauptanliegen gibt es auch Unterabteilungen, die sich mit der Erforschung von Verbindungen vom Kern der Mathematik zu anderen Bereichen befassen: zur Logik, zur Mengenlehre (Grundlagen), zur empirischen Mathematik der verschiedenen Wissenschaften (angewandte Mathematik) und in jüngerer Zeit zur rigorosen Untersuchung der Unsicherheit.
Einige Meinungsverschiedenheiten über die Grundlagen der Mathematik bestehen bis heute fort.
Als solches ist es die Heimat von Gödels Unvollständigkeitstheoremen, die (informell) implizieren, dass jedes effektive formale System, das grundlegende Arithmetik enthält, wenn es vernünftig ist (was bedeutet, dass alle Theoreme, die bewiesen werden können, wahr sind), notwendigerweise unvollständig ist (was bedeutet, dass es wahre Theoreme gibt). was in diesem System nicht bewiesen werden kann).
Die moderne Logik gliedert sich in Rekursionstheorie, Modelltheorie und Beweistheorie und ist eng mit der theoretischen Informatik sowie der Kategorientheorie verbunden.
Die Berechenbarkeitstheorie untersucht die Grenzen verschiedener theoretischer Modelle des Computers, darunter das bekannteste Modell – die Turing-Maschine.
Die Betrachtung der natürlichen Zahlen führt auch zu den transfiniten Zahlen, die den Begriff „Unendlichkeit“ formalisieren.
So kann man Gruppen, Ringe, Felder und andere abstrakte Systeme studieren; Zusammen bilden solche Studien (für durch algebraische Operationen definierte Strukturen) den Bereich der abstrakten Algebra.
Die Trigonometrie ist der Zweig der Mathematik, der sich mit den Beziehungen zwischen den Seiten und Winkeln von Dreiecken sowie mit den trigonometrischen Funktionen befasst.
Konvexe und diskrete Geometrie wurden zur Lösung von Problemen in der Zahlentheorie und der Funktionalanalyse entwickelt, werden heute jedoch mit Blick auf Anwendungen in der Optimierung und Informatik verfolgt.
Lügengruppen werden verwendet, um Raum, Struktur und Veränderung zu untersuchen.
Funktionen treten hier als zentraler Begriff auf, der eine sich ändernde Größe beschreibt.
Eine von vielen Anwendungen der Funktionalanalyse ist die Quantenmechanik.
Statistiker (die im Rahmen eines Forschungsprojekts arbeiten) „erstellen sinnvolle Daten“ mit Zufallsstichproben und randomisierten Experimenten; Der Entwurf einer statistischen Stichprobe oder eines Experiments legt die Analyse der Daten fest (bevor die Daten verfügbar werden).
Numerische Analyse untersucht Methoden für Analyseprobleme unter Verwendung von Funktionsanalyse und Approximationstheorie; Die numerische Analyse umfasst das Studium der Approximation und Diskretisierung im Großen und Ganzen mit besonderem Augenmerk auf Rundungsfehler.
Die Chern-Medaille wurde 2010 zur Anerkennung von Lebensleistungen eingeführt.
Diese Liste erlangte unter Mathematikern große Berühmtheit, und mindestens neun der Probleme wurden inzwischen gelöst.
Der Wert von Pi wurde zuerst von ihm berechnet.
Es waren die Pythagoräer, die den Begriff „Mathematik“ prägten und mit denen das Studium der Mathematik um ihrer selbst willen begann.
Aufgrund eines politischen Streits bestrafte die christliche Gemeinde in Alexandria sie in der Annahme, sie sei darin verwickelt, indem sie sie nackt auszog und ihr mit Muscheln (manche sagen Dachziegel) die Haut abkratzte.
Während der gesamten Regierungszeit bestimmter Kalifen wurden Mittel für die Übersetzung wissenschaftlicher Texte in andere Sprachen bereitgestellt, und es stellte sich heraus, dass bestimmte Gelehrte Experten für die von ihnen übersetzten Werke wurden und im Gegenzug weitere Unterstützung für die Weiterentwicklung bestimmter Wissenschaften erhielten.
Ein bemerkenswertes Merkmal vieler Gelehrter, die im Mittelalter unter muslimischer Herrschaft arbeiteten, ist, dass sie oft Universalgelehrte waren.
In dieser Zeit des Übergangs von einer überwiegend feudalen und kirchlichen Kultur zu einer überwiegend säkularen Kultur gingen viele namhafte Mathematiker anderen Berufen nach: Luca Pacioli (Begründer des Rechnungswesens); Niccolò Fontana Tartaglia (namhafter Ingenieur und Buchhalter); Gerolamo Cardano (frühester Begründer der Wahrscheinlichkeits- und Binomialentwicklung); Robert Recorde (Arzt) und François Viète (Rechtsanwalt).
Die britischen Universitäten dieser Zeit übernahmen einige Ansätze, die den italienischen und deutschen Universitäten bekannt waren, aber da sie bereits über erhebliche Freiheiten und Autonomie verfügten, hatten die Veränderungen dort mit dem Zeitalter der Aufklärung begonnen, denselben Einflüssen, die Humboldt inspirierten.
Die Studierenden konnten in Seminaren oder Laboren forschen und begannen, Doktorarbeiten mit wissenschaftlicherem Inhalt anzufertigen.
Mathematiker und angewandte Mathematiker gelten als zwei der MINT-Berufe (Naturwissenschaften, Technik, Ingenieurwesen und Mathematik).
Versicherungsmathematiker befassen sich auch mit finanziellen Fragen, darunter Fragen zur Höhe der Rentenbeiträge, die zur Erzielung eines bestimmten Ruhestandseinkommens erforderlich sind, und zur Art und Weise, wie ein Unternehmen Ressourcen investieren sollte, um seine Kapitalrendite angesichts potenzieller Risiken zu maximieren.
Das Hieroglyphensystem für ägyptische Ziffern ging, wie auch für die späteren römischen Ziffern, auf Strichmarkierungen zurück, die zum Zählen verwendet wurden.
Frühe Zahlensysteme mit Positionsschreibweise waren nicht dezimal, einschließlich des Sexagesimalsystems (Basis 60) für babylonische Zahlen und des Vigesimalsystems (Basis 20), das die Maya-Zahlen definierte.
Vor den Werken Euklids um 300 v. Chr. überschnitten sich griechische Mathematikstudien mit philosophischen und mystischen Überzeugungen.
Den alten Griechen fehlte bis zur hellenistischen Zeit ein Symbol für die Null, und sie verwendeten drei verschiedene Symbolsätze als Ziffern: einen Satz für die Einerstelle, einen für die Zehnerstelle und einen für die Hunderterstelle.
Ihr Algorithmus für die lange Division war derselbe, und der im 20. Jahrhundert populäre Algorithmus zur ziffernweisen Quadratwurzel war Archimedes bekannt (der ihn möglicherweise erfunden hat).
Die alten Chinesen verfügten über fortgeschrittene arithmetische Studien, die von der Shang-Dynastie bis zur Tang-Dynastie reichten, von einfachen Zahlen bis hin zu fortgeschrittener Algebra.
Für die Hunderterstelle wurden dann die Symbole für die Einerstelle wiederverwendet und so weiter.
Die alten Chinesen waren die ersten, die negative Zahlen sinnvoll entdeckten, verstanden und anwendeten.
Sein Zeitgenosse, der syrische Bischof Severus Sebokht (650 n. Chr.), sagte: „Indianer besitzen eine Berechnungsmethode, die kein Wort genug loben kann.“
Auch die Araber lernten diese neue Methode und nannten sie Hesab.
Das Aufblühen der Algebra in der mittelalterlichen islamischen Welt und auch im Europa der Renaissance war ein Ergebnis der enormen Vereinfachung von Berechnungen durch die Dezimalschreibweise.
Arithmetische Ausdrücke müssen entsprechend der vorgesehenen Operationsfolge ausgewertet werden.
Beispielsweise können digitale Computer vorhandene Additionsschaltkreise wiederverwenden und zusätzliche Schaltkreise für die Implementierung einer Subtraktion einsparen, indem sie die Methode des Zweierkomplements zur Darstellung der additiven Inversen verwenden, die äußerst einfach in Hardware zu implementieren ist (Negation).
Auch die Multiplikation verbindet zwei Zahlen zu einer einzigen Zahl, dem Produkt.
Wenn man sich die Zahlen als in einer Linie liegend vorstellt, ist die Multiplikation mit einer Zahl größer als 1, beispielsweise
Jede durch Null geteilte Dividende ist undefiniert.
Der Grundsatz der Arithmetik wurde erstmals von Carl Friedrich Gauß bewiesen.
Unter Positionsnotation (auch bekannt als „Stellenwertnotation“) versteht man die Darstellung oder Kodierung von Zahlen unter Verwendung desselben Symbols für die verschiedenen Größenordnungen (z. B. „Einerstelle“, „Zehnerstelle“, „Hunderterstelle“). und mit einem Basispunkt die gleichen Symbole zur Darstellung von Brüchen verwenden (z. B. die „Zehntelstelle“, „Hundertstelstelle“).
Die Verwendung von 0 als Platzhalter und damit die Verwendung einer Positionsnotation wird erstmals im Jain-Text aus Indien mit dem Titel Lokavibhâga aus dem Jahr 458 n. Chr. bezeugt, und erst im frühen 13. Jahrhundert wurden diese Konzepte über das übertragen Wissenschaft der arabischen Welt, wurden von Fibonacci unter Verwendung des hindu-arabischen Zahlensystems in Europa eingeführt.
Das Ergebnis wird durch wiederholtes Addieren einzelner Ziffern jeder Zahl an derselben Position von rechts nach links berechnet.
Die Ziffer ganz rechts ist der Wert für die aktuelle Position, und das Ergebnis für die nachfolgende Addition der Ziffern links erhöht sich um den Wert der zweiten Ziffer (ganz links), die immer eins (wenn nicht null) ist.
Eine Multiplikationstabelle mit zehn Zeilen und zehn Spalten listet die Ergebnisse für jedes Ziffernpaar auf.
Ähnliche Techniken gibt es für die Subtraktion und Division.
In der mathematischen Terminologie wird dieses Merkmal als Abschluss definiert und die vorherige Liste wird als beschrieben.
Die Summe in der Pence-Spalte beträgt 25.
Dieser Vorgang wird mit den Werten in der Schilling-Spalte wiederholt, mit dem zusätzlichen Schritt, den aus der Penny-Spalte übertragenen Wert hinzuzufügen.
Eine typische Broschüre mit 150 Seiten umfasste tabellarische Vielfache „von eins bis zehntausend zu den verschiedenen Preisen von einem Farthing bis zu einem Pfund“.
Diese Studie wird manchmal als Algorithmus bezeichnet.
Auch die Arithmetik wurde von islamischen Gelehrten verwendet, um die Anwendung der Regeln im Zusammenhang mit Zakat und Irth zu lehren.
Addition (normalerweise durch das Pluszeichen gekennzeichnet) ist eine der vier Grundoperationen der Arithmetik, die anderen drei sind Subtraktion, Multiplikation und Division.
In der Algebra, einem anderen Bereich der Mathematik, kann die Addition auch an abstrakten Objekten wie Vektoren, Matrizen, Unterräumen und Untergruppen durchgeführt werden.
Die Verwendung des Gerundivsuffixes -nd führt zu „addend“, „hinzufügendes Ding“.
„Summe“ und „Summand“ leiten sich vom lateinischen Substantiv summa „das Höchste, die Spitze“ und dem dazugehörigen Verb summare ab.
Die späteren mittelenglischen Begriffe „adden“ und „adding“ wurden von Chaucer populär gemacht.
Sollte beispielsweise der Ausdruck a + b + c so definiert werden, dass er (a + b) + c oder a + (b + c) bedeutet?
Sogar einige nichtmenschliche Tiere zeigen eine begrenzte Fähigkeit zur Addition, insbesondere Primaten.
Mit zunehmender Erfahrung lernen Kinder, schneller zu addieren, indem sie die Kommutativität der Addition ausnutzen, indem sie von der größeren Zahl aus hochzählen, in diesem Fall beginnend mit drei und zählen „vier, fünf“.
Null: Da Null die additive Identität ist, ist das Hinzufügen von Null trivial.
Man richtet zwei Dezimalbrüche übereinander aus, wobei sich der Dezimalpunkt an derselben Stelle befindet.
Handelt es sich bei den Summanden um die Drehzahlen zweier Wellen, können diese mit einem Differenzial addiert werden.
Es nutzte einen schwerkraftunterstützten Tragemechanismus.
Zum Subtrahieren musste der Operator das Komplement des Pascal-Rechners verwenden, was genauso viele Schritte erforderte wie eine Addition.
Sowohl XOR- als auch AND-Gatter sind in digitaler Logik einfach zu realisieren und ermöglichen die Realisierung von Volladdiererschaltungen, die wiederum zu komplexeren logischen Operationen kombiniert werden können.
Viele Implementierungen sind tatsächlich Hybride dieser letzten drei Designs.
Ein unerwarteter arithmetischer Überlauf ist eine recht häufige Ursache für Programmfehler.
Wörtlich genommen ist die obige Definition eine Anwendung des Rekursionssatzes auf die teilweise geordnete Menge N2.
Wenn entweder a oder b Null ist, behandeln Sie es als Identität.
Dabei wird die Halbgruppe durch die natürlichen Zahlen gebildet und die Gruppe ist die additive Gruppe der ganzen Zahlen.
Die Kommutativität und Assoziativität der reellen Addition sind unmittelbar; Definiert man die reelle Zahl 0 als die Menge der negativen Rationalzahlen, erkennt man leicht, dass es sich dabei um die additive Identität handelt.
Man muss beweisen, dass diese Operation wohldefiniert ist und sich mit Co-Cauchy-Folgen befasst.
Die Menge der ganzen Zahlen Modulo 2 hat nur zwei Elemente; Die von ihr übernommene Additionsoperation wird in der booleschen Logik als „exklusive Oder“-Funktion bezeichnet.
Diese liefern zwei unterschiedliche Verallgemeinerungen der Addition natürlicher Zahlen zum Transfiniten.
Es gibt noch mehr Verallgemeinerungen der Multiplikation als der Addition.
Wenn zwei nichtnegative Zahlen a und b unterschiedliche Größenordnungen haben, entspricht ihre Summe tatsächlich ungefähr ihrem Maximum.
Es beinhaltet die Idee der Summe einer einzelnen Zahl, die sie selbst ist, und der leeren Summe, die Null ist.
Integration ist eine Art „Summation“ über ein Kontinuum, oder genauer und allgemeiner, über eine differenzierbare Mannigfaltigkeit.
Linearkombinationen sind besonders nützlich in Kontexten, in denen eine einfache Addition gegen eine Normalisierungsregel verstoßen würde, beispielsweise beim Mischen von Strategien in der Spieltheorie oder bei der Überlagerung von Zuständen in der Quantenmechanik.
Die Division ist eine der vier Grundoperationen der Arithmetik, also die Art und Weise, wie Zahlen kombiniert werden, um neue Zahlen zu bilden.
Diejenigen, in denen eine euklidische Division (mit Rest) definiert ist, werden euklidische Domänen genannt und umfassen Polynomringe in einer unbestimmten Form (die Multiplikation und Addition über Formeln mit einer Variablen definieren).
Dieses Divisionszeichen wird auch allein zur Darstellung der Divisionsoperation selbst verwendet, beispielsweise als Beschriftung einer Taste eines Taschenrechners.
Das Verteilen mehrerer Objekte auf einmal in jeder Runde des Teilens auf jeden Teil führt zu der Idee, eine Form der Teilung zu „chunkeln“, bei der man wiederholt Vielfache des Divisors vom Dividenden selbst subtrahiert.
Man kann Logarithmentabellen verwenden, um zwei Zahlen zu dividieren, indem man die Logarithmen der beiden Zahlen subtrahiert und dann den Antilogarithmus des Ergebnisses nachschlägt.
Einige Programmiersprachen wie C behandeln die Ganzzahldivision wie im Fall 5 oben, sodass die Antwort eine Ganzzahl ist.
Ebenso ist die rechte Division von b durch a (geschrieben) die Lösung y der Gleichung.
Beispiele hierfür sind Matrixalgebren und Quaternionalgebren.
Die Eingabe eines solchen Ausdrucks in die meisten Taschenrechner führt zu einer Fehlermeldung.
Da diese Ersetzung die größere der beiden Zahlen reduziert, führt die Wiederholung dieses Vorgangs zu immer kleineren Zahlenpaaren, bis die beiden Zahlen gleich werden.
Die Tatsache, dass die GCD immer auf diese Weise ausgedrückt werden kann, wird als Bézouts Identität bezeichnet.
Mit dieser Verbesserung erfordert der Algorithmus nie mehr Schritte als das Fünffache der Anzahl der Stellen (Basis 10) der kleineren ganzen Zahl.
Der euklidische Algorithmus hat viele theoretische und praktische Anwendungen.
Der euklidische Algorithmus kann verwendet werden, um diophantische Gleichungen zu lösen, beispielsweise um Zahlen zu finden, die mehrere Kongruenzen gemäß dem chinesischen Restsatz erfüllen, um Kettenbrüche zu konstruieren und um genaue rationale Näherungen an reelle Zahlen zu finden.
Der größte gemeinsame Teiler wird oft als gcd(a, b) oder einfacher als (a, b) geschrieben, obwohl die letztere Schreibweise mehrdeutig ist, wird sie auch für Konzepte wie ein Ideal im Ring der ganzen Zahlen verwendet, was eng beieinander liegt im Zusammenhang mit GCD.
Beispielsweise sind weder 6 noch 35 eine Primzahl, da beide zwei Primfaktoren haben: 6 u003d 2 × 3 und 35 u003d 5 × 7.
Es wird angenommen, dass die Faktorisierung großer Ganzzahlen ein rechentechnisch sehr schwieriges Problem ist und die Sicherheit vieler weit verbreiteter kryptografischer Protokolle auf ihrer Undurchführbarkeit beruht.
Die Menge aller ganzzahligen Linearkombinationen von a und b ist tatsächlich dieselbe wie die Menge aller Vielfachen von g (mg, wobei m eine ganze Zahl ist).
Mit anderen Worten: Vielfache der kleineren Zahl rk−1 werden von der größeren Zahl rk−2 subtrahiert, bis der Rest rk kleiner als rk−1 ist.
Daher teilt c den anfänglichen Rest r0, da r0 u003d a − q0b u003d mc − q0nc u003d (m − q0n)c.
Wir versuchen zunächst, das Rechteck mit B-by-B-Quadratkacheln zu kacheln. Dies lässt jedoch ein r0-mal-b-Restrechteck unbesetzt zurück, wobei r0 < b. Anschließend versuchen wir, das verbleibende Rechteck mit r0 mal r0 quadratischen Kacheln zu kacheln.
Der Satz, der der Definition der euklidischen Division zugrunde liegt, stellt sicher, dass ein solcher Quotient und Rest immer existieren und eindeutig sind.
Am Ende der Schleifeniteration enthält die Variable b den Rest rk, während die Variable a ihren Vorgänger rk−1 enthält.
Der Mathematiker und Historiker B. L. van der Waerden schlägt vor, dass Buch VII aus einem Lehrbuch über Zahlentheorie stammt, das von Mathematikern aus der Schule des Pythagoras verfasst wurde.
Jahrhunderte später wurde Euklids Algorithmus unabhängig voneinander sowohl in Indien als auch in China entdeckt, hauptsächlich um diophantische Gleichungen zu lösen, die in der Astronomie auftraten, und um genaue Kalender zu erstellen.
Der euklidische Algorithmus wurde erstmals numerisch beschrieben und in Europa in der zweiten Auflage von Bachets Problèmes plaisants et délectables (Angenehme und erfreuliche Probleme, 1624) populär gemacht.
Im 19. Jahrhundert führte der euklidische Algorithmus zur Entwicklung neuer Zahlensysteme, etwa der Gaußschen Ganzzahlen und der Eisenstein-Ganzzahlen.
Peter Gustav Lejeune Dirichlet scheint der erste gewesen zu sein, der den Euklidischen Algorithmus als Grundlage für einen Großteil der Zahlentheorie beschrieben hat.
Dedekind war beispielsweise der erste, der den Zweiquadratsatz von Fermat mithilfe der eindeutigen Faktorisierung von Gaußschen ganzen Zahlen bewies.
Weitere Anwendungen des Euklid-Algorithmus wurden im 19. Jahrhundert entwickelt.
Es wurden mehrere neuartige Algorithmen für ganzzahlige Beziehungen entwickelt, beispielsweise der Algorithmus von Helaman Ferguson und R.W. Forcade (1979) und der LLL-Algorithmus.
Die Spieler nehmen abwechselnd m Vielfache des kleineren Stapels vom größeren ab.
Indem u die Möglichkeit gegeben wird, über alle möglichen ganzen Zahlen zu variieren, kann aus einer einzigen Lösung (x1, y1) eine unendliche Lösungsfamilie generiert werden.
In diesem Feld werden die Ergebnisse jeder mathematischen Operation (Addition, Subtraktion, Multiplikation oder Division) modulo 13 reduziert; Das heißt, Vielfache von 13 werden addiert oder subtrahiert, bis das Ergebnis im Bereich von 0–12 liegt.
Nehmen wir nun an, dass das Ergebnis für alle Werte von N bis M − 1 gilt.
Zur Veranschaulichung: Die Wahrscheinlichkeit eines Quotienten von 1, 2, 3 oder 4 beträgt ungefähr 41,5 %, 17,0 %, 9,3 % bzw. 5,9 %.
Ein ineffizienter Ansatz zum Ermitteln des GCD zweier natürlicher Zahlen a und b besteht darin, alle ihre gemeinsamen Teiler zu berechnen; der GCD ist dann der größte gemeinsame Teiler.
Wie oben erwähnt, entspricht der GCD dem Produkt der Primfaktoren, die die beiden Zahlen a und b gemeinsam haben. Derzeitige Methoden zur Primfaktorzerlegung sind ebenfalls ineffizient; Viele moderne Kryptografiesysteme beruhen sogar auf dieser Ineffizienz.
Der GCD-Algorithmus von Lehmer verwendet dasselbe allgemeine Prinzip wie der binäre Algorithmus, um GCD-Berechnungen in beliebigen Basen zu beschleunigen.
Der euklidische Algorithmus kann zur Lösung linearer diophantischer Gleichungen und chinesischer Restprobleme für Polynome verwendet werden; Es können auch Kettenbrüche von Polynomen definiert werden.
Jede euklidische Domäne ist eine eindeutige Faktorisierungsdomäne (UFD), obwohl das Gegenteil nicht der Fall ist.
Ein euklidischer Bereich ist immer ein Hauptidealbereich (PID), ein Integralbereich, in dem jedes Ideal ein Hauptideal ist.
Zähler und Nenner werden auch in ungewöhnlichen Brüchen verwendet, darunter zusammengesetzte Brüche, komplexe Brüche und gemischte Zahlen.
Der Begriff wurde ursprünglich verwendet, um diese Art von Bruch vom in der Astronomie verwendeten Sexagesimalbruch zu unterscheiden.
Dies wurde im Lehrbuch „The Ground of Arts“ aus dem 17. Jahrhundert erklärt.
Das Produkt eines Bruchs und seines Kehrwerts ist 1, daher ist der Kehrwert die multiplikative Umkehrung eines Bruchs.
Der Rest wird zum Zähler des Bruchteils.
Da 5×17 (u003d 85) größer als 4×18 (u003d 72) ist, lautet das Ergebnis des Vergleichs.
Da ein Drittel eines Viertels ein Zwölftel ist, sind zwei Drittel eines Viertels zwei Zwölftel.
Manchmal ist eine unendlich wiederholte Dezimalzahl erforderlich, um die gleiche Genauigkeit zu erreichen.
Die Ägypter verwendeten vor Christus ägyptische Brüche.
Ihre Methoden gaben die gleiche Antwort wie moderne Methoden.
Ein moderner Ausdruck von Brüchen, bekannt als Bhinnarasi, scheint seinen Ursprung in Indien im Werk von Aryabhatta, Brahmagupta und Bhaskara zu haben.
In der Mathematik ist die modulare Arithmetik ein Rechensystem für ganze Zahlen, bei dem Zahlen „umlaufen“, wenn sie einen bestimmten Wert, den sogenannten Modul, erreichen.
Eine sehr praktische Anwendung ist die Berechnung von Prüfsummen innerhalb von Seriennummernkennungen.
RSA und Diffie-Hellman verwenden modulare Potenzierung.
Es wird von den effizientesten Implementierungen des größten gemeinsamen Polynomteilers, der exakten linearen Algebra und der Gröbner-Basisalgorithmen für ganze Zahlen und rationale Zahlen verwendet.
Die Modulo-Operation, wie sie in vielen Programmiersprachen und Taschenrechnern implementiert ist, ist eine in diesem Zusammenhang häufig verwendete Anwendung der modularen Arithmetik.
Die Methode des Ausrechnens von Neunen bietet eine schnelle Überprüfung dezimaler arithmetischer Berechnungen, die von Hand durchgeführt werden.
Ein lineares Kongruenzsystem kann in polynomialer Zeit mit einer Form der Gaußschen Eliminierung gelöst werden, Einzelheiten siehe Satz der linearen Kongruenz.
Die Multiplikation von ganzen Zahlen (einschließlich negativer Zahlen), rationalen Zahlen (Brüchen) und reellen Zahlen wird durch eine systematische Verallgemeinerung dieser Grunddefinition definiert.
Das Produkt zweier Messungen ist eine neue Art der Messung.
Die Umkehroperation der Multiplikation ist die Division.
Die Division einer Zahl ungleich 0 durch sich selbst ergibt 1.
Diese implizite Verwendung der Multiplikation kann zu Mehrdeutigkeiten führen, wenn die verketteten Variablen zufällig mit dem Namen einer anderen Variablen übereinstimmen, wenn ein Variablenname vor einer Klammer mit einem Funktionsnamen verwechselt werden kann oder wenn die Reihenfolge von Operationen korrekt bestimmt wird.
Die zu multiplizierenden Zahlen werden allgemein als „Faktoren“ bezeichnet.
Da das Ergebnis einer Multiplikation außerdem nicht von der Reihenfolge der Faktoren abhängt, ist die Unterscheidung zwischen „Multiplikand“ und „Multiplikator“ nur auf einer sehr elementaren Ebene und in einigen Multiplikationsalgorithmen, wie beispielsweise der langen Multiplikation, sinnvoll.
Das Ergebnis einer Multiplikation wird als Produkt bezeichnet.
Mit dem Rechenschieber konnten Zahlen schnell mit einer Genauigkeit von etwa drei Stellen multipliziert werden.
Die allgemeine Theorie wird durch Dimensionsanalyse gegeben.
Die komplexen Zahlen haben keine Ordnung.
Hier haben wir die Identität 1, im Gegensatz zu Gruppen unter Addition, bei denen die Identität typischerweise 0 ist.
Um dies zu sehen, betrachten Sie die Menge der invertierbaren quadratischen Matrizen einer bestimmten Dimension über einem bestimmten Feld.
Eine weitere bemerkenswerte Tatsache ist, dass die ganzen Zahlen bei der Multiplikation keine Gruppe bilden – selbst wenn wir Null ausschließen.
In der Mathematik ist ein Prozentsatz (von lateinisch per centum „mal hundert“) eine Zahl oder ein Verhältnis, ausgedrückt als Bruchteil von 100.
Die Berechnung mit diesen Brüchen entsprach der Berechnung von Prozentsätzen.
Wenn Sie über einen Prozentsatz sprechen, ist es wichtig, anzugeben, in welchem Verhältnis er steht (d. h. was die Summe ist, die 100 % entspricht).
Wenn man von einem „10 %-Anstieg“ oder einem „10 %-Rückgang“ einer Größe spricht, ist die übliche Interpretation, dass sich dieser auf den Anfangswert dieser Größe bezieht.
Die gleiche Verwechslung zwischen den unterschiedlichen Konzepten von Prozent (Alter) und Prozentpunkten kann möglicherweise zu einem großen Missverständnis führen, wenn Journalisten beispielsweise über Wahlergebnisse berichten und sowohl neue Ergebnisse als auch Unterschiede zu früheren Ergebnissen als Prozentsätze ausdrücken.
Der Begriff wurde dem Lateinischen per centum zugeschrieben.
Grammatik- und Styleguides unterscheiden sich häufig hinsichtlich der Schreibweise von Prozentsätzen.
Bei sehr niedrigen Zinssätzen wird die Zahl 0 eingefügt, wenn der Zinssatz weniger als 1 % beträgt, z. B. „% Treasury Stock“, nicht „% Treasury Stock“.)
Ebenso wird die Siegquote einer Mannschaft, also der Anteil der Spiele, die der Verein gewonnen hat, normalerweise auch als Dezimalanteil ausgedrückt; Eine Mannschaft mit einer Gewinnquote von .500 hat 50 % ihrer Spiele gewonnen.
Die Subtraktion unterliegt auch vorhersehbaren Regeln für verwandte Operationen, wie etwa Addition und Multiplikation.
Die Subtraktion natürlicher Zahlen ist eine der einfachsten numerischen Aufgaben.
Formal wird die Zahl, die subtrahiert wird, als Subtrahend bezeichnet, während die Zahl, von der sie subtrahiert wird, Minuend ist.
„Subtraction“ ist ein englisches Wort, das vom lateinischen Verb subtrahere abgeleitet ist, das wiederum eine Verbindung aus sub „von unten“ und trahere „ziehen“ ist.
Von Position 3 sind keine Schritte nach links nötig, um bei 3 zu bleiben, also .
Um einen solchen Vorgang darzustellen, muss die Zeile verlängert werden.
Die führende Ziffer „1“ des Ergebnisses wird dann verworfen.
An der Zehnerstelle ist 0 kleiner als 1, also wird die 0 um 10 erhöht und die Differenz zur 1, also 9, wird an der Zehnerstelle notiert.
Die Subtraktion erfolgt dann an der Hunderterstelle, wobei 6 nicht kleiner als 5 ist, sodass die Differenz an der Hunderterstelle des Ergebnisses notiert wird.
Vielmehr erhöht es die Hunderterstelle des Subtrahends um eins.
Die Antwort ist 1 und wird an der Hunderterstelle des Ergebnisses notiert.
Dieser Satz wurde erstmals 1637 von Pierre de Fermat am Rand einer Kopie von Arithmetica vermutet, wo er behauptete, er habe einen Beweis, der zu umfangreich sei, um in den Rand zu passen.
Der Fünf-Farben-Satz, für den es einen kurzen elementaren Beweis gibt, besagt, dass fünf Farben ausreichen, um eine Karte zu färben, und wurde im späten 19. Jahrhundert bewiesen; Allerdings erwies sich der Nachweis, dass vier Farben ausreichen, als deutlich schwieriger.
Es war der erste große Satz, der mit einem Computer bewiesen wurde.
Darüber hinaus muss jede Karte, die möglicherweise ein Gegenbeispiel sein könnte, einen Teil haben, der wie eine dieser 1.936 Karten aussieht.
Es wurde ursprünglich 1908 von Steinitz und Tietze formuliert.
Eine Sorte V über einem endlichen Körper mit q Elementen hat eine endliche Anzahl rationaler Punkte sowie Punkte über jedem endlichen Körper mit qk Elementen, die diesen Körper enthalten.
Ursprünglich von Henri Poincaré vermutet, betrifft der Satz einen Raum, der lokal wie ein gewöhnlicher dreidimensionaler Raum aussieht, aber zusammenhängend und endlich groß ist und keine Grenze hat (eine geschlossene 3-Mannigfaltigkeit).
Nach fast einem Jahrhundert der Bemühungen von Mathematikern präsentierte Grigori Perelman einen Beweis der Vermutung in drei Veröffentlichungen, die 2002 und 2003 auf arXiv verfügbar gemacht wurden.
Perelman vervollständigte diesen Teil des Beweises.
Informell wird gefragt, ob jedes Problem, dessen Lösung schnell von einem Computer überprüft werden kann, auch schnell von einem Computer gelöst werden kann. Es wird allgemein vermutet, dass die Antwort Nein lautet.
Es ist nicht bewiesen, welche davon falsch ist, aber es wird allgemein angenommen, dass die erste Vermutung wahr und die zweite falsch ist.
Beispielsweise wurde die Collatz-Vermutung, die sich darauf bezieht, ob bestimmte Folgen ganzer Zahlen enden oder nicht, für alle ganzen Zahlen bis zu 1,2 × 1012 (über einer Billion) getestet.
Bei diesen Beweisen kann es sich um unterschiedliche Arten handeln, beispielsweise um die Verifizierung ihrer Folgen oder um starke Zusammenhänge mit bekannten Ergebnissen.
Eine Beweismethode, die anwendbar ist, wenn es nur eine endliche Anzahl von Fällen gibt, die zu Gegenbeispielen führen könnten, ist als „Brute Force“ bekannt: Bei diesem Ansatz werden alle möglichen Fälle berücksichtigt und es wird gezeigt, dass sie keine Gegenbeispiele liefern.
Die Kontinuumshypothese, die versucht, die relative Kardinalität bestimmter unendlicher Mengen zu ermitteln, erwies sich schließlich als unabhängig von der allgemein akzeptierten Menge der Zermelo-Fraenkel-Axiome der Mengenlehre.
Nur wenige Zahlentheoretiker bezweifeln, dass die Riemann-Hypothese wahr ist.
Die Logistikkarte ist eine Polynomabbildung, die oft als archetypisches Beispiel dafür angeführt wird, wie chaotisches Verhalten aus sehr einfachen nichtlinearen dynamischen Gleichungen entstehen kann.
Kepler hat bewiesen, dass es sich um die Grenze des Verhältnisses aufeinanderfolgender Fibonacci-Zahlen handelt.
Aus zwei Gründen kann diese Darstellung Probleme verursachen.
Beispielsweise sind die beiden Darstellungen 0,999... und 1 in dem Sinne äquivalent, dass sie dieselbe Zahl darstellen.
Mithilfe von Computern und Supercomputern wurden einige der mathematischen Konstanten, darunter π, e und die Quadratwurzel aus 2, auf mehr als hundert Milliarden Stellen berechnet.
Einige Konstanten unterscheiden sich so sehr von der üblichen Art, dass eine neue Notation erfunden wurde, um sie sinnvoll darzustellen.
Manchmal ist das Symbol, das eine Konstante darstellt, ein ganzes Wort.
0 (Null) ist eine Zahl und die numerische Ziffer, mit der diese Zahl in Zahlen dargestellt wird.
Zu den Namen für die Zahl 0 im Englischen gehören „zero“, „naught“ (UK), „naught“ (US; ), „nil“ oder – in Kontexten, in denen sie durch mindestens eine benachbarte Ziffer vom Buchstaben „O“ unterschieden wird – „oh“ oder „o“.
Für den einfachen Begriff des Mangels werden oft die Wörter „nichts“ und „nichts“ verwendet.
Im Zusammenhang mit Telefonnummern wird es oft „oh“ genannt.
Das Symbol nfr, was „schön“ bedeutet, wurde auch verwendet, um die Basishöhe in Zeichnungen von Gräbern und Pyramiden anzuzeigen, und Entfernungen wurden relativ zur Basislinie als ober- oder unterhalb dieser Linie gemessen.
Der babylonische Platzhalter war keine echte Null, da er weder allein noch am Ende einer Zahl verwendet wurde.
Im Jahr 150 n. Chr. verwendete Ptolemaios unter dem Einfluss von Hipparchos und den Babyloniern in seinem Werk zur mathematischen Astronomie namens Syntaxis Mathematica, auch bekannt als Almagest, ein Symbol für Null.
Diese Verwendung wurde im Jahr 525 n. Chr. in einer entsprechenden Tabelle wiederholt, die von Dionysius Exiguus neben römischen Ziffern auch mit dem lateinischen nulla oder „keine“ übersetzt wurde.
Der Lokavibhāga, ein Jain-Text zur Kosmologie, der in einer mittelalterlichen Sanskrit-Übersetzung des Prakrit-Originals überliefert ist und intern auf das Jahr 458 n. Chr. (Saka-Ära 380) datiert wird, verwendet ein dezimales Stellenwertsystem, einschließlich einer Null.
Im Jahr 813 verwendete al-Khwarizmi die hinduistischen Ziffern in seinen astronomischen Tabellen.
Dieses Buch wurde später im 12. Jahrhundert unter dem Titel Algoritmi de numero Indorum ins Lateinische übersetzt.
Ich setzte mein Studium intensiv fort und lernte das Geben und Nehmen der Disputation.
Ich habe mich bemüht, dieses Buch in seiner Gesamtheit so verständlich wie möglich zu verfassen und es in fünfzehn Kapitel zu unterteilen.
Die neun indischen Zahlen sind: 9 8 7 6 5 4 3 2 1.
254–255 enthalten 0 als natürliche Zahl. In diesem Fall ist sie die einzige natürliche Zahl, die nicht positiv ist.
Als Wert oder Zahl ist Null nicht dasselbe wie die Ziffer Null, die in Zahlensystemen mit Positionsschreibweise verwendet wird.
Die Zahl 0 kann als natürliche Zahl betrachtet werden oder auch nicht, aber sie ist eine ganze Zahl und daher eine rationale Zahl und eine reelle Zahl (sowie eine algebraische Zahl und eine komplexe Zahl).
Es kann keine Primzahl sein, weil es unendlich viele Faktoren hat, und es kann nicht zusammengesetzt sein, weil es nicht als Produkt von Primzahlen ausgedrückt werden kann (da 0 immer einer der Faktoren sein muss).
Diese Regeln gelten für jede reelle oder komplexe Zahl x, sofern nicht anders angegeben.
Die auf die leere Menge angewendete Kardinalitätsfunktion gibt die leere Menge als Wert zurück und weist ihr dadurch 0 Elemente zu.
In der abstrakten Algebra wird 0 üblicherweise zur Bezeichnung eines Nullelements verwendet, das ein neutrales Element für die Addition (falls in der betrachteten Struktur definiert) und ein absorbierendes Element für die Multiplikation (falls definiert) ist.
Bei einigen Größen unterscheidet sich das Nullniveau natürlich von allen anderen Niveaus, während es bei anderen mehr oder weniger willkürlich gewählt wird.
Es hat sich gezeigt, dass ein Cluster aus vier Neutronen stabil genug sein kann, um als eigenständiges Atom betrachtet zu werden.
Beispielsweise werden die Elemente eines Arrays in C beginnend bei 0 nummeriert, sodass für ein Array mit n Elementen die Reihenfolge der Array-Indizes von 0 bis verläuft.
In Datenbanken kann es vorkommen, dass ein Feld keinen Wert hat.
Bei Textfeldern ist dies weder ein Leerzeichen noch eine leere Zeichenfolge.
Jede Berechnung, die einen Nullwert enthält, liefert ein Nullergebnis.
Wenn in der Formel 1 der amtierende Weltmeister im Jahr nach seinem Sieg im Titelrennen nicht mehr an der Formel 1 teilnimmt, erhält einer der Fahrer des Teams, mit dem der amtierende Weltmeister den Titel gewonnen hat, 0.
Ursprünglich machten Schreibmaschinen keinen Unterschied in der Form zwischen O und 0; Einige Modelle verfügten nicht einmal über einen separaten Schlüssel für die Ziffer 0.
Die Ziffer 0 mit einem Punkt in der Mitte scheint als Option auf IBM 3270-Displays entstanden zu sein und hat sich in einigen modernen Computerschriften wie Andalé Mono und in einigen Reservierungssystemen von Fluggesellschaften fortgesetzt.
1 (Eins, auch Einheit und Einheit genannt) ist eine Zahl und eine numerische Ziffer, die zur Darstellung dieser Zahl in Zahlen verwendet wird.
In Vorzeichenkonventionen, bei denen Null weder als positiv noch als negativ betrachtet wird, ist 1 die erste und kleinste positive ganze Zahl.
Daraus lassen sich die meisten, wenn nicht alle Eigenschaften von 1 ableiten.
Es ist also die ganze Zahl nach Null.
Es wurde im Mittelalter durch auf Arabisch verfasste wissenschaftliche Werke über den Maghreb und Andalusien nach Europa übertragen.
Stile, die den langen Aufwärtsstrich bei Ziffer 1 nicht verwenden, verwenden normalerweise auch nicht den horizontalen Strich durch die Vertikale der Ziffer 7.
Per Definition ist 1 die Größe, der Absolutwert oder die Norm einer komplexen Einheitszahl, eines Einheitsvektors und einer Einheitsmatrix (häufiger als Identitätsmatrix bezeichnet).
In der Kategorientheorie wird 1 manchmal verwendet, um das Endobjekt einer Kategorie zu bezeichnen.
Da die Exponentialfunktion zur Basis 1 (1x) immer gleich 1 ist, existiert ihre Umkehrfunktion nicht (die als Logarithmus zur Basis 1 bezeichnet werden würde, wenn sie existieren würde).
Ebenso werden Vektoren oft in Einheitsvektoren (d. h. Vektoren der Größe eins) normiert, da diese oft wünschenswertere Eigenschaften haben.
Es ist auch die erste und zweite Zahl in der Fibonacci-Folge (0 ist die Null) und die erste Zahl in vielen anderen mathematischen Folgen.
Dennoch kann die abstrakte Algebra den Körper mit einem Element betrachten, das kein Singleton und überhaupt keine Menge ist.
Ein Binärcode ist eine Folge von 1 und 0, die in Computern zur Darstellung jeglicher Art von Daten verwendet wird.
+1 ist die elektrische Ladung von Positronen und Protonen.
Der neupythagoräische Philosoph Nikomachos von Gerasa bekräftigte, dass die Eins keine Zahl, sondern die Quelle der Zahl sei.
We Are Number One ist ein Lied aus der Kinderfernsehsendung LazyTown aus dem Jahr 2014, das als Meme an Popularität gewann.
Im Verbandsfußball wird die Nummer 1 oft dem Torwart gegeben.
1 ist die niedrigste Zahl, die von Spielern der National Hockey League (NHL) verwendet werden darf; Die Liga verbot Ende der 1990er Jahre die Verwendung von 00 und 0 (die höchste erlaubte Zahl war 98).
Jede zufällige Ziffernfolge enthält beliebig lange Teilfolgen, die nach dem Satz des unendlichen Affen nicht zufällig erscheinen.
Zweitens ist eine „Quadratur des Kreises“ nicht möglich, da mit Zirkel und Lineal keine transzendente Zahl konstruiert werden kann.
Der indische Astronom Aryabhata verwendete in seinem Āryabhaṭīya (499 n. Chr.) einen Wert von 3,1416.
Der persische Astronom Jamshīd al-Kāshī stellte im Jahr 1424 mithilfe eines Polygons mit 3×228 Seiten neun Sexagesimalstellen her, was ungefähr 16 Dezimalstellen entspricht, was etwa 180 Jahre lang als Weltrekord galt.
Diese vermeiden die Abhängigkeit von unendlichen Reihen.
In der von Salamin und Brent modifizierten Form wird er auch als Brent-Salamin-Algorithmus bezeichnet.
Dies steht im Gegensatz zu unendlichen Reihen oder iterativen Algorithmen, die alle Zwischenziffern beibehalten und verwenden, bis das Endergebnis vorliegt.
Solche Merkhilfen nennt man Mnemotechniken.
Die Ziffern sind große Holzzeichen, die an der kuppelartigen Decke befestigt sind.
Eine numerische Ziffer ist ein einzelnes Symbol, das allein (z. B. „2“) oder in Kombinationen (z. B. „25“) verwendet wird, um Zahlen in einem Positionszahlensystem darzustellen.
Ein Positionszahlensystem hat eine eindeutige Ziffer für jede ganze Zahl von Null bis zur Basis des Zahlensystems, jedoch nicht einschließlich dieser.
Die ursprünglichen Ziffern waren den modernen Ziffern sehr ähnlich, bis hin zu den Glyphen, die zur Darstellung der Ziffern verwendet wurden.
Die Mayas verwendeten ein Muschelsymbol zur Darstellung der Null.
Das thailändische Zahlensystem ist bis auf die zur Darstellung der Ziffern verwendeten Symbole mit dem hindu-arabischen Zahlensystem identisch.
Es handelt sich bei beiden um Basis-3-Systeme.
Mehrere Autoren haben in den letzten 300 Jahren auf eine Möglichkeit der Positionsschreibweise hingewiesen, die einer modifizierten Dezimaldarstellung gleichkommt.
Beispielsweise ist 1111 (eintausendeinhundertelf) eine Repunit.
Neben der Zählung von zehn Fingern zählen in manchen Kulturen auch die Fingerknöchel, der Raum zwischen den Fingern und Zehen sowie die Finger.
Steinzeitkulturen, darunter auch alte indigene amerikanische Gruppen, nutzten Zählkarten für Glücksspiele, persönliche Dienstleistungen und Handelswaren.
Ab etwa 3500 v. Chr. wurden Tonmarken nach und nach durch Zahlenzeichen ersetzt, die mit einem runden Stift in verschiedenen Winkeln in Tontafeln (ursprünglich Behälter für Münzen) eingeprägt und dann gebacken wurden.
Diese keilförmigen Zahlzeichen ähnelten den runden Zahlzeichen, die sie ersetzten, und behielten die additive Zeichen-Wert-Schreibweise der runden Zahlzeichen bei.
Sexagesimalzahlen waren ein gemischtes Basissystem, das die abwechselnde Basis 10 und Basis 6 in einer Folge keilförmiger vertikaler Keile und Winkel beibehielt.
Einzigartige Truppenzahlen und Reismengen erscheinen als einzigartige Kombinationen dieser Zahlen.
Herkömmliche Zahlen sind ziemlich schwierig zu multiplizieren und zu dividieren.
Juden begannen, ein ähnliches System (hebräische Ziffern) zu verwenden, wobei die ältesten bekannten Beispiele Münzen aus der Zeit um 100 v. Chr. waren.
Die Maya Mittelamerikas verwendeten ein gemischtes System mit der Basis 18 und der Basis 20, das möglicherweise von den Olmeken geerbt war, einschließlich erweiterter Funktionen wie Positionsnotation und einer Null.
Das Wissen über die Kodierung der Knoten und Farben wurde im 16. Jahrhundert von den spanischen Konquistadoren unterdrückt und ist nicht erhalten geblieben, obwohl in der Andenregion immer noch einfache Quipu-ähnliche Aufzeichnungsgeräte verwendet werden.
Null wurde erstmals im 7. Jahrhundert n. Chr. von Brahmagupta in Indien verwendet.
Arabische Mathematiker erweiterten das System um Dezimalbrüche, und Muḥammad ibn Mūsā al-Ḵwārizmī schrieb im 9. Jahrhundert ein wichtiges Werk darüber.
Das Binärsystem (Basis 2) wurde im 17. Jahrhundert von Gottfried Leibniz propagiert.
Die Variablen, für die die Gleichung gelöst werden muss, werden auch als Unbekannte bezeichnet, und die Werte der Unbekannten, die die Gleichung erfüllen, werden als Lösungen der Gleichung bezeichnet.
Eine bedingte Gleichung gilt nur für bestimmte Werte der Variablen.
Sehr oft wird angenommen, dass die rechte Seite einer Gleichung Null ist.
Eine Gleichung ist vergleichbar mit einer Waage, auf die Gewichte gelegt werden.
Dies ist die Ausgangsidee der algebraischen Geometrie, einem wichtigen Bereich der Mathematik.
Um Gleichungen aus beiden Familien zu lösen, verwendet man algorithmische oder geometrische Techniken, die aus der linearen Algebra oder der mathematischen Analyse stammen.
Diese Gleichungen sind im Allgemeinen schwierig; Bei der Suche geht es oft nur darum, das Vorhandensein oder Nichtvorhandensein einer Lösung herauszufinden und, falls vorhanden, die Anzahl der Lösungen zu zählen.
In der Abbildung sind x, y und z jeweils unterschiedliche Größen (in diesem Fall reelle Zahlen), die als Kreisgewichte dargestellt werden, und x, y und z haben jeweils ein anderes Gewicht.
Daher ist die Gleichung mit unbestimmtem R die allgemeine Gleichung für den Kreis.
Der Prozess, Lösungen zu finden oder, im Fall von Parametern, die Unbekannten durch die Parameter auszudrücken, wird als Lösen der Gleichung bezeichnet.
Beide Seiten einer Gleichung mit einer Größe ungleich Null multiplizieren oder dividieren.
Eine algebraische Gleichung ist univariat, wenn sie nur eine Variable umfasst.
In der Mathematik ist die Theorie linearer Systeme die Grundlage und ein grundlegender Bestandteil der linearen Algebra, einem Fach, das in den meisten Teilen der modernen Mathematik verwendet wird.
Dieser Formalismus ermöglicht es, die Positionen und Eigenschaften der Brennpunkte eines Kegelschnitts zu bestimmen.
Dieser von Descartes dargelegte Standpunkt bereichert und modifiziert die Art der Geometrie, die von den antiken griechischen Mathematikern konzipiert wurde.
Eine exponentielle diophantische Gleichung ist eine Gleichung, bei der Exponenten der Terme der Gleichung Unbekannte sein können.
Die moderne algebraische Geometrie basiert auf abstrakteren Techniken der abstrakten Algebra, insbesondere der kommutativen Algebra, mit der Sprache und den Problemen der Geometrie.
Ein Punkt der Ebene gehört zu einer algebraischen Kurve, wenn seine Koordinaten eine gegebene Polynomgleichung erfüllen.
In der reinen Mathematik werden Differentialgleichungen aus verschiedenen Perspektiven untersucht, wobei es vor allem um ihre Lösungen geht – die Menge der Funktionen, die die Gleichung erfüllen.
Lineare Differentialgleichungen, deren Lösungen mit Koeffizienten addiert und multipliziert werden können, sind gut definiert und verstanden, und es werden exakte Lösungen in geschlossener Form erhalten.
PDEs können zur Beschreibung einer Vielzahl von Phänomenen wie Schall, Wärme, Elektrostatik, Elektrodynamik, Flüssigkeitsströmung, Elastizität oder Quantenmechanik verwendet werden.
Eine Lösung ist eine Zuweisung von Werten zu den unbekannten Variablen, die die Gleichheit in der Gleichung wahr macht.
Die Menge aller Lösungen einer Gleichung ist ihre Lösungsmenge.
Abhängig vom Kontext kann das Lösen einer Gleichung darin bestehen, entweder eine beliebige Lösung (das Finden einer einzelnen Lösung reicht aus), alle Lösungen oder eine Lösung zu finden, die weitere Eigenschaften erfüllt, beispielsweise die Zugehörigkeit zu einem bestimmten Intervall.
In diesem Fall können die Lösungen nicht aufgeführt werden.
Die Vielfalt der Gleichungstypen ist groß, ebenso wie die entsprechenden Methoden.
Dies kann an mangelnden mathematischen Kenntnissen liegen; Manche Probleme konnten erst nach jahrhundertelanger Anstrengung gelöst werden.
Polynome kommen in vielen Bereichen der Mathematik und Naturwissenschaften vor.
Viele Autoren verwenden diese beiden Wörter synonym.
Formal lautet der Name des Polynoms P, nicht P(x), aber die Verwendung der Funktionsschreibweise P(x) stammt aus einer Zeit, als die Unterscheidung zwischen einem Polynom und der zugehörigen Funktion unklar war.
Man kann es jedoch in jedem Bereich verwenden, in dem Addition und Multiplikation definiert sind (d. h. in jedem Ring).
Polynome kleinen Grades haben spezifische Namen erhalten.
Das Polynom 0, von dem man annehmen kann, dass es überhaupt keine Terme hat, wird Nullpolynom genannt.
Da der Grad eines Polynoms ungleich Null der größte Grad eines Termes ist, hat dieses Polynom den Grad zwei.
Polynome können nach der Anzahl der Terme mit Koeffizienten ungleich Null klassifiziert werden, sodass ein Polynom mit einem Term als Monom bezeichnet wird, ein Polynom mit zwei Termen als Binomial bezeichnet wird und ein Polynom mit drei Termen als Trinom bezeichnet wird.
Wenn es zum Definieren einer Funktion verwendet wird, ist die Domäne nicht so eingeschränkt.
Ein Polynom in einer Unbestimmtheit wird als univariates Polynom bezeichnet, ein Polynom in mehr als einer Unbestimmtheit wird als multivariates Polynom bezeichnet.
Im Körper der komplexen Zahlen sind die irreduziblen Faktoren linear.
Wenn der Grad größer als eins ist, hat der Graph keine Asymptote.
In der elementaren Algebra werden Methoden wie die quadratische Formel gelehrt, um alle Polynomgleichungen ersten und zweiten Grades in einer Variablen zu lösen.
Allerdings können Wurzelfindungsalgorithmen verwendet werden, um numerische Näherungen der Wurzeln eines Polynomausdrucks beliebigen Grades zu finden.
Seit dem 16. Jahrhundert sind ähnliche Formeln (unter Verwendung von Kubikwurzeln zusätzlich zu Quadratwurzeln), aber viel komplizierter, für Gleichungen dritten und vierten Grades bekannt (siehe kubische Gleichung und quartische Gleichung).
Im Jahr 1830 bewies Évariste Galois, dass die meisten Gleichungen mit höherem Grad als vier nicht durch Radikale gelöst werden können, und zeigte, dass man für jede Gleichung entscheiden kann, ob sie durch Radikale lösbar ist, und wenn ja, diese lösen kann.
Dennoch wurden Formeln für lösbare Gleichungen der Grade 5 und 6 veröffentlicht (siehe Quintische Funktion und Sextische Gleichung).
Die effizientesten Algorithmen ermöglichen die einfache Lösung (auf einem Computer) von Polynomgleichungen mit einem höheren Grad als 1.000 (siehe Algorithmus zur Wurzelfindung).
Für eine Reihe von Polynomgleichungen mit mehreren Unbekannten gibt es Algorithmen, um zu entscheiden, ob sie eine endliche Anzahl komplexer Lösungen haben, und, wenn diese Anzahl endlich ist, um die Lösungen zu berechnen.
Eine Polynomgleichung, bei der man nur an den Lösungen interessiert ist, die ganze Zahlen sind, wird diophantische Gleichung genannt.
Für reellwertige Funktionen können die Koeffizienten als reelle Zahlen angenommen werden.
Diese Äquivalenz erklärt, warum Linearkombinationen Polynome genannt werden.
Bei Koeffizienten in einem Ring muss „nicht konstant“ durch „nicht konstant oder nicht einheitlich“ ersetzt werden (bei Koeffizienten in einem Körper stimmen beide Definitionen überein).
Wenn die Koeffizienten zu ganzen Zahlen, rationalen Zahlen oder einem endlichen Körper gehören, gibt es Algorithmen zum Testen der Irreduzibilität und zum Berechnen der Faktorisierung in irreduzible Polynome (siehe Faktorisierung von Polynomen).
Das charakteristische Polynom einer Matrix oder eines linearen Operators enthält Informationen über die Eigenwerte des Operators.
Die elegante und praktische Notation, die wir heute verwenden, entwickelte sich jedoch erst ab dem 15. Jahrhundert.
Dadurch wird das Quadrat „vervollständigt“ und die linke Seite in ein perfektes Quadrat umgewandelt.
Der Satz von Descartes besagt, dass die Radien von jeweils vier sich berührenden (einander tangentialen) Kreisen eine bestimmte quadratische Gleichung erfüllen.
Babylonische Mathematiker ab etwa 400 v. Chr. und chinesische Mathematiker ab etwa 200 v. Chr. nutzten geometrische Dissektionsmethoden, um quadratische Gleichungen mit positiven Wurzeln zu lösen.
Euklid, der griechische Mathematiker, entwickelte um 300 v. Chr. eine abstraktere geometrische Methode.
Al-Khwarizmi geht noch einen Schritt weiter und liefert eine vollständige Lösung für die allgemeine quadratische Gleichung, indem er für jede quadratische Gleichung eine oder zwei numerische Antworten akzeptiert und dabei geometrische Beweise liefert.
Insbesondere Abū Kāmil Shujā ibn Aslam (Ägypten, 10. Jahrhundert) war der erste, der irrationale Zahlen (oft in Form einer Quadratwurzel, Kubikwurzel oder vierten Wurzel) als Lösungen quadratischer Gleichungen oder als Koeffizienten in einer Gleichung akzeptierte.
Er verwendete einen Nullmeridian durch die Kanarischen Inseln, sodass alle Längengrade positiv waren.
Hinduistische und muslimische Astronomen entwickelten diese Ideen weiter, fügten viele neue Standorte hinzu und verbesserten häufig die Daten von Ptolemäus.
Im späteren Mittelalter erwachte das Interesse an Geographie im Westen wieder, als die Reisen zunahmen und die arabische Wissenschaft durch den Kontakt mit Spanien und Nordafrika bekannt wurde.
Christoph Kolumbus unternahm zwei Versuche, Mondfinsternisse zu nutzen, um seinen Längengrad zu ermitteln, den ersten am 14. September 1494 auf der Insel Saona (zweite Reise) und den zweiten am 29. Februar 1504 auf Jamaika (vierte Reise).
Ursprünglich ein Beobachtungsgerät, wurde es im Laufe des nächsten halben Jahrhunderts durch Entwicklungen zu einem präzisen Messinstrument.
An Land kam es in der Zeit von der Entwicklung von Teleskopen und Pendeluhren bis zur Mitte des 18. Jahrhunderts zu einem stetigen Anstieg der Zahl von Orten, deren Längengrad mit angemessener Genauigkeit bestimmt werden konnte, oft mit Fehlern von weniger als einem Grad und fast immer innerhalb von weniger als einem Grad 2-3°.
Genaue Beobachtungen bei Meereswellen durchzuführen ist viel schwieriger als an Land, und Pendeluhren funktionieren unter diesen Bedingungen nicht gut.
Es wurden zwei Belohnungsstufen angeboten, für Lösungen innerhalb von 1° und 0,5°.
Diese Arbeit wurde vom Board of Longitude mit Tausenden von Pfund unterstützt und belohnt, aber er kämpfte darum, Geld bis zur Höchstprämie von 20.000 Pfund zu erhalten, und erhielt schließlich 1773 nach der Intervention des Parlaments eine zusätzliche Zahlung.
Mondentfernungen wurden nach 1790 allgemein verwendet.
Schnell erkannte man, dass der Telegraph zur Übertragung eines Zeitsignals zur Längengradbestimmung genutzt werden konnte.
Die Vermessung erstellte in den Jahren 1874–90 Ketten kartierter Standorte durch Mittel- und Südamerika, die Westindischen Inseln und bis nach Japan und China.
Dies änderte sich, als Anfang des 20. Jahrhunderts die drahtlose Telegrafie verfügbar wurde.
Nach dem Zweiten Weltkrieg kamen Funknavigationssysteme allgemein zum Einsatz.
Mit Ausnahme der magnetischen Deklination erwiesen sich alle Methoden als praktikabel.
Der Längengrad an einem Punkt kann durch Berechnen der Zeitdifferenz zwischen dem Längengrad an seinem Standort und der koordinierten Weltzeit (UTC) bestimmt werden.
Das Wort „nahe“ wird verwendet, weil der Punkt möglicherweise nicht in der Mitte der Zeitzone liegt. Auch die Zeitzonen sind politisch festgelegt, sodass ihre Mittelpunkte und Grenzen oft nicht auf Meridianen im Vielfachen von 15° liegen.
Die internationale Standardkonvention (ISO 6709), dass Osten positiv ist, steht im Einklang mit einem rechtshändigen kartesischen Koordinatensystem mit dem Nordpol nach oben.
Seitdem sind sie zum Standardansatz übergegangen.
Das Geoid ist die Form, die die Meeresoberfläche unter dem Einfluss der Schwerkraft der Erde, einschließlich der Anziehungskraft und der Erdrotation, annehmen würde, wenn andere Einflüsse wie Winde und Gezeiten fehlen würden.
Dies kann nur durch umfangreiche Gravitationsmessungen und -berechnungen ermittelt werden.
Obwohl die physische Erde Abweichungen von +8.848 m (Mount Everest) und −10.984 (Marianengraben) aufweist, reicht die Abweichung des Geoids von einem Ellipsoid von +85 m (Island) bis −106 m (Südindien), also insgesamt weniger als 200 m .
Wenn die kontinentalen Landmassen von einer Reihe von Tunneln oder Kanälen durchzogen wären, würde auch der Meeresspiegel in diesen Kanälen nahezu mit dem Geoid übereinstimmen.
Das bedeutet, dass man bei Reisen mit dem Schiff die Wellen des Geoids nicht wahrnimmt; Die lokale Vertikale (Lotlinie) steht immer senkrecht zum Geoid und der lokale Horizont tangential dazu.
Das liegt daran, dass GPS-Satelliten, die um den Schwerpunkt der Erde kreisen, Höhen nur relativ zu einem geozentrischen Referenzellipsoid messen können.
Moderne GPS-Empfänger verfügen über ein in ihrer Software implementiertes Raster, mit dem sie aus der aktuellen Position die Höhe des Geoids (z. B. des EGM-96-Geoids) über dem Ellipsoid des World Geodetic System (WGS) ermitteln.
Wenn diese Kugel dann mit Wasser bedeckt wäre, wäre das Wasser nicht überall gleich hoch.
Aus diesem Grund verfügen viele tragbare GPS-Empfänger über integrierte Wellentabellen zur Bestimmung der Höhe über dem Meeresspiegel.
Die ersten auf GOCE-Satellitendaten basierenden Produkte wurden im Juni 2010 über die Erdbeobachtungs-Benutzerdienstetools der Europäischen Weltraumorganisation (ESA) online verfügbar.
Das Geoid ist eine bestimmte Äquipotentialfläche und die Berechnung ist etwas aufwendig.
Ein Globus ist ein kugelförmiges Modell der Erde, eines anderen Himmelskörpers oder der Himmelssphäre.
Als Himmelsglobus bezeichnet man einen Modellglobus der Himmelssphäre.
Es könnte Nationen und Großstädte sowie das Netzwerk von Breiten- und Längengradlinien zeigen.
Typischerweise wird auch die Himmelssphäre in Konstellationen unterteilt.
Die erste bekannte Erwähnung eines Globus stammt von Strabo und beschreibt den Globus von Kisten aus der Zeit um 150 v. Chr.
Viele Globen haben einen Umfang von einem Meter, sind also Modelle der Erde im Maßstab 1:40 Millionen.
Auf den meisten modernen Globen sind auch Parallelen und Meridiane eingeprägt, sodass man die ungefähren Koordinaten eines bestimmten Ortes erkennen kann.
Frühe Erdgloben, die die gesamte Alte Welt abbildeten, wurden in der islamischen Welt konstruiert.
Behaim war ein deutscher Kartograph, Seefahrer und Kaufmann.
Bevor Behaim den Globus konstruierte, war er viel gereist.
Ein weiterer früher Globus, der Hunt-Lenox-Globus, ca.
Es handelt sich möglicherweise um den ältesten Globus, der die Neue Welt zeigt.
Ein Faksimile-Globus, der Amerika zeigt, wurde 1507 von Martin Waldseemüller angefertigt.
Globus IMP, elektromechanische Geräte, darunter 5-Zoll-Globen, wurden von 1961 bis 2002 in sowjetischen und russischen Raumschiffen als Navigationsinstrumente eingesetzt.
Diese Methode der Globusherstellung wurde 1802 in einem Stich in The English Encyclopedia von George Kearsley illustriert.
Diese wird in eine Maschine gegeben, die die Scheibe in eine halbkugelförmige Form bringt.
Diese Globen waren „riesig“ und sehr teuer.
Letzterer hat ein sowjetisches Einschussloch durch Deutschland.
Ein Großkreis einer Kugel, auch Orthodrom genannt, ist der Schnittpunkt der Kugel mit einer Ebene, die durch den Mittelpunkt der Kugel verläuft.
Dieser Sonderfall eines Kreises einer Kugel steht im Gegensatz zu einem kleinen Kreis, also dem Schnittpunkt der Kugel mit einer Ebene, die nicht durch den Mittelpunkt geht.
Die Ausnahme bildet ein Paar antipodaler Punkte, für das es unendlich viele Großkreise gibt.
Die Länge des kleinen Bogens eines Großkreises wird in der Riemannschen Geometrie als Abstand zwischen zwei Punkten auf einer Kugeloberfläche angesehen, wobei solche Großkreise Riemannsche Kreise genannt werden.
Ein weiterer Großkreis ist derjenige, der die Land- und Wasserhalbkugel trennt.
In der Kartographie ist eine Kartenprojektion eine Möglichkeit, die Oberfläche eines Globus in eine Ebene zu glätten, um eine Karte zu erstellen.
Abhängig vom Zweck der Karte sind einige Verzerrungen akzeptabel, andere nicht. Daher existieren unterschiedliche Kartenprojektionen, um einige Eigenschaften des kugelähnlichen Körpers auf Kosten anderer Eigenschaften zu bewahren.
Projektionen sind Gegenstand mehrerer rein mathematischer Bereiche, darunter Differentialgeometrie, projektive Geometrie und Mannigfaltigkeiten.
Vielmehr ist jede mathematische Funktion, die Koordinaten von der gekrümmten Oberfläche deutlich und gleichmäßig in die Ebene umwandelt, eine Projektion.
Die Erde und andere große Himmelskörper lassen sich im Allgemeinen besser als abgeplattete Sphäroide modellieren, während kleine Objekte wie Asteroiden oft unregelmäßige Formen haben.
Da die gekrümmte Erdoberfläche nicht isometrisch zu einer Ebene ist, führt die Beibehaltung der Formen zwangsläufig zu einem variablen Maßstab und damit zu einer nichtproportionalen Darstellung der Flächen.
Der Zweck der Karte bestimmt, welche Projektion die Grundlage für die Karte bilden soll.
Datensätze sind geografische Informationen; Ihre Sammlung hängt vom gewählten Datum (Modell) der Erde ab.
Wie Tissots Indikatrix basiert auch die Goldberg-Gott-Indikatrix auf Infinitesimalzahlen und zeigt Flexions- und Schiefeverzerrungen (Biegung und Schiefheit).
Manchmal werden sphärische Dreiecke verwendet.
Eine andere Möglichkeit, lokale Verzerrungen zu visualisieren, sind Graustufen oder Farbabstufungen, deren Schattierung das Ausmaß der Winkelverformung oder Flächenausdehnung darstellt.
Da die tatsächliche Form der Erde unregelmäßig ist, gehen in diesem Schritt Informationen verloren.
Zum Vergleich: Man kann eine Orangenschale nicht glätten, ohne sie zu zerreißen und zu verziehen.)
Tangente bedeutet, dass die Oberfläche den Globus berührt, ihn aber nicht durchschneidet. Sekante bedeutet, dass die Oberfläche den Globus durchschneidet.
Wenn diese Linien eine Breitengradparallel sind, wie bei konischen Projektionen, spricht man von einer Standardparallele.
Dies gilt für jede zylindrische oder pseudozylindrische Projektion im Normalaspekt.
Der Maßstab ist entlang aller geraden Linien, die von einem bestimmten geografischen Standort ausgehen, konstant.
Ob sphärisch oder ellipsoidisch, die besprochenen Prinzipien gelten ohne Verlust der Allgemeingültigkeit.
Das Ellipsoidmodell wird üblicherweise zur Erstellung topografischer Karten und für andere Karten großen und mittleren Maßstabs verwendet, die eine genaue Darstellung der Landoberfläche erfordern.
Im Vergleich zum am besten passenden Ellipsoid würde ein Geoidmodell die Charakterisierung wichtiger Eigenschaften wie Abstand, Konformität und Äquivalenz verändern.
Für unregelmäßige Planetenkörper wie Asteroiden werden jedoch manchmal Modelle verwendet, die dem Geoid ähneln, um daraus Karten zu projizieren.
Die Projektionen werden im Hinblick auf den Kontakt einer gigantischen Oberfläche mit der Erde beschrieben, gefolgt von einem impliziten Skalierungsvorgang.
Wo die Lichtquelle entlang der in dieser letzten Einschränkung beschriebenen Linie ausstrahlt, ergibt die Unterschiede zwischen den verschiedenen „natürlichen“ zylindrischen Projektionen.
Dieser Zylinder wird um die Erde gewickelt, auf sie projiziert und dann abgerollt.
Nord-Süd-Abstände weder gestreckt noch gestaucht (1): äquirechteckige Projektion oder „Plattenkarree“.
Da diese Projektion Nord-Süd-Abstände um den Kehrwert der Ost-West-Ausdehnung skaliert, bleibt die Fläche auf Kosten der Formen erhalten.
Andere Meridiane sind länger als der Mittelmeridian und neigen sich nach außen, vom Mittelmeridian weg.
Daher sind die Meridiane entlang einer bestimmten Parallele gleichmäßig verteilt.
Die resultierende konische Karte weist in der Nähe dieser Standardparallelen eine geringe Verzerrung in Bezug auf Maßstab, Form und Fläche auf.
Kann aus einer Perspektive konstruiert werden, die unendlich weit vom Tangentenpunkt entfernt ist; r(d) u003d c sin .
Nahseitige perspektivische Projektion, die den Blick aus dem Weltraum in einer endlichen Entfernung simuliert und daher weniger als eine vollständige Halbkugel zeigt, wie sie beispielsweise in The Blue Marble 2012 verwendet wird.
Der oder die speziellen Punkte können bei der Projektion zu einem Linien- oder Kurvensegment gestreckt werden.
Azimutal äquidistant: Abstände vom Zentrum und Rand bleiben erhalten.
Daher gibt es viele Projektionen, die den vielfältigen Verwendungszwecken von Karten und ihren großen Maßstäben gerecht werden.
Referenzkarten der Welt erscheinen häufig auf Kompromissprojektionen.
Die Mercator-Projektion ist eine zylindrische Kartenprojektion, die 1569 vom flämischen Geographen und Kartographen Gerardus Mercator vorgestellt wurde.
Als Nebeneffekt vergrößert die Mercator-Projektion die Größe von Objekten außerhalb des Äquators.
Angesichts der Geometrie einer Sonnenuhr könnten diese Karten jedoch durchaus auf der ähnlichen zentralen Zylinderprojektion basieren, einem Grenzfall der gnomonischen Projektion, die die Grundlage für eine Sonnenuhr bildet.
Dabei handelte es sich jedoch um einen einfachen und häufigen Fall einer Fehlidentifizierung.
Mercator betitelte die Karte: „Eine neue und erweiterte Beschreibung der Erde, korrigiert für den Gebrauch durch Seeleute“.
Im Laufe der Jahre wurden verschiedene Hypothesen aufgestellt, aber auf jeden Fall dürften Mercators Freundschaft mit Pedro Nunes und sein Zugang zu den von Nunes erstellten loxodromischen Tabellen seine Bemühungen unterstützt haben.
Die damit verbundene Mathematik wurde jedoch ab etwa 1589 vom Mathematiker Thomas Harriot entwickelt, aber nie veröffentlicht.
Zwei Hauptprobleme verhinderten seine sofortige Anwendung: die Unmöglichkeit, den Längengrad auf See mit ausreichender Genauigkeit zu bestimmen, und die Tatsache, dass bei der Navigation magnetische Richtungen anstelle von geografischen Richtungen verwendet wurden.
Die Dominanz auf den Weltkarten begann jedoch erst im 19. Jahrhundert, als das Problem der Positionsbestimmung weitgehend gelöst war.
Aufgrund dieses Drucks reduzierten die Verlage im Laufe des 20. Jahrhunderts schrittweise die Verwendung der Projektion.
Um dies zu erreichen, wird die unvermeidliche Ost-West-Ausdehnung der Karte, die mit zunehmender Entfernung vom Äquator zunimmt, in der Mercator-Projektion von einer entsprechenden Nord-Süd-Ausdehnung begleitet, sodass an jedem Punkt die Ost-West-Skala vorliegt entspricht der Nord-Süd-Skala und ist somit eine konforme Kartenprojektion.
Bei Breiten über 70° Nord oder Süd ist die Mercator-Projektion praktisch unbrauchbar, da der lineare Maßstab an den Polen unendlich groß wird.
Ellesmere Island im Norden des arktischen Archipels Kanadas scheint ungefähr so groß wie Australien zu sein, obwohl Australien über 39-mal so groß ist.
Die Fläche Grönlands ist allein mit der Fläche der Demokratischen Republik Kongo vergleichbar.
Alaska scheint genauso groß zu sein wie Australien, obwohl Australien tatsächlich viereinhalb Mal so groß ist.
Schweden erscheint viel größer als Madagaskar.
Eine Weltkarte auf einem regelmäßigen Ikosaeder durch gnomonische Projektion.
Aufgrund dieser Kritik verwenden moderne Atlanten nicht mehr die Mercator-Projektion für Weltkarten oder für vom Äquator entfernte Gebiete, sondern bevorzugen andere zylindrische Projektionen oder Formen flächentreuer Projektionen.
Arno Peters sorgte ab 1972 für Kontroversen, als er zur Behebung der Probleme des Mercator die sogenannte Gall-Peters-Projektion vorschlug und behauptete, es sei sein eigenes Originalwerk, ohne auf frühere Arbeiten von Kartographen wie Galls Werk aus dem Jahr 1855 zu verweisen.
Die Reichweite für a unter den möglichen Auswahlmöglichkeiten beträgt etwa 35 km, aber für Anwendungen im kleinen Maßstab (große Region) kann diese Variation ignoriert werden und Durchschnittswerte von 6.371 km bzw. 40.030 km für den Radius und den Umfang angenommen werden.
Eine zylindrische Kartenprojektion wird durch Formeln spezifiziert, die die geografischen Koordinaten des Breitengrads φ und des Längengrads λ mit kartesischen Koordinaten auf der Karte mit Ursprung am Äquator und x-Achse entlang des Äquators verknüpfen.
Da der Zylinder am Äquator tangential zum Globus verläuft, ist der Skalierungsfaktor zwischen Globus und Zylinder am Äquator eins, aber nirgendwo sonst.
Die Differenz (λ − λ0) wird im Bogenmaß angegeben.
Noch extremere Kürzungen wurden verwendet: Ein finnischer Schulatlas wurde bei etwa 76°N und 56°S beschnitten, was einem Seitenverhältnis von 1,97 entspricht.
Schmalere Streifen sind besser: Sek. 8° u003d 1,01, sodass ein Streifen mit einer Breite von 16° (zentriert auf dem Äquator) auf 1 % oder 1 Teil von 100 genau ist.
Der Wert von e2 beträgt für alle Referenzellipsoide etwa 0,006.)
Für das obige Modell entspricht 1 cm 1.500 km bei einem Breitengrad von 60°.
Diese Sehne bildet in der Mitte einen Winkel von 2arcsin(cos φ sin ) und der Großkreisabstand zwischen A und B beträgt 2a arcsin(cos φ sin ).
Bei anderen Körpern wird normalerweise auf ein festes Oberflächenmerkmal verwiesen, bei dem es sich beim Mars um den Meridian handelt, der durch den Krater Airy-0 verläuft.
Konventionell wird er für Erde, Mond und Sonne in Graden im Bereich von –180° bis +180° ausgedrückt. Für andere Körper wird ein Bereich von 0° bis 360° verwendet.
Der Maßstab einer Karte ist das Verhältnis einer Entfernung auf der Karte zur entsprechenden Entfernung am Boden.
Der erste Weg ist das Verhältnis der Größe des erzeugenden Globus zur Größe der Erde.
Viele Karten geben den Nominalmaßstab an und zeigen möglicherweise sogar einen Balkenmaßstab (manchmal auch einfach „Maßstab“ genannt) an, um ihn darzustellen.
Unter „Skala“ versteht man in diesem Fall den Skalierungsfaktor (u003d Punkteskala u003d bestimmte Skala).
Die Kartenprojektion ist von entscheidender Bedeutung für das Verständnis, wie der Maßstab auf der Karte variiert.
Dies ist eine Übersicht über praktisch alle bekannten Projektionen von der Antike bis 1993.
Kleiner Maßstab bezieht sich auf Weltkarten oder Karten großer Regionen wie Kontinente oder große Nationen.
Großformatige Karten zeigen kleinere Gebiete detaillierter, beispielsweise Kreiskarten oder Stadtpläne.
Allerdings verwenden Kartographen, wie oben erläutert, den Begriff „großmaßstäblich“ für weniger umfangreiche Karten – solche, die ein kleineres Gebiet zeigen.
Dies wird häufig durch die Unmöglichkeit veranschaulicht, eine Orangenschale auf einer ebenen Fläche glattzustreichen, ohne sie zu zerreißen und zu verformen.
Umgekehrt implizieren isotrope Skalierungsfaktoren auf der Karte eine konforme Projektion.
Die Bezeichnung „klein“ bedeutet, dass bei einer gegebenen Messgenauigkeit keine Änderung des Skalierungsfaktors über das Element hinweg festgestellt werden kann.
Wir sagen, dass diese Koordinaten die Projektionskarte definieren, die logisch von den tatsächlich gedruckten (oder angezeigten) Karten unterschieden werden muss.
Da die Punktskala je nach Position und Richtung variiert, wird die Projektion des Kreises auf der Projektion verzerrt.
Durch die Überlagerung dieser Verzerrungsellipsen auf der Kartenprojektion wird deutlich, wie sich die Punktskala auf der Karte ändert.
Das Verhältnis der Hauptachse zur Nebenachse beträgt.
Der Maßstab ist am Äquator wahr (ku003d1), so dass die Multiplikation seiner Länge auf einer gedruckten Karte mit dem Kehrwert des RF (oder Hauptmaßstabs) den tatsächlichen Umfang der Erde ergibt.
Das obere Diagramm zeigt die isotrope Mercator-Skalenfunktion: Die Skala auf der Parallele ist dieselbe wie die Skala auf dem Meridian.
Daher ist die tangentiale Mercator-Projektion innerhalb eines Streifens mit einer Breite von 3,24 Grad, der auf dem Äquator zentriert ist, sehr genau.
Diese Beobachtungen führten zur Entwicklung der transversalen Mercator-Projektionen, bei denen ein Meridian „wie ein Äquator“ der Projektion behandelt wird, sodass wir eine genaue Karte innerhalb eines engen Abstands zu diesem Meridian erhalten.
Die vier Himmelsrichtungen oder Kardinalpunkte sind die vier Hauptkompassrichtungen: Norden, Osten, Süden und Westen, üblicherweise mit den Initialen N, E, S und W bezeichnet.
Wenn man nach Osten oder Westen fährt, kann man nur am Äquator nach Osten oder Westen fahren und geradeaus fahren (ohne steuern zu müssen).
Der Nordpol der Magnetnadel zeigt zum geografischen Nordpol der Erde und umgekehrt.
In der Mitte des Tages liegt es im Süden für Zuschauer auf der Nordhalbkugel, die nördlich des Wendekreises des Krebses leben, und im Norden für diejenigen auf der Südhalbkugel, die südlich des Wendekreises des Steinbocks leben.
An diesen Orten muss man zunächst feststellen, ob sich die Sonne von Osten nach Westen über Norden oder Süden bewegt, indem man ihre Bewegungen beobachtet – links nach rechts bedeutet, dass sie durch Süden geht, während rechts nach links bedeutet, dass sie durch Norden geht; oder man kann die Schatten der Sonne beobachten.
Aufgrund der axialen Neigung der Erde gibt es unabhängig vom Standort des Betrachters nur zwei Tage im Jahr, an denen die Sonne genau im Osten aufgeht.
Damit diese Methode auf der Südhalbkugel funktioniert, ist die 12 auf die Sonne gerichtet und der Punkt in der Mitte zwischen Stundenzeiger und 12 Uhr zeigt den Norden an.
Diese Achse schneidet die Himmelssphäre am nördlichen und südlichen Himmelspol, die für den Beobachter direkt über dem genauen Norden bzw. Süden am Horizont zu liegen scheinen.
Das resultierende Foto zeigt eine Vielzahl konzentrischer Bögen (Teile perfekter Kreise), aus denen sich leicht der genaue Mittelpunkt ableiten lässt und die dem Himmelspol entsprechen, der direkt über der Position des wahren Pols (Nord oder Süd) liegt Horizont.
Die genaue Position des Pols ändert sich im Laufe der Jahrtausende aufgrund der Präzession der Tagundnachtgleiche.
Das Sternchen „Großer Wagen“ kann zur Suche nach Polaris verwendet werden.
Da es den wahren und nicht den magnetischen Norden findet, ist es immun gegen Störungen durch lokale Magnetfelder oder Magnetfelder an Bord.
Auf den meisten Karten im mittelalterlichen Europa ist beispielsweise der Osten (O) oben angegeben.
Topografische Karten umfassen Höhenangaben, typischerweise anhand von Höhenlinien.
Der Nordpunkt ist dann der Punkt auf der Extremität, der dem Himmelsnordpol am nächsten liegt.
Geht man vom Nordpunkt aus im Uhrzeigersinn um die Scheibe herum, trifft man der Reihe nach auf den Westpunkt, den Südpunkt und dann auf den Ostpunkt.
Im vormodernen Europa allgemeiner wurden zwischen acht und 32 Himmelsrichtungen – Himmelsrichtungen und Himmelsrichtungen – mit Namen versehen.
Systeme mit fünf Himmelsrichtungen (vier Himmelsrichtungen und das Zentrum) umfassen jene aus dem vormodernen China sowie traditionelle türkische, tibetische und Ainu-Kulturen.
Einige schließen möglicherweise auch „oben“ und „unten“ als Richtungen ein und konzentrieren sich daher auf eine Kosmologie mit sieben Richtungen.
Der Norden wird mit dem Himalaya und dem Himmel in Verbindung gebracht, während der Süden mit der Unterwelt oder dem Land der Väter (Pitr loka) in Verbindung gebracht wird.
Norden ist einer der vier Himmelsrichtungen bzw. Himmelsrichtungen.
Septentrionalis ist von septentriones, „die sieben Pflugochsen“, ein Name von Ursa Major.
Beispielsweise kann kefer auf Lezgisch sowohl „Unglaube“ als auch „Norden“ bedeuten, da es nördlich der muslimischen Heimat der Lezgianer Gebiete gibt, die früher von nichtmuslimischen kaukasischen und türkischen Völkern bewohnt wurden.
Bei rotierenden astronomischen Objekten bezeichnet Norden oft die Seite, die sich aus der Ferne entlang der Rotationsachse scheinbar gegen den Uhrzeigersinn dreht.
Aber einfache Verallgemeinerungen zu diesem Thema sollten als unhaltbar betrachtet werden und als wahrscheinlich, dass sie weit verbreitete Missverständnisse über den Erdmagnetismus widerspiegeln.
Diese Konvention hat sich aus der Verwendung eines Kompasses entwickelt, der den Norden oben anzeigt.
95 % des globalen Nordens verfügen über ausreichend Nahrung und Unterkunft sowie ein funktionierendes Bildungssystem.
Die Verwendung des Begriffs „Süden“ kann auch länderbezogen sein, insbesondere in Fällen spürbarer wirtschaftlicher oder kultureller Kluft.
Selten erstreckt sich die Bedeutung auf Bolivien, und im engsten Sinne umfasst sie nur Chile, Argentinien und Uruguay.
Westen ist die Richtung, die der Rotation der Erde um ihre Achse entgegengesetzt ist, und ist daher die allgemeine Richtung, in die sich die Sonne scheinbar ständig bewegt und schließlich untergeht.
Im alten Ägypten galt der Westen als Tor zur Unterwelt und ist die Himmelsrichtung, die im Zusammenhang mit dem Tod betrachtet wird, wenn auch nicht immer mit einer negativen Konnotation.
Im Judentum geht man davon aus, dass der Westen der Shekinah (Gegenwart) Gottes zugewandt ist, so wie in der jüdischen Geschichte die Stiftshütte und der spätere Jerusalemer Tempel nach Osten ausgerichtet waren, wobei sich Gottes Gegenwart im Allerheiligsten die Stufen nach Westen hinauf befand.
Der Polarkreis ist einer der beiden Polarkreise und der nördlichste der fünf großen Breitenkreise, wie sie auf Erdkarten dargestellt sind.
Ein Breitengradkreis oder eine Breitengradlinie auf der Erde ist ein abstrakter kleiner Ost-West-Kreis, der alle Orte rund um die Erde (ohne Berücksichtigung der Höhe) an einer bestimmten Breitenkoordinatenlinie verbindet.
Breitenkreise unterscheiden sich von Längenkreisen, bei denen es sich allesamt um Großkreise mit dem Erdmittelpunkt in der Mitte handelt, da die Breitenkreise mit zunehmender Entfernung vom Äquator kleiner werden.
Ein Breitengradkreis steht senkrecht auf allen Meridianen.
Der Äquator ist der längste Breitenkreis und der einzige Breitenkreis, der auch ein Großkreis ist.
Auf einer Karte können die Breitenkreise parallel sein oder auch nicht, und ihr Abstand kann variieren, je nachdem, welche Projektion verwendet wird, um die Erdoberfläche auf eine Ebene abzubilden.
Beispielsweise sind bei einer Mercator-Projektion die Breitenkreise in der Nähe der Pole weiter voneinander entfernt, um lokale Maßstäbe und Formen beizubehalten, während bei einer Gall-Peters-Projektion die Breitenkreise in der Nähe der Pole näher beieinander liegen, sodass Flächenvergleiche möglich sind genau.
Es gibt viele kleinere Begriffe, die zu unterschiedlichen Tagesverschiebungen von einigen Metern in jede Richtung führen.
54°40'N Die Grenze zwischen russischen Territorien des 19. Jahrhunderts im Norden und widersprüchlichen amerikanischen und britischen Landansprüchen im Westen Nordamerikas.
43°30'N In den USA die Grenze zwischen Minnesota und Iowa.
42°N Ursprünglich die nördliche Grenze von Neuspanien.
41°N In den USA ein Teil der Grenze zwischen Wyoming und Utah, die Grenze zwischen Wyoming und Colorado und ein Teil der Grenze zwischen Nebraska und Colorado.
38°N Die Grenze zwischen der sowjetischen und der amerikanischen Besatzungszone in Korea und später zwischen Nordkorea und Südkorea von 1945 bis zum Koreakrieg (1950–1953).
Geografisch ist es eine westliche Verlängerung der Grenze zwischen Virginia und North Carolina und Teil der Grenze zwischen Kentucky und Tennessee.
Außerdem Teil der Grenze zwischen North Carolina und Georgia.
32°N In den USA Teil der Grenze zwischen New Mexico und Texas.
25°N Teil der Grenze zwischen Mauretanien und Mali.
17°N Die Teilung zwischen der Republik Vietnam (Südvietnam) und der Demokratischen Republik Vietnam (Nordvietnam) während des Vietnamkrieges.
8°N Teil der Grenze zwischen Somalia und Äthiopien.
7°S Ein kurzer Abschnitt der Grenze zwischen der Demokratischen Republik Kongo und Angola.
Die Künste sind ein sehr breites Spektrum menschlicher Praktiken des kreativen Ausdrucks, des Geschichtenerzählens und der kulturellen Teilhabe.
Sie können Geschick und Vorstellungskraft einsetzen, um Objekte und Performances zu produzieren, Erkenntnisse und Erfahrungen zu vermitteln und neue Umgebungen und Räume zu konstruieren.
Sie können auch einen bestimmten Aspekt einer komplexeren Kunstform entwickeln oder dazu beitragen, beispielsweise in der Kinematographie.
Die erste Bedeutung des Wortes Kunst ist „Art des Handelns“.
In ihrer grundlegendsten abstrakten Definition ist Kunst ein dokumentierter Ausdruck eines Lebewesens durch oder auf einem zugänglichen Medium, so dass jeder es sehen, hören oder erleben kann.
Eine solche öffentliche Bewertung hängt von verschiedenen subjektiven Faktoren ab.
Im antiken Griechenland wurde alle Kunst und jedes Handwerk mit demselben Wort bezeichnet: techne.
Die antike römische Kunst stellte Götter als idealisierte Menschen dar, dargestellt mit charakteristischen Unterscheidungsmerkmalen (z. B. dem Blitz des Zeus).
Charakteristisch für diesen Stil ist, dass das Lokalkolorit oft durch einen Umriss definiert wird (ein zeitgenössisches Äquivalent ist der Cartoon).
In der modernen Wissenschaft werden die Künste normalerweise mit den Geisteswissenschaften oder als Teilbereich davon gruppiert.
Das Wort Architektur kommt vom griechischen arkhitekton, „Baumeister, Bauleiter“, von αρχι- (arkhi) „Chef“ + τεκτων (tekton) „Baumeister, Zimmermann“.
Im modernen Sprachgebrauch ist Architektur die Kunst und Disziplin, ein komplexes Objekt oder System zu schaffen oder einen impliziten oder scheinbaren Plan daraus abzuleiten.
Geplante Architektur manipuliert Raum, Volumen, Textur, Licht, Schatten oder abstrakte Elemente, um eine angenehme Ästhetik zu erreichen.
Während einige Keramikprodukte als Kunst gelten, gelten andere als dekorative, industrielle oder angewandte Kunstobjekte.
In einer Töpferei oder Keramikfabrik entwirft, fertigt und dekoriert eine Gruppe von Menschen die Töpferwaren.
Im Allgemeinen geht es darum, Markierungen auf einer Oberfläche zu machen, indem man mit einem Werkzeug Druck ausübt oder ein Werkzeug über eine Oberfläche bewegt.
Die wichtigsten beim Zeichnen verwendeten Techniken sind Strichzeichnung, Schraffur, Kreuzschraffur, Zufallsschraffur, Kritzeln, Tupfen und Mischen.
Gemälde können naturalistisch und gegenständlich (wie in einem Stillleben oder Landschaftsgemälde), fotografisch, abstrakt, narrativ, symbolistisch (wie in der symbolistischen Kunst), emotional (wie im Expressionismus) oder politischer Natur (wie im Artivismus) sein.
Das Substantiv „Literatur“ kommt vom lateinischen Wort littera und bedeutet „ein einzelnes geschriebenes Zeichen (Buchstabe)“.
Jede Disziplin in den darstellenden Künsten ist zeitlicher Natur, das heißt, das Produkt wird über einen bestimmten Zeitraum aufgeführt.
Tanz wird auch verwendet, um Methoden der nonverbalen Kommunikation (siehe Körpersprache) zwischen Menschen oder Tieren (z. B. Bienentanz, Paarungstanz), Bewegung in unbelebten Objekten (z. B. im Wind tanzende Blätter) und bestimmte Musikformen oder Genres zu beschreiben .
Die Entstehung, Aufführung, Bedeutung und sogar die Definition von Musik variieren je nach Kultur und sozialem Kontext.
Der Komponist Richard Wagner erkannte die Verschmelzung so vieler Disziplinen in einem einzigen Opernwerk, beispielhaft dargestellt in seinem Zyklus „Der Ring des Nibelungen“.
Andere Werke des späten 19., 20. und 21. Jahrhunderts haben andere Disziplinen auf einzigartige und kreative Weise miteinander verbunden, beispielsweise die Performancekunst.
John Cage wird von vielen eher als Performancekünstler denn als Komponist angesehen, obwohl er die letztere Bezeichnung bevorzugte.
Die angewandte Kunst umfasst Bereiche wie Industriedesign, Illustration und kommerzielle Kunst.
Innerhalb der Sozialwissenschaften zeigen Kulturökonomen, wie das Spielen von Videospielen die Beteiligung an traditionelleren Kunstformen und kulturellen Praktiken fördert, was auf die Komplementarität zwischen Videospielen und Künsten schließen lässt.
Architektur (lateinisch architectura, vom griechischen ἀρχιτέκτων arkhitekton „Architekt“, von ἀρχι – „Chef“ und τέκτων „Schöpfer“) ist sowohl der Prozess als auch das Produkt der Planung, des Entwurfs und des Baus von Gebäuden oder anderen Bauwerken.
Die Praxis, die in der prähistorischen Zeit begann, wurde von Zivilisationen auf allen sieben Kontinenten als Ausdrucksmittel für die Kultur genutzt.
Im 19. Jahrhundert verkündete Louis Sullivan, dass „die Form der Funktion folgt“. "
Architektur begann als ländliche, mündlich gesprochene Architektur, die sich durch Versuch und Irrtum zu einer erfolgreichen Nachahmung entwickelte.
Während des europäischen Mittelalters entstanden gesamteuropäische Stile romanischer und gotischer Kathedralen und Abteien, während in der Renaissance klassische Formen bevorzugt wurden, die von namentlich bekannten Architekten umgesetzt wurden.
Der Schwerpunkt lag auf modernen Techniken, Materialien und vereinfachten geometrischen Formen und ebnete den Weg für Hochhausaufbauten.
Eine einheitliche oder kohärente Form oder Struktur.
Der wichtigste Aspekt der Schönheit war daher ein inhärenter Teil eines Objekts und nicht etwas, das oberflächlich angewendet wurde, und basierte auf universellen, erkennbaren Wahrheiten.
Im 16. Jahrhundert schrieb der italienische manieristische Architekt, Maler und Theoretiker Sebastiano Serlio Tutte L’Opere D’Architettura et Prospetiva (Gesamtwerk über Architektur und Perspektive).
Pugin glaubte, dass die gotische Architektur die einzige „wahre christliche Form der Architektur“ sei.
Zu den Philosophien, die moderne Architekten und ihre Herangehensweise an die Gebäudegestaltung beeinflusst haben, gehören Rationalismus, Empirismus, Strukturalismus, Poststrukturalismus, Dekonstruktion und Phänomenologie.
Die Architektur und der Städtebau der klassischen Zivilisationen wie der griechischen und der römischen entwickelten sich eher aus bürgerlichen als aus religiösen oder empirischen Idealen und es entstanden neue Gebäudetypen.
Texte zur Architektur werden seit der Antike verfasst.
Insbesondere die buddhistische Architektur zeigte eine große regionale Vielfalt.
Die Rolle des Architekten war in der Regel dieselbe wie die des Maurermeisters oder Magister lathomorum, wie er manchmal in zeitgenössischen Dokumenten beschrieben wird.
Gebäude wurden bestimmten Architekten zugeschrieben – Brunelleschi, Alberti, Michelangelo, Palladio – und der Personenkult hatte begonnen.
Die formelle Architekturausbildung im 19. Jahrhundert, beispielsweise an der École des Beaux-Arts in Frankreich, legte großen Wert auf die Anfertigung schöner Zeichnungen und wenig auf Kontext und Durchführbarkeit.
Besonders hervorzuheben ist der Deutsche Werkbund, der 1907 gegründet wurde, um maschinell hergestellte Objekte von besserer Qualität herzustellen.
Als moderne Architektur zum ersten Mal praktiziert wurde, war sie eine Avantgarde-Bewegung mit moralischen, philosophischen und ästhetischen Grundlagen.
Der Ansatz der Architekten der Moderne bestand darin, Gebäude auf reine Formen zu reduzieren und historische Bezüge und Ornamente zugunsten funktionaler Details zu entfernen.
Architekten wie Mies van der Rohe, Philip Johnson und Marcel Breuer arbeiteten daran, Schönheit zu schaffen, die auf den inhärenten Qualitäten von Baumaterialien und modernen Bautechniken beruhte, indem sie traditionelle historische Formen gegen vereinfachte geometrische Formen eintauschten und die neuen Mittel und Methoden zelebrierten, die die Industrie ermöglichte Revolution, einschließlich der Stahlskelettbauweise, aus der Hochhausaufbauten hervorgingen.
Die vorbereitenden Prozesse für den Entwurf eines großen Gebäudes werden immer komplizierter und erfordern Vorstudien zu Themen wie Haltbarkeit, Nachhaltigkeit, Qualität, Geld und Einhaltung lokaler Gesetze.
Umweltverträglichkeit ist zu einem Mainstream-Thema geworden, mit tiefgreifenden Auswirkungen auf den Architektenberuf.
Dieser große Wandel in der Architektur hat auch dazu geführt, dass sich die Architekturschulen stärker auf die Umwelt konzentrieren.
Das LEED-Bewertungssystem (Leadership in Energy and Environmental Design) des U.S. Green Building Council hat dabei eine entscheidende Rolle gespielt.
Dabei kann es sich auch um den ursprünglichen Entwurf und Nutzungsplan handeln, der dann später an einen geänderten Zweck umgestaltet wird, oder um einen deutlich überarbeiteten Entwurf zur adaptiven Wiederverwendung der Gebäudehülle.
Der vorläufige Entwurf des Schiffes, sein detaillierter Entwurf, der Bau, die Erprobung, der Betrieb und die Wartung, der Stapellauf und das Trockendocken sind die Hauptaktivitäten.
Umgekehrt kann sakrale Architektur als Ort der Meta-Intimität auch nicht-monolithisch, vergänglich und äußerst privat, persönlich und nicht-öffentlich sein.
Mit dem Aufstieg des Christentums und des Islam wurden religiöse Gebäude zunehmend zu Zentren der Anbetung, des Gebets und der Meditation.
Indien war von Handelsrouten von Kaufleuten aus Siraf und China durchzogen und überstand die Invasionen von Ausländern, was zu vielfältigen Einflüssen ausländischer Elemente auf einheimische Stile führte.
Ein bestehendes Beispiel befindet sich in Nalanda (Bihar).
Entsprechend den Veränderungen in der religiösen Praxis wurden Stupas nach und nach in Chaitya-Grihas (Stupas) integriert.
Buddhistische Tempel wurden erst später und außerhalb Südasiens entwickelt, wo der Buddhismus ab den ersten Jahrhunderten n. Chr. allmählich zurückging. Ein frühes Beispiel ist jedoch der Mahabodhi-Tempel in Bodh Gaya in Bihar.
Im hinduistischen Glauben repräsentiert der Tempel sowohl den Makrokosmos des Universums als auch den Mikrokosmos des inneren Raums.
Es entwickelte sich über einen Zeitraum von mehr als 2000 Jahren.
Darüber hinaus wurde der Stein durch Ziegel ersetzt, die klassische Ordnung wurde weniger streng eingehalten, Mosaike ersetzten geschnitzte Verzierungen und komplexe Kuppeln wurden errichtet.
Die frühesten Stile der islamischen Architektur brachten während der Umayyaden-Dynastie Moscheen mit „arabischem Grundriss“ oder Säulenmoscheen hervor.
In Iwan-Moscheen blicken ein oder mehrere Iwans auf einen zentralen Innenhof, der als Gebetsraum dient.
Die Spitze des Minaretts ist immer der höchste Punkt in Moscheen, die über ein Minarett verfügen, und oft auch der höchste Punkt in der unmittelbaren Umgebung.
Folglich übernahmen die Moscheearchitekten die Form des Glockenturms für ihre Minarette, die im Wesentlichen dem gleichen Zweck dienten – dem Aufruf der Gläubigen zum Gebet.
Obwohl Kuppeln normalerweise die Form einer Halbkugel hatten, machten die Moguln in Indien in Südasien und Persien zwiebelförmige Kuppeln populär.
Normalerweise befindet sich gegenüber dem Eingang zur Gebetshalle die Qibla-Wand, die den optisch hervorgehobenen Bereich innerhalb der Gebetshalle darstellt.
In der Qibla-Wand, normalerweise in der Mitte, befindet sich der Mihrab, eine Nische oder Vertiefung, die auf die Qibla-Wand hinweist.
Der Mihrab dient als Ort, an dem der Imam regelmäßig die fünf täglichen Gebete leitet.
Es besteht aus einem Kirchenschiff, Querschiffen und dem Altar am östlichen Ende (siehe Diagramm der Kathedrale).
Die meisten Architekturhistoriker betrachten Michelangelos Entwurf des Petersdoms in Rom als Vorläufer des Barockstils; Dies lässt sich an breiteren Innenräumen (anstelle langer, schmaler Kirchenschiffe), einer spielerischeren Aufmerksamkeit für Licht und Schatten, umfangreicher Verzierung, großen Fresken, einem Schwerpunkt auf Innenkunst und häufig einer dramatischen zentralen Außenprojektion erkennen.
Während weltliche Bauten eindeutig den größeren Einfluss auf die Entwicklung der modernen Architektur hatten, finden sich in religiösen Gebäuden des 20. Jahrhunderts mehrere hervorragende Beispiele moderner Architektur.
Es wurde als eine „Phalanx von Kämpfern“ beschrieben, die sich auf den Schwanz drehten und zum Himmel zeigten.
Der Tempel in Independence, Missouri, wurde vom japanischen Architekten Gyo Obata nach dem Konzept der Kammernautilus entworfen.
Die Basilika Unserer Lieben Frau von Licheń hingegen ist ein viel traditionelleres Bauwerk.
Ein Architekturstil ist eine Reihe von Merkmalen und Merkmalen, die ein Gebäude oder eine andere Struktur bemerkenswert oder historisch identifizierbar machen.
Die meiste Architektur kann in eine Chronologie von Stilen eingeordnet werden, die sich im Laufe der Zeit ändert, um veränderte Moden, Überzeugungen und Religionen oder das Aufkommen neuer Ideen, Technologien oder Materialien widerzuspiegeln, die neue Stile ermöglichen.
Zu jeder Zeit können verschiedene Stile in Mode sein, und wenn sich ein Stil ändert, geschieht dies normalerweise schrittweise, da Architekten lernen und sich an neue Ideen anpassen.
Beispielsweise entstanden Renaissance-Ideen um 1425 in Italien und verbreiteten sich in den nächsten 200 Jahren in ganz Europa, wobei die französische, deutsche, englische und spanische Renaissance erkennbar den gleichen Stil, jedoch mit einzigartigen Merkmalen, aufwies.
Nachdem ein architektonischer Stil aus der Mode gekommen ist, kann es zu Wiederbelebungen und Neuinterpretationen kommen.
Der spanische Missionsstil wurde 100 Jahre später als Mission Revival wiederbelebt und entwickelte sich bald zum spanischen Kolonial-Revival.
Ein Beispiel manieristischer Architektur ist die Villa Farnese in Caprarola in der rauen Landschaft außerhalb Roms.
Durch Antwerpen wurden Renaissance- und Manierismus-Stile in England, Deutschland sowie Nord- und Osteuropa im Allgemeinen weit verbreitet eingeführt.
Das Harmonieideal der Renaissance wich freieren und fantasievolleren Rhythmen.
Architekturtheorie ist der Akt des Nachdenkens, Diskutierens und Schreibens über Architektur.
Architekturtheorie ist oft didaktisch, und Theoretiker neigen dazu, in der Nähe der Schulen zu bleiben oder von dort aus zu arbeiten.
Dies bedeutet jedoch nicht, dass solche Werke nicht existierten, da viele Werke die Antike nie überlebt haben.
Es wurde vermutlich zwischen 27 und 23 v. Chr. verfasst und ist die einzige bedeutende zeitgenössische Quelle zur klassischen Architektur, die erhalten geblieben ist.
Es schlägt auch die drei Grundgesetze vor, denen Architektur gehorchen muss, um als solche angesehen zu werden: firmitas, utilitas, venustas, übersetzt im 17. Jahrhundert von Sir Henry Wotton in den englischen Slogan firmness, Commodity und Delight (bedeutet strukturelle Angemessenheit, funktionale Angemessenheit). , und Schönheit).
Da es sich bei den Architekturtheorien um Strukturen handelte, wurden weniger davon transkribiert.
Diese Theorien nahmen die Entwicklung des Funktionalismus in der modernen Architektur vorweg.
Dies wiederum bildete die Grundlage für den Jugendstil im Vereinigten Königreich, beispielhaft dargestellt durch die Arbeit von Charles Rennie Mackintosh, und beeinflusste die Wiener Secession.
Die im mittleren Drittel des 19. Jahrhunderts geborene Generation war weitgehend begeistert von den Möglichkeiten, die Sempers Kombination aus atemberaubendem historischem Umfang und methodischer Granularität bot.
Die Moderne lehnte diese Gedanken ab und Le Corbusier lehnte das Werk energisch ab.
Ein weiterer einflussreicher Planungstheoretiker dieser Zeit war Ebenezer Howard, der die Gartenstadtbewegung gründete.
Eine frühe Verwendung des Begriffs „moderne Architektur“ in gedruckter Form erfolgte im Titel eines Buches von Otto Wagner, der seinen Schülern Beispiele seiner eigenen, für die Wiener Secession repräsentativen Arbeiten mit Illustrationen im Jugendstil und didaktischen Lehren gab.
Frank Lloyd Wright war zwar modern in seiner Ablehnung des historischen Wiederauflebens, war aber eigenwillig in seiner Theorie, die er in zahlreichen Schriften darlegte.
Wright war poetischer und hielt fest an der Sichtweise des 19. Jahrhunderts fest, dass der kreative Künstler ein einzigartiges Genie sei.
Dies war auch bei Pädagogen im akademischen Bereich wie Dalibor Vesely oder Alberto-Perez Gomez der Fall, und in den letzten Jahren wurde diese philosophische Ausrichtung durch die Forschung einer neuen Generation von Theoretikern (z. B. Jeffrey Kipnis oder Sanford Kwinter) verstärkt.
Andere, wie Beatriz Colomina und Mary McLeod, erweitern das historische Verständnis von Architektur um kleinere oder unbedeutendere Diskurse, die die Entwicklung architektonischer Ideen im Laufe der Zeit beeinflusst haben.
In ihren Theorien wird Architektur mit einer Sprache verglichen, die bei jedem Gebrauch neu erfunden und neu erfunden werden kann.
Seit dem Jahr 2000 muss sich auch die Architekturtheorie dem rasanten Aufstieg von Urbanismus und Globalisierung stellen.
Im letzten Jahrzehnt ist die sogenannte „digitale“ Architektur entstanden.
Auch Architekten entwerfen organisch anmutende Gebäude und versuchen, eine neue Formensprache zu entwickeln.
Seit dem Aufkommen dieser neuen architektonischen Tendenzen beschäftigen sich viele Theoretiker und Architekten mit diesen Themen und entwickeln Theorien und Ideen wie Patrick Schumachers Parametrismus.
Byzantinische Architektur ist die Architektur des Byzantinischen Reiches oder Oströmischen Reiches.
Prächtige goldene Mosaike brachten mit ihrer grafischen Einfachheit Licht und Wärme in die Herzen der Kirchen.
Einige der Säulen bestanden ebenfalls aus Marmor.
Edelholzmöbel wie Betten, Stühle, Hocker, Tische, Bücherregale und silberne oder goldene Tassen mit wunderschönen Reliefs schmückten byzantinische Innenräume.
Bei klassischen Tempeln war nur das Äußere wichtig, da nur die Priester das Innere betraten, wo die Statue der Gottheit aufbewahrt wurde, der der Tempel geweiht war.
Besonders diejenigen in der Markuskathedrale in Venedig (1071) erregten John Ruskins Interesse.
Auf den östlichen Säulen sind gelegentlich der Adler, der Löwe und das Lamm geschnitzt, jedoch konventionell behandelt.
Verbundsäulen säumen den Hauptraum des Kirchenschiffs.
Die Säulen sind mit Blattwerk in den verschiedensten Variationen gefüllt.
Weitere Bauwerke sind die Ruinen des Großen Palastes von Konstantinopel, die innovativen Mauern von Konstantinopel (mit 192 Türmen) und die Basilika-Zisterne (mit Hunderten recycelter klassischer Säulen).
Die paläologische Zeit ist in einem Dutzend ehemaliger Kirchen in Istanbul gut vertreten, insbesondere in St. Saviour in Chora und St. Mary Pammakaristos.
Die Kirche der Heiligen Apostel (Thessaloniki) gilt als archetypisches Bauwerk der Spätzeit, deren Außenwände aufwendig mit komplexen Ziegelmustern oder glasierter Keramik verziert sind.
Bei den zentralen Kirchen St. Sergius (Konstantinopel) und San Vitale (Ravenna) wurde der Raum unter der Kuppel durch Apsidenzusätze am Achteck vergrößert.
Dieses ununterbrochene Gebiet mit einer Länge von etwa 80 m und einer Breite von mehr als 30 m ist vollständig von einem System von Wohnflächen bedeckt.
Bei den Heiligen Aposteln (6. Jahrhundert) wurden fünf Kuppeln auf einem kreuzförmigen Grundriss angebracht; die zentrale Kuppel war die höchste.
Manchmal war der zentrale Raum quadratisch, manchmal achteckig, oder zumindest gab es acht statt vier Pfeiler, die die Kuppel stützten, und das Kirchenschiff und die Querschiffe waren im Verhältnis schmaler.
Noch davor steht ein quadratisches Gericht.
Direkt unter der Mitte der Kuppel befindet sich der Ambo, von dem aus die Heiligen Schriften verkündet wurden, und unter dem Ambo auf Bodenhöhe befand sich der Platz für den Sängerchor.
Reihen ansteigender Sitze rund um die Rundung der Apsis mit dem Thron des Patriarchen an der mittleren östlichen Spitze bildeten das Synthronon.
Die Kuppeln und Gewölbe an der Außenseite waren mit Blei oder mit Ziegeln römischer Art bedeckt.
In den markanten frühislamischen Denkmälern in Syrien (709–715) sind erhebliche byzantinische Einflüsse erkennbar.
Es wurden Ziegel mit den Maßen 70 cm x 35 cm x 5 cm verwendet, die mit einem ca. 5 cm dicken Mörtel zusammengeklebt wurden.
Das vielleicht auffälligste Merkmal der Hagia Irene ist der strenge Kontrast zwischen Innen- und Außendesign.
Dieser Stil beeinflusste den Bau mehrerer anderer Gebäude, beispielsweise des Petersdoms.
Der Bau der endgültigen Version der Hagia Sophia, die noch heute steht, wurde von Kaiser Justinian beaufsichtigt.
Gotische Architektur (oder Spitzarchitektur) ist ein Baustil, der in Europa vom späten 12. bis zum 16. Jahrhundert, im Hoch- und Spätmittelalter, besonders beliebt war und in einigen Gegenden bis ins 17. und 18. Jahrhundert überlebte.
Der damalige Stil wurde manchmal als Opus Francigenum (wörtl.
Die wichtigste technische Innovation und eine der anderen charakteristischen Designkomponenten ist der Schwibbogen.
Es gibt jedoch keine Hinweise darauf, dass ein Zusammenhang zwischen der armenischen Architektur und der Entwicklung des gotischen Stils in Westeuropa bestand.
Daher war der gotische Stil, der im Gegensatz zur klassischen Architektur stand, aus dieser Sicht mit der Zerstörung von Fortschritt und Raffinesse verbunden.
Der Begriff „Sarazenen“ war noch im 18. Jahrhundert gebräuchlich und bezog sich typischerweise auf alle muslimischen Eroberer, einschließlich der Mauren und Araber.
Seine Abneigung gegen den Stil war so groß, dass er sich weigerte, der neuen St. Pauls-Kirche ein gotisches Dach zu verpassen, obwohl er dazu gedrängt wurde.
Mehrere Autoren haben sich gegen diese Behauptung ausgesprochen und behauptet, dass der gotische Stil höchstwahrscheinlich auf anderen Wegen, beispielsweise über Spanien oder Sizilien, nach Europa gelangt sei.
Es wurde auch von theologischen Lehren beeinflusst, die mehr Licht forderten, und von technischen Verbesserungen bei Gewölben und Strebepfeilern, die eine viel größere Höhe und größere Fenster ermöglichten.
Rippengewölbe wurden in einigen Teilen der Kathedrale von Durham (1093–) und in der Abtei von Lessay in der Normandie (1098) verwendet.
Das Herzogtum Normandie, bis zum 13. Jahrhundert Teil des Anjou-Reiches, entwickelte eine eigene Version der Gotik.
Ein Beispiel der frühen normannischen Gotik ist die Kathedrale von Bayeux (1060–70), wo das romanische Kirchenschiff und der Chor im gotischen Stil umgebaut wurden.
Die Kathedrale von Coutances wurde ab etwa 1220 im gotischen Stil umgebaut.
Suger rekonstruierte Teile der alten romanischen Kirche mit Kreuzrippengewölbe, um Wände zu entfernen und mehr Platz für Fenster zu schaffen.
Darüber hinaus installierte er über dem Portal an der Fassade eine kreisrunde Rosette.
Die Kathedrale von Durham war die erste Kathedrale mit einem Kreuzrippengewölbe und wurde zwischen 1093 und 1104 erbaut.
Einer der Baumeister, der vermutlich an der Kathedrale von Sens gearbeitet hat, Wilhelm von Sens, reiste später nach England und wurde der Architekt, der zwischen 1175 und 1180 den Chor der Kathedrale von Canterbury im neuen gotischen Stil umbaute.
Französische gotische Kirchen wurden sowohl durch den Chorumgang und die Seitenkapellen rund um den Chor von Saint-Denis als auch durch die paarigen Türme und Dreifachtüren an der Westfassade stark beeinflusst.
Die Erbauer von Notre-Dame gingen noch einen Schritt weiter und führten Strebepfeiler ein, schwere Stützpfeiler außerhalb der Mauern, die durch Bögen mit den oberen Mauern verbunden waren.
Sein Werk wurde von Wilhelm dem Engländer fortgeführt, der 1178 seinen französischen Namensvetter ablöste.
Tiercerons – dekorative Gewölberippen – scheinen erstmals bei Gewölben der Kathedrale von Lincoln verwendet worden zu sein, die um 1200 installiert wurden.
Der erste Bau der Hochgotik war die Kathedrale von Chartres, eine bedeutende Wallfahrtskirche südlich von Paris.
Die Wände waren mit Buntglasfenstern gefüllt, die hauptsächlich die Geschichte der Jungfrau Maria darstellten, aber in einer kleinen Ecke jedes Fensters auch das Handwerk der Zünfte illustrierten, die diese Fenster gestiftet hatten.
In Mitteleuropa tauchte der hochgotische Stil im Heiligen Römischen Reich auf, zuerst in Toul (1220–), dessen romanische Kathedrale im Stil der Kathedrale von Reims umgebaut wurde; dann die Liebfrauenkirche in Trier (1228–) und dann im gesamten Reich, beginnend mit der Elisabethkirche in Marburg (1235–) und dem Dom in Metz (um 1235–).
Spitzbogenfenster wurden durch mehrere Lichter ersetzt, die durch geometrische Balkenmaßwerke getrennt waren.
Weitere Merkmale der Hochgotik waren die Entwicklung größerer Rosettenfenster mit Maßwerk, höhere und längere Strebepfeiler, die bis zu den höchsten Fenstern reichen konnten, und Skulpturenwände mit biblischen Geschichten, die die Fassade und die Fronten füllten das Querschiff.
Die hohen und dünnen Wände der französischen Rayonnant-Gotik, die durch die Strebepfeiler ermöglicht wurden, ermöglichten immer anspruchsvollere Flächen aus Glas und verziertem Maßwerk, verstärkt durch Eisenarbeiten.
Masons erarbeitete eine Reihe von Maßwerkmustern für Fenster – von den einfachen geometrischen über die netzartigen bis hin zu den krummlinigen Mustern –, die das Spitzbogenfenster abgelöst hatten.
Zu den Kirchen mit Merkmalen dieses Stils gehören die Westminster Abbey (1245–), die Kathedralen von Lichfield (nach 1257–) und Exeter (1275–), die Bath Abbey (1298–) und der Retro-Chor der Wells Cathedral (ca. 1320–). .
Besonders verbreitet war die Verwendung von Ogees.
Beispiele für französisches Flamboyant-Gebäude sind die Westfassade der Kathedrale von Rouen und insbesondere die Fassaden der Sainte-Chapelle de Vincennes (1370er Jahre) und der Abteikirche Mont-Saint-Michel (1448).
Es erschien erstmals im Kreuzgang und im Kapitelsaal (ca. 1332) der Old St Paul's Cathedral in London von William de Ramsey.
Senkrecht wird manchmal auch als „dritte Spitze“ bezeichnet und wurde über drei Jahrhunderte lang verwendet; die fächergewölbte Treppe der Christ Church in Oxford, erbaut um 1640.
Die Könige von Frankreich kannten den neuen italienischen Stil aus erster Hand aufgrund des Feldzugs Karls VIII. nach Neapel und Mailand (1494) und insbesondere der Feldzüge Ludwigs XII. und Franz I. (1500–1505) zur Wiederherstellung der französischen Kontrolle über Mailand und Genua.
Das Château de Blois (1515–24) führte die Renaissance-Loggia und die offene Treppe ein.
Unter Heinrich VIII. und Elisabeth I. war England von der architektonischen Entwicklung auf dem Kontinent weitgehend isoliert.
Shute veröffentlichte 1570 das erste Buch in englischer Sprache über klassische Architektur.
Der Spitzbogen hat seinen Ursprung nicht in der gotischen Architektur; Sie wurden im Nahen Osten seit Jahrhunderten sowohl in der vorislamischen als auch in der islamischen Architektur für Bögen, Arkaden und Kreuzrippengewölbe eingesetzt.
Manchmal wurden sie auch für praktischere Zwecke verwendet, etwa um Quergewölbe auf die gleiche Höhe wie Diagonalgewölbe zu bringen, wie im Kirchenschiff und den Gängen der 1093 erbauten Kathedrale von Durham.
Im Gegensatz zum halbkreisförmigen Tonnengewölbe römischer und romanischer Gebäude, bei dem das Gewicht direkt nach unten drückte und dicke Wände und kleine Fenster erforderte, bestand das gotische Rippengewölbe aus sich diagonal kreuzenden Bogenrippen.
Dem nach außen gerichteten Druck gegen die Mauern wurde durch das Gewicht von Strebepfeilern und späteren Strebepfeilern entgegengewirkt.
Sie waren sehr schwierig zu bauen und konnten nur einen begrenzten Raum durchqueren.
Die abwechselnden Reihen abwechselnder Säulen und Pfeiler, die das Gewicht der Gewölbe tragen, wurden durch einfache Pfeiler ersetzt, die jeweils das gleiche Gewicht trugen.
Das erste dieser neuen Gewölbe hatte eine zusätzliche Rippe, ein sogenanntes Tierceron, das entlang der Mitte des Gewölbes verlief.
Diese Gewölbe kopierten oft die Formen des kunstvollen Maßwerks der Spätgotik.
Ein zweiter Typ wurde ein Netzgewölbe genannt, das ein Netzwerk zusätzlicher dekorativer Rippen in Dreiecken und anderen geometrischen Formen aufwies, die zwischen oder über den Querrippen platziert waren.
Ein Beispiel ist der Kreuzgang der Kathedrale von Gloucester (um 1370).
Sie wurden später in Sens, in Notre-Dame de Paris und in Canterbury in England verwendet.
In der Zeit der Hochgotik wurde eine neue Form eingeführt, die aus einem zentralen Kern bestand, der von mehreren angebauten schlanken Säulen oder Kolonetten umgeben war, die bis zu den Gewölben reichten.
In England wurden die gebündelten Säulen häufig mit Steinringen verziert, ebenso wie Säulen mit geschnitzten Blättern.
Anstelle des korinthischen Kapitells verwendeten einige Säulen ein steifes Blattdesign.
In späteren Bauwerken hatten die Strebepfeiler oft mehrere Bögen, die jeweils bis zu einer anderen Ebene des Bauwerks reichten.
Die Bögen hatten einen zusätzlichen praktischen Zweck; sie enthielten Bleikanäle, die Regenwasser vom Dach ableiteten; Es wurde aus den Mündern steinerner Wasserspeier ausgestoßen, die in Reihen auf den Strebepfeilern aufgestellt waren.
Sie hatten auch einen praktischen Zweck; Sie dienten oft als Glockentürme, die Glockentürme trugen, deren Glocken die Uhrzeit anzeigten, indem sie Gottesdienste ankündigten, vor Feuer oder feindlichen Angriffen warnten und besondere Anlässe wie militärische Siege und Krönungen feierten.
Da der Bau einer Kathedrale in der Regel viele Jahre dauerte und extrem teuer war, schwand die öffentliche Begeisterung, als der Turm gebaut werden sollte, und der Geschmack änderte sich.
Chartres wäre noch ausgelassener gewesen, wenn der zweite Plan befolgt worden wäre; es sah sieben Türme rund um das Querschiff und das Heiligtum vor.
Die früh- und hochgotische Kathedrale von Laon hat einen quadratischen Laternenturm über der Vierung des Querschiffs; zwei Türme an der Westfront; und zwei Türme an den Enden der Querschiffe.
In der Normandie hatten Kathedralen und große Kirchen oft mehrere Türme, die im Laufe der Jahrhunderte gebaut wurden. Die Abbaye aux Hommes (begonnen 1066) in Caen verfügt über neun Türme und Türme, die an der Fassade, den Querschiffen und in der Mitte angebracht sind.
Eine Variante der Turmspitze war die Flèche, eine schlanke, speerartige Turmspitze, die normalerweise am Querschiff angebracht wurde, wo sie das Kirchenschiff kreuzte.
Die Kathedrale von Amiens hat einen Turm.
Es wurde 1786 im Zuge eines Modernisierungsprogramms der Kathedrale entfernt, aber in einer neuen, von Eugène Viollet-le-Duc entworfenen Form wieder aufgebaut.
In der englischen Gotik befand sich der Hauptturm oft an der Kreuzung von Querschiff und Kirchenschiff und war viel höher als die anderen.
Ein Vierungsturm wurde 1493–1501 an der Kathedrale von Canterbury von John Wastell errichtet, der zuvor am King's College in Cambridge gearbeitet hatte.
In der Mitte der Kreuzung musste ein ungewöhnlicher Doppelbogen errichtet werden, um dem Turm die zusätzliche Unterstützung zu geben, die er brauchte.
Der Bau begann 1724 erneut nach dem Entwurf von Nicholas Hawksmoor, nachdem zunächst Christopher Wren 1710 einen Entwurf vorgeschlagen hatte, wurde jedoch 1727 erneut eingestellt.
Der Bau des Kölner Doms wurde im 13. Jahrhundert nach dem Plan des Doms von Amiens begonnen, in der Gotik wurden jedoch nur die Apsis und der Sockel eines Turms fertiggestellt.
Der Turm des Ulmer Münsters hat eine ähnliche Geschichte: Er begann 1377, endete 1543 und wurde erst im 19. Jahrhundert fertiggestellt.
Die Kathedrale von Burgos war eher von Nordeuropa inspiriert.
Plattenmaßwerk war der erste Maßwerktyp, der entwickelt wurde und in der späteren Phase der Frühgotik oder First Pointed entstand.
Maßwerk ist sowohl praktisch als auch dekorativ, denn die immer größeren Fenster gotischer Gebäude benötigten maximalen Halt gegen den Wind.
Den Höhepunkt seiner Raffinesse erreichte das Plattenmaßwerk in den Fenstern der Kathedrale von Chartres aus dem 12. Jahrhundert und im Rosettenfenster „Dean's Eye“ in der Kathedrale von Lincoln.
Maßwerk aus Stein, ein wichtiges dekoratives Element des gotischen Stils, wurde erstmals kurz nach 1211 in der Kathedrale von Reims in der von Jean D'Orbais erbauten Apsis verwendet.
Ab ca. 1240 wurde Maßwerk mit zunehmender Komplexität und abnehmendem Gewicht üblich.
Rayonnant verwendete auch Zierleisten zweier unterschiedlicher Arten im Maßwerk, wo frühere Stile Zierleisten einer einzigen Größe mit unterschiedlich großen Pfosten verwendet hatten.
Die Pfosten des geometrischen Stils hatten typischerweise Kapitelle, aus denen gebogene Balken hervorgingen.
Die Pfosten verzweigten sich daraufhin in Y-förmige Muster, die zusätzlich mit Zacken verziert waren.
In Second Pointed (14. Jahrhundert) wurden sich überkreuzende Maßwerke mit Ogees ausgearbeitet, wodurch ein komplexes netzartiges Muster entstand, das als retikuliertes Maßwerk bekannt ist.
Diese Formen werden als Dolche, Fischblasen oder Mouchettes bezeichnet.
Perpendicular strebte nach Vertikalität und verzichtete auf die geschwungenen Linien des Curvilinear-Stils zugunsten ununterbrochener, gerader Pfosten von oben bis unten, die von horizontalen Riegeln und Balken durchzogen sind.
Die Riegel waren oft mit kleinen Zinnen gekrönt.
Es bedeckte häufig die Fassaden und die Innenwände des Kirchenschiffs und des Chors waren mit Blendarkaden bedeckt.
2 Tonnengewölbe oder Kreuzrippengewölbe Kreuzrippengewölbe entstanden in der Romanik und wurden in der Gotik ausgearbeitet.
Sie haben ein langes Kirchenschiff, das den Hauptteil der Kirche bildet, in der die Gemeindemitglieder ihren Gottesdienst feierten. ein Querschiff, das Querschiff genannt wurde, und dahinter im Osten der Chor, auch Chor oder Presbyterium genannt, der normalerweise dem Klerus vorbehalten war.
Ein Gang, der Ambulatorium genannt wurde, umgab den Chor.
Die frühen Kathedralen wie Notre-Dame hatten sechsteilige Rippengewölbe mit abwechselnden Säulen und Pfeilern, während spätere Kathedralen einfachere und stärkere vierteilige Gewölbe mit identischen Säulen hatten.
In der frühgotischen Architektur Frankreichs waren die Querschiffe meist kurz, in der Rayonnant-Zeit wurden sie jedoch länger und erhielten große Rosettenfenster.
In England waren Querschiffe wichtiger, und die Grundrisse waren in der Regel viel komplexer als in französischen Kathedralen, mit der Hinzufügung von angeschlossenen Marienkapellen, einem achteckigen Kapitelsaal und anderen Bauwerken (siehe Pläne der Kathedrale von Salisbury und des York Minster unten).
Eine Erhebung hatte typischerweise vier Ebenen.
Darüber befand sich eine schmalere Galerie, das sogenannte Triforium, das ebenfalls für zusätzliche Dicke und Halt sorgte.
Dieses System wurde in der Noyon-Kathedrale, der Sens-Kathedrale und anderen frühen Bauwerken verwendet.
Die Tribüne verschwand, wodurch die Arkaden höher sein konnten.
Eine ähnliche Anordnung wurde in England in der Kathedrale von Salisbury, der Kathedrale von Lincoln und der Kathedrale von Ely übernommen.
Möglich wurde dies durch die Entwicklung des Strebepfeilers, der die Last des Dachgewichts auf die Stützen außerhalb der Wände übertrug.
Die Kathedrale von Beauvais erreichte mit der gotischen Technik die Grenzen des Machbaren.
Gotische Fassaden wurden nach dem Vorbild der romanischen Fassaden gestaltet.
Die Skulptur des zentralen Tympanons war dem Jüngsten Gericht gewidmet, die Skulptur links der Jungfrau Maria und die Skulptur rechts den Heiligen, die in dieser besonderen Kathedrale geehrt wurden.
Sie folgten der vom Heiligen Thomas von Aquin zum Ausdruck gebrachten Lehre, dass Schönheit eine „Harmonie der Kontraste“ sei.
In England wurde das Rosettenfenster oft durch mehrere Spitzbogenfenster ersetzt.
Die Portale waren mit hohen Bogengiebeln gekrönt, die aus konzentrischen Bögen mit Skulpturen bestanden.
Die Türme waren mit eigenen Bögen geschmückt, oft gekrönt von Zinnen.
Während französische Kathedralen die Höhe der Fassade betonten, legten englische Kathedralen, insbesondere in der früheren Gotik, oft Wert auf die Breite.
Er löste sich von der französischen Betonung der Höhe, entfernte die Säulenstatuen und Statuen in den gewölbten Eingängen und bedeckte die Fassade mit farbenfrohen Mosaiken biblischer Szenen (die aktuellen Mosaike stammen aus einem späteren Zeitpunkt).
Der Bildhauer Andrea Pisano fertigte die berühmten Bronzetüren für das Baptisterium von Florenz (1330–1336).
Um den Chor und das östliche Ende herum gibt es normalerweise einen einfachen oder doppelten Chorumgang bzw. Gang, sodass Gemeindemitglieder und Pilger problemlos im östlichen Ende herumlaufen können.
Abt Suger verwendete zunächst die neuartige Kombination aus Rippengewölben und Strebepfeilern, um die dicken Wände durch Buntglas zu ersetzen und so diesen Teil der Kirche für das zu öffnen, was er als „göttliches Licht“ ansah.
Es gibt drei solcher Kapellen in der Kathedrale von Chartres, sieben in Notre Dame de Paris, in der Kathedrale von Amiens, im Prager Dom und im Kölner Dom sowie neun in der Basilika des Heiligen Antonius von Padua in Italien.
In einem Edikt des Zweiten Konzils von Nicäa aus dem Jahr 787 hieß es: „Die Komposition religiöser Bilder darf nicht der Inspiration von Künstlern überlassen werden; sie orientiert sich an den Grundsätzen der katholischen Kirche und der religiösen Tradition.“
Mit der Weiterentwicklung des Stils rückte die Skulptur nach und nach immer stärker in den Vordergrund, übernahm die Säulen des Portals und kletterte nach und nach über die Portale, bis Statuen in Nischen die gesamte Fassade bis zu den Querschiffen bedeckten, wie in der Kathedrale von Wells, und wie in der Kathedrale von Amiens, auch an der Innenseite der Fassade.
Dies legte ein Muster komplexer Ikonographie fest, das auch in anderen Kirchen übernommen wurde.
Das Tympanon über dem zentralen Portal an der Westfassade von Notre-Dame de Paris veranschaulicht anschaulich das Jüngste Gericht: Sünder werden in die Hölle geführt und gute Christen in den Himmel aufgenommen.
Die Qualen der Hölle wurden noch anschaulicher dargestellt.
Sie waren Teil der visuellen Botschaft für die ungebildeten Gläubigen, Symbole des Bösen und der Gefahr, die diejenigen bedrohten, die den Lehren der Kirche nicht folgten.
Sie wurden durch Figuren im gotischen Stil ersetzt, die Eugène Viollet-le-Duc während der Restaurierung im 19. Jahrhundert entworfen hatte.
Religiöse Lehren im Mittelalter, insbesondere die Schriften von Pseudo-Dionysius dem Areopagiten, einem Mystiker aus dem 6. Jahrhundert, dessen Buch „De Coelesti Hierarchia“ bei Mönchen in Frankreich beliebt war, lehrten, dass alles Licht göttlich sei.
Die Fenster auf der Nordseite, oft im Schatten, hatten Fenster mit Darstellungen des Alten Testaments.
Die Details wurden mit Glasemaille auf das Glas gemalt und dann in einem Ofen gebrannt, um die Emaille auf dem Glas zu verschmelzen.
Sainte-Chapelle wurde zum Vorbild für andere Kapellen in ganz Europa.
Klares Glas wurde in farbiges Glas getaucht, dann wurden Teile des farbigen Glases abgeschliffen, um genau den richtigen Farbton zu erhalten.
Eines der berühmtesten extravaganten Gebäude war die Sainte-Chapelle de Vincennes (1370er Jahre) mit Glaswänden vom Boden bis zur Decke.
Die Herstellung der Buntglasfenster war äußerst aufwendig und teuer.
Die Rose war ein Symbol der Jungfrau Maria und wurde besonders in ihr gewidmeten Kirchen, einschließlich Notre-Dame de Paris, verwendet.
Der Bau des Palais de la Cité in Paris, in der Nähe von Notre-Dame de Paris, begann im Jahr 1119 und war bis 1417 die Hauptresidenz der französischen Könige.
Durch die Entwicklung der Artillerie wurde es jedoch bald überflüssig und im 15. Jahrhundert wurde es in einen komfortablen Wohnpalast umgebaut.
Das älteste erhaltene Beispiel in England ist wahrscheinlich das Mob Quad des Merton College an der Universität Oxford, das zwischen 1288 und 1378 erbaut wurde.
Ein ähnlicher akademischer Kreuzgang wurde in den 1140er Jahren am Queen's College in Oxford errichtet, wahrscheinlich nach einem Entwurf von Reginald Ely.
Einige Colleges, wie das Balliol College in Oxford, haben den militärischen Stil gotischer Burgen übernommen, mit Zinnen und zinnenbewehrten Mauern.
Er schrieb 1447, dass er wollte, dass seine Kapelle „in großer Form, sauber und substanziell bleibt und den Überfluss an allzu großen, seltsamen Arbeiten mit Fideikommisse und geschäftiger Formgebung hervorhebt“.
Die Mauern hatten innen zwei Ebenen mit Laufgängen, eine zinnenbewehrte Brüstung mit Zinnen und vorspringende Pechnasen, von denen aus Raketen auf Belagerer abgeworfen werden konnten.
Burgen waren von einem tiefen Wassergraben umgeben, der von einer einzigen Zugbrücke überspannt wurde.
Ein gutes erhaltenes Beispiel ist das Château de Dourdan in der Nähe von Nemours.
Die Umstellung bedeutete Kompromisse, da lateinische Kirchen nach Osten und Moscheen nach Mekka ausgerichtet sind.
Lala-Mustafa-Pascha-Moschee in Famagusta, Nordzypern.
Der gotische Stil wurde als veraltet, hässlich und sogar barbarisch beschrieben.
Irland war im 17. und 18. Jahrhundert eine Insel gotischer Architektur. Der Bau der Kathedrale von Derry (abgeschlossen 1633), der Kathedrale von Sligo (ca. 1730) und der Kathedrale von Down (1790–1818) sind weitere Beispiele.
Die beiden Westtürme der Westminster Abbey wurden zwischen 1722 und 1745 von Nicholas Hawksmoor erbaut und eröffneten damit eine neue Periode des gotischen Wiederauflebens.
Diese Periode allgemeinerer Anziehungskraft, die sich von 1855 bis 1885 erstreckt, ist in Großbritannien als Hochviktorianische Gotik bekannt.
Ab der zweiten Hälfte des 19. Jahrhunderts wurde es in Großbritannien immer üblicher, die Neugotik bei der Gestaltung nichtkirchlicher und nichtstaatlicher Gebäudetypen zu verwenden.
Landschaftsarchitekten bearbeiten Strukturen und Außenräume im Landschaftsaspekt des Entwurfs – groß oder klein, städtisch, vorstädtisch und ländlich, und mit „harten“ (gebauten) und „weichen“ (bepflanzten) Materialien unter Einbeziehung ökologischer Nachhaltigkeit.
Sie können auch Vorschläge zur Genehmigung und Überwachung von Verträgen für die Bauarbeiten prüfen.
Der erste Mensch, der über die Schaffung einer Landschaft schrieb, war Joseph Addison im Jahr 1712.
Im späten 19. Jahrhundert wurde der Begriff „Landschaftsarchitekt“ erstmals von professionellen Landschaftsarchitekten verwendet und etablierte sich fest, nachdem Frederick Law Olmsted Jr. und Beatrix Jones (später Farrand) zusammen mit anderen die American Society of Landscape Architects (ASLA) gründeten 1899.
Ihre Projekte können von Standortuntersuchungen bis hin zur ökologischen Bewertung großer Gebiete für Planungs- oder Managementzwecke reichen.
Ihre Arbeit ist in schriftlichen Grundsatz- und Strategieerklärungen verankert und ihr Aufgabenbereich umfasst die Masterplanung für neue Entwicklungen, Landschaftsbewertungen und -bewertungen sowie die Erstellung von Landschaftsmanagement- oder Politikplänen.
In den letzten Jahren sind der Bedarf und das Interesse an therapeutischen Gärten zunehmend gestiegen.
Dazu gehörten der Central Park in New York City, der Prospect Park in Brooklyn, New York und das Emerald Necklace-Parksystem in Boston.
Sie war Designberaterin für über ein Dutzend Universitäten, darunter: Princeton in Princeton, New Jersey; Yale in New Haven, Connecticut; und das Arnold Arboretum für Harvard in Boston, Massachusetts.
Stadtplaner sind qualifiziert, Aufgaben unabhängig von Landschaftsarchitekten auszuführen, und im Allgemeinen bereitet der Lehrplan von Landschaftsarchitekturprogrammen Studierende nicht darauf vor, Stadtplaner zu werden.
Roberto Burle Marx kombinierte in Brasilien den internationalen Stil und einheimische brasilianische Pflanzen und Kultur zu einer neuen Ästhetik.
Er verbreitete ein System zur Analyse der Schichten einer Stätte, um ein umfassendes Verständnis der qualitativen Merkmale eines Ortes zu erlangen.
Nach der AILA-Anerkennung führen Landschaftsarchitekten in allen sechs Bundesstaaten und Territorien Australiens den Titel „Registered Landscape Architect“.
Innerhalb Neuseelands können Mitglieder von NZILA, sobald sie ihre berufliche Stellung erreicht haben, den Titel „Registered Landscape Architect NZILA“ führen.
Die Mission von ILASA besteht darin, den Beruf der Landschaftsarchitektur voranzutreiben und hohe Standards professioneller Dienstleistungen für seine Mitglieder aufrechtzuerhalten sowie den Beruf der Landschaftsarchitektur in allen Angelegenheiten zu vertreten, die die Interessen der Mitglieder des Instituts berühren könnten.
Derzeit gibt es im Vereinigten Königreich fünfzehn akkreditierte Programme.
Im Jahr 2008 startete das LI eine große Rekrutierungskampagne mit dem Titel „Ich möchte Landschaftsarchitekt werden“, um das Studium der Landschaftsarchitektur zu fördern.
In einigen Bundesstaaten ist auch das Bestehen eines Staatsexamens erforderlich.
Bereits im 6. Jahrhundert v. Chr. bedeckte der Sand die Statuen des Haupttempels bis zu den Knien.
Ein Plan zur Rettung der Tempel basierte auf einer Idee von William MacQuitty, einen klaren Süßwasserdamm um die Tempel zu errichten, wobei das Wasser im Inneren auf der gleichen Höhe wie der Nil gehalten werden sollte.
Sie waren der Ansicht, dass durch die Erhöhung der Tempel die Auswirkungen der Erosion des Sandsteins durch Wüstenwinde ignoriert wurden.
Zwischen 1964 und 1968 wurde das gesamte Gelände sorgfältig in große Blöcke (bis zu 30 Tonnen, durchschnittlich 20 Tonnen) zerschnitten, abgebaut, angehoben und an einem neuen Standort 65 Meter höher und 200 Meter hinter dem Fluss, einem der größten, wieder aufgebaut Herausforderungen der archäologischen Technik in der Geschichte.
Viele Besucher reisen auch mit dem Flugzeug über einen eigens für die Tempelanlage errichteten Flugplatz oder über die Straße von Assuan, der nächstgelegenen Stadt, an.
Die Kolossalstatuen entlang der linken Wand tragen die weiße Krone Oberägyptens, während die auf der gegenüberliegenden Seite die Doppelkrone Ober- und Unterägyptens (Pschent) tragen.
Das berühmteste Relief zeigt den König, der auf seinem Streitwagen Pfeile gegen seine flüchtenden Feinde schießt, die gefangen genommen werden.
Es gibt Darstellungen von Ramses und Nefertari mit den heiligen Booten von Amun und Ra-Horakhty.
Bei diesen Daten handelt es sich angeblich um den Geburtstag bzw. den Krönungstag des Königs.
Tatsächlich muss dieses Datum laut Berechnungen, die auf der Grundlage des heliakischen Aufgangs des Sterns Sirius (Sothis) und von Archäologen gefundenen Inschriften erstellt wurden, der 22. Oktober gewesen sein.
Dies war tatsächlich das zweite Mal in der Geschichte des alten Ägypten, dass ein Tempel einer Königin gewidmet wurde.
Traditionell standen die Statuen der Königinnen neben denen des Pharaos, waren aber nie höher als seine Knie.
Die Kapitelle der Säulen tragen das Gesicht der Göttin Hathor; Diese Art von Säule ist als Hathoric bekannt.
An der Süd- und der Nordwand dieser Kammer befinden sich zwei anmutige und poetische Basreliefs, die den König und seine Gemahlin zeigen, wie sie Hathor, die als Kuh auf einem Boot dargestellt ist, das durch ein Dickicht aus Papyri segelt, Papyruspflanzen überreichen.
Es wird angenommen, dass keines der heutigen Gebäude aus der Zeit vor dem 17. Jahrhundert stammt, aber sie wurden wahrscheinlich mit den gleichen Baumethoden und Designs errichtet, die schon Jahrhunderte zuvor verwendet wurden.
Entlang dieser Route befanden sich weitere Kasbahs und Ksour, beispielsweise das nahegelegene Tamdaght im Norden.
Die Gebäude des Dorfes sind innerhalb einer Verteidigungsmauer mit Ecktürmen und einem Tor zusammengefasst.
Das Dorf verfügt außerdem über eine Reihe öffentlicher oder gemeinschaftlicher Gebäude wie eine Moschee, eine Karawanserei, eine Kasbah (burgartige Festung) und den Marabout von Sidi Ali oder Amer.
Es bestand aus komprimierter Erde und Schlamm, die normalerweise mit anderen Materialien vermischt wurden, um die Haftung zu verbessern.
Der Assuan-Staudamm, oder genauer gesagt seit den 1960er Jahren der Assuan-Staudamm, ist der weltweit größte Staudamm, der zwischen 1960 und 1970 in Assuan, Ägypten, über den Nil gebaut wurde.
Wie die frühere Umsetzung hatte auch der Staudamm erhebliche Auswirkungen auf die Wirtschaft und Kultur Ägyptens.
Diese natürlichen Überschwemmungen waren jedoch unterschiedlich, da Hochwasserjahre die gesamte Ernte zerstören konnten, während Niedrigwasserjahre zu ausgedehnter Dürre und in der Folge zu Hungersnöten führen konnten.
Stattdessen wurde der Niltalplan des britischen Hydrologen Harold Edwin Hurst favorisiert, der vorsah, Wasser im Sudan und in Äthiopien zu speichern, wo die Verdunstung viel geringer ist.
Ursprünglich waren sowohl die Vereinigten Staaten als auch die UdSSR daran interessiert, die Entwicklung des Staudamms zu unterstützen.
Damals befürchteten die USA, dass sich der Kommunismus auf den Nahen Osten ausbreiten würde, und sahen in Nasser den natürlichen Anführer einer antikommunistischen prokapitalistischen Arabischen Liga.
Nachdem die UN 1955 einen israelischen Angriff auf ägyptische Streitkräfte in Gaza kritisiert hatte, wurde Nasser klar, dass er sich nicht als Anführer des panarabischen Nationalismus darstellen konnte, wenn er sein Land nicht militärisch gegen Israel verteidigen konnte.
Nasser akzeptierte diese Bedingungen nicht und bat die UdSSR um Unterstützung.
Dulles war mehr verärgert über Nassers diplomatische Anerkennung Chinas, die in direktem Widerspruch zu Dulles' Politik der Eindämmung des Kommunismus stand.
Er war auch irritiert über Nassers Neutralität und seine Versuche, beide Seiten des Kalten Krieges auszuspielen.
Der riesige Stein- und Lehmdamm wurde vom sowjetischen Hydroprojekt-Institut zusammen mit einigen ägyptischen Ingenieuren entworfen.
Umgekehrt überflutete der Damm ein großes Gebiet und führte zur Umsiedlung von über 100.000 Menschen.
Die Kosten-Nutzen-Bewertung des Staudamms bleibt auch Jahrzehnte nach seiner Fertigstellung umstritten.
Wenn man die negativen ökologischen und sozialen Auswirkungen des Staudamms außer Acht lässt, dürften sich die Kosten somit innerhalb von nur zwei Jahren amortisiert haben.
Ein anderer Beobachter war anderer Meinung und empfahl den Abriss des Damms.
Der Damm milderte die Auswirkungen von Überschwemmungen, beispielsweise in den Jahren 1964, 1973 und 1988.
Rund um den Nassersee ist eine neue Fischereiindustrie entstanden, die jedoch aufgrund der Entfernung zu wichtigen Märkten Schwierigkeiten hat.
Etwa eine halbe Million Familien wurden auf diesem neuen Land angesiedelt.
Auf anderen zuvor bewässerten Flächen stiegen die Erträge, da in kritischen Zeiten mit geringem Wasserabfluss Wasser zur Verfügung gestellt werden konnte.
Im Sudan wurden 50.000 bis 70.000 sudanesische Nubier aus der Altstadt von Wadi Halfa und den umliegenden Dörfern vertrieben.
Die Regierung entwickelte ein Bewässerungsprojekt namens New Halfa Agricultural Development Scheme für den Anbau von Baumwolle, Getreide, Zuckerrohr und anderen Feldfrüchten.
Es wurden Wohnungen und Einrichtungen für 47 Dorfeinheiten gebaut, deren Verhältnis zueinander dem in Alt-Nubien ähnelte.
Der durch das Sediment dem Land zugefügte Nährstoffwert betrug lediglich 6.000 Tonnen Kali, 7.000 Tonnen Phosphorpentoxid und 17.000 Tonnen Stickstoff.
Auch der Salzgehalt des Bodens nahm zu, da der Abstand zwischen der Oberfläche und dem Grundwasserspiegel klein genug war (1–2 m, je nach Bodenbeschaffenheit und Temperatur), um das Ansaugen von Wasser durch Verdunstung zu ermöglichen, sodass sich die relativ geringen Salzkonzentrationen im Grundwasser ansammelten auf der Bodenoberfläche im Laufe der Jahre.
Bis in die 1950er Jahre war nur ein kleiner Teil Oberägyptens nicht von der Beckenbewässerung (geringe Transmission) auf die Dauerbewässerung (hohe Transmission) umgestellt worden.
S. haematobium ist seitdem vollständig verschwunden.
Dies bedeutet, dass das tote Speichervolumen nach 300–500 Jahren aufgefüllt wäre, wenn sich die Sedimente im gesamten Seegebiet mit der gleichen Geschwindigkeit ansammeln würden.
Nach dem Bau des Staudamms wuchsen Wasserunkräuter im klareren Wasser viel schneller, unterstützt durch Düngemittelrückstände.
Nach der Fertigstellung des Staudamms ging die Fischerei im Mittelmeer und in Brackwasserseen zurück, da Nährstoffe, die den Nil hinunter ins Mittelmeer flossen, hinter dem Staudamm zurückgehalten wurden.
Vor dem Bau des Staudamms gab es Anlass zur Sorge, dass der Pegel des Flussbetts stromabwärts des Staudamms aufgrund der Erosion, die durch den Fluss von sedimentfreiem Wasser verursacht wird, sinken könnte.
Auch die Bauindustrie aus roten Ziegeln, die aus Hunderten von Fabriken bestand, die Nilsedimentablagerungen entlang des Flusses nutzten, wurde negativ beeinflusst.
Aufgrund der geringeren Trübung des Wassers dringt das Sonnenlicht tiefer in das Nilwasser ein.
Die Bauarbeiten begannen 1995 und nach Ausgaben von rund 220 Millionen US-Dollar wurde der Komplex am 16. Oktober 2002 offiziell eingeweiht.
Die Nachbildung der antiken Bibliothek wurde nicht nur von anderen Einzelpersonen und Behörden übernommen, sondern fand auch Unterstützung bei ägyptischen Politikern.
Das Engagement der UNESCO ab 1986 bot eine großartige Gelegenheit für das Projekt, eine wirklich internationale Ausrichtung zu erlangen.
Dieses Architektenteam bestand aus zehn Mitgliedern, die sechs Länder vertraten.
Die ersten Zusagen zur Finanzierung des Projekts wurden 1990 auf einer Konferenz in Assuan gemacht: 65 Millionen US-Dollar, größtenteils von den MENA-Staaten.
Im Jahr 2010 erhielt die Bibliothek eine Schenkung von 500.000 Büchern von der französischen Nationalbibliothek, Bibliothèque nationale de France (BnF).
Der Hauptlesesaal liegt unter einem 32 Meter hohen Glasdach, das wie eine Sonnenuhr zum Meer geneigt ist und einen Durchmesser von rund 160 Metern hat.
Mit etwa 1.316 Artefakten bietet die Sammlung des Antiquitätenmuseums einen Einblick in die ägyptische Geschichte von der Pharaonenzeit über die Eroberung Alexanders des Großen bis hin zu den römischen Zivilisationen und dem Aufkommen des Islam in ganz Ägypten.
Mikrofilm: Dieser Abschnitt umfasst Mikrofilme von rund 30.000 seltenen Manuskripten und 50.000 Dokumenten sowie eine Sammlung der British Library mit rund 14.000 arabischen, persischen und türkischen Manuskripten, die als die größte Sammlung Europas gilt.
Im Jahr 2010 erhielt die Bibliothek jedoch weitere 500.000 Bücher von der Bibliothèque nationale de France.)
Die Große Moschee von Djenné ist ein großes Banco- oder Lehmgebäude, das von vielen Architekten als eine der größten Errungenschaften des sudanesisch-sahelischen Architekturstils angesehen wird.
Das früheste Dokument, in dem die Moschee erwähnt wird, ist Abd al-Sadis Tarikh al-Sudan, der die frühe Geschichte wiedergibt, vermutlich aus der mündlichen Überlieferung, wie sie Mitte des 17. Jahrhunderts existierte.
Sein unmittelbarer Nachfolger baute die Türme der Moschee, während der nachfolgende Sultan die Umfassungsmauer errichtete.
Dies dürfte das Gebäude gewesen sein, das Caillié gesehen hat.
Die neue Moschee war ein großes, niedriges Gebäude ohne Türme oder Verzierungen.
Der Wiederaufbau wurde 1907 unter Einsatz von Zwangsarbeitern unter der Leitung von Ismaila Traoré, der Leiterin der Maurergilde von Djenné, abgeschlossen.
Es gab Debatten darüber, inwieweit der Entwurf der wiederaufgebauten Moschee dem französischen Einfluss unterlag.
Er glaubte, dass die Zapfen das Gebäude einem barocken Tempel ähneln ließen, der dem Gott der Zäpfchen gewidmet war.
Er sagt auch, dass die Menschen vor Ort mit dem neuen Gebäude so unzufrieden waren, dass sie sich weigerten, es zu reinigen, und dies erst taten, als ihnen eine Gefängnisstrafe drohte.
Das größere Grab im Süden enthält die Überreste von Almany Ismaïla, einem wichtigen Imam des 18. Jahrhunderts.
In einigen Fällen wurden sogar die ursprünglichen Oberflächen einer Moschee mit Fliesen bedeckt, wodurch ihr historisches Erscheinungsbild zerstört und in einigen Fällen die strukturelle Integrität des Gebäudes gefährdet wurde.
1996 veranstaltete das Vogue-Magazin ein Modeshooting in der Moschee.
Der Zugang erfolgt über sechs Treppen, die jeweils mit Zinnen verziert sind.
Die Gebetsmauer oder Qibla der Großen Moschee ist nach Osten in Richtung Mekka ausgerichtet und überblickt den Marktplatz der Stadt.
Die kegelförmigen Türme oder Zinnen an der Spitze jedes Minaretts sind mit Straußeneiern gekrönt.
Durch die kleinen, unregelmäßig angeordneten Fenster an der Nord- und Südwand gelangt nur wenig natürliches Licht in das Innere der Halle.
Der Imam leitet die Gebete vom Mihrab im größeren Mittelturm aus.
Rechts vom Mihrab im Mittelturm befindet sich eine zweite Nische, die Kanzel oder Minbar, von der aus der Imam seine Freitagspredigt hält.
Die hofseitigen Wände der Galerien sind durch Rundbogenöffnungen unterbrochen.
Anstelle einer einzelnen zentralen Nische verfügte der Mihrab-Turm ursprünglich über zwei große Aussparungen, die an die Form der Eingangsbögen in der Nordwand erinnerten.
Die Aushärtung dauert mehrere Tage, muss aber regelmäßig umgerührt werden, eine Aufgabe, die normalerweise kleinen Jungen obliegt, die in der Mischung spielen und so den Inhalt aufrühren.
Zu Beginn des Festivals findet ein Rennen statt, bei dem es darum geht, wer als Erster den Gips in der Moschee abliefert.
Im Jahr 1930 wurde in der südfranzösischen Stadt Fréjus eine ungenaue Nachbildung der Djenné-Moschee errichtet.
Die ursprüngliche Moschee beherbergte im Mittelalter eines der wichtigsten islamischen Lernzentren in Afrika. Tausende von Studenten kamen in die Madrassas von Djenné, um den Koran zu studieren.
Am 20. Januar 2006 löste der Anblick einer Männergruppe, die auf das Dach der Moschee einhackte, einen Aufstand in der Stadt aus.
In der Moschee riss der Mob die Ventilatoren heraus, die ihm die US-Botschaft zur Zeit des Irak-Krieges geschenkt hatte, und tobte anschließend durch die Stadt.
Die Große Sphinx von Gizeh, allgemein als Sphinx von Gizeh oder einfach nur Sphinx bezeichnet, ist eine Kalksteinstatue einer liegenden Sphinx, eines Fabelwesens.
Darüber hinaus deuten der Winkel und die Lage der Südwand der Anlage darauf hin, dass der Damm, der die Chephren-Pyramide und den Taltempel verbindet, bereits existierte, bevor die Sphinx geplant wurde.
Bei der erneuten Ausgrabung der Stele im Jahr 1925 blätterten die Textzeilen, die sich auf Khaf bezogen, ab und wurden zerstört.
Der Kult der Sphinx hielt bis ins Mittelalter an.
Alexandria, Rosetta, Damiette, Kairo und die Pyramiden von Gizeh werden immer wieder beschrieben, jedoch nicht unbedingt umfassend.
Sieben Jahre nach seinem Besuch in Gizeh beschrieb André Thévet (Cosmographie de Levant, 1556) die Sphinx als „den Kopf eines Kolosses, geschaffen von Isis, der Tochter des Inachus, die damals von Jupiter so geliebt wurde“.
Johannes Helferichs (1579) Sphinx ist eine Frau mit schmalem Gesicht, runden Brüsten und einer Perücke mit glattem Haar; Der einzige Vorteil gegenüber Thévet besteht darin, dass die Haare an die ausgestellten Spitzen des Kopfschmucks erinnern.
Obwohl bestimmte Traktate auf der Stele wahrscheinlich korrekt sind, wird diese Passage durch archäologische Beweise widerlegt und daher als Geschichtsrevisionismus der Spätzeit angesehen, eine gezielte Fälschung, die von den örtlichen Priestern geschaffen wurde, um dem zeitgenössischen Isis-Tempel eine antike Geschichte zu verleihen hatte nie.
Jüngste Entdeckungen zeigen jedoch deutlich, dass es tatsächlich nicht vor der Herrschaft von Khafre, in der vierten Dynastie, erbaut wurde.“
Maspero glaubte, die Sphinx sei „das älteste Denkmal Ägyptens“.
Ein Teil seines Kopfschmucks war 1926 durch Erosion abgefallen, die auch tief in den Hals eingeschnitten war.
Die Schicht, in der der Kopf geformt wurde, ist viel härter.
Andere Geschichten schreiben es einem Werk der Mamluken zu.
Laut al-Maqrīzī glaubten viele Menschen in der Gegend, dass die zunehmende Sandbedeckung des Gizeh-Plateaus eine Vergeltung für al-Dahrs Tat der Verunstaltung sei.
Al-Minufi erklärte, dass der Kreuzzug von Alexandria im Jahr 1365 eine göttliche Strafe für einen Sufi-Scheich des Khanqah von Sa'id war, der sich die Nase abbrach.
Die Idee wird von der Wissenschaft als Pseudoarchäologie betrachtet, da keine schriftlichen oder archäologischen Beweise dafür vorliegen, dass dies der Grund für die Ausrichtung der Sphinx ist.
Spekulationen über versteckte Kammern unter der Sphinx durch esoterische Persönlichkeiten wie H. Spencer Lewis haben eine lange Geschichte.
Es wird angenommen, dass es sich um die am zweithäufigsten besuchte historische Stätte in Ägypten handelt. Nur der Pyramidenkomplex von Gizeh in der Nähe von Kairo erhält mehr Besuche.
Die drei anderen Teile, der Bezirk Mut, der Bezirk Montu und der abgerissene Tempel von Amenophis IV., sind für die Öffentlichkeit geschlossen.
Der ursprüngliche Tempel wurde von Hatschepsut zerstört und teilweise restauriert, obwohl ein anderer Pharao ihn umbaute, um den Schwerpunkt oder die Ausrichtung des heiligen Bereichs zu ändern.
Der Bau von Tempeln begann im Reich der Mitte und wurde bis in die ptolemäische Zeit fortgesetzt.
Die dargestellten Gottheiten reichen von einigen der am frühesten verehrten bis hin zu denen, die viel später in der Geschichte der altägyptischen Kultur verehrt wurden.
Diese Architrave wurden möglicherweise mithilfe von Hebeln auf diese Höhe gehoben.
Hätte man für die Rampen Stein verwendet, hätte man viel weniger Material verbrauchen können.
Die endgültige Schnitzerei wurde nach dem Einsetzen der Trommeln ausgeführt, damit diese beim Aufstellen nicht beschädigt wurden.
Die Stadt Theben scheint vor der 11. Dynastie keine große Bedeutung gehabt zu haben, und frühere Tempelbauten waren relativ klein, mit Schreinen, die den frühen Gottheiten Thebens, der Erdgöttin Mut und Montu, gewidmet waren.
Amun (manchmal auch Amen genannt) war lange Zeit die örtliche Schutzgottheit von Theben.
Während der 18. Dynastie, als Theben zur Hauptstadt des vereinten alten Ägypten wurde, fanden im Amun-Re-Bezirk umfangreiche Bauarbeiten statt.
Ein weiteres ihrer Projekte an diesem Ort, die Rote Kapelle oder Chapelle Rouge von Karnak, war als barkischer Schrein gedacht und stand ursprünglich möglicherweise zwischen ihren beiden Obelisken.
Er ist als unvollendeter Obelisk bekannt und liefert den Beweis dafür, wie Obelisken abgebaut wurden.
Die letzte größere Änderung am Grundriss des Amun-Re-Bezirks war die Hinzufügung des Ersten Pylons und der massiven Umfassungsmauern, die den gesamten Bezirk umgeben und beide von Nektanebos I. aus der 30. Dynastie errichtet wurden.
Der Karnak-Tempelkomplex wurde erstmals 1589 von einem unbekannten Venezianer beschrieben, obwohl in seinem Bericht kein Name für den Komplex angegeben ist.
Protais' Schriften über ihre Reise wurden von Melchisédech Thévenot (Relations de divers voyages curieux, Ausgaben 1670–1696) und Johann Michael Vansleb (The Present State of Egypt, 1678) veröffentlicht.
Nach Ausgrabungs- und Restaurierungsarbeiten durch das Team der Johns Hopkins University unter der Leitung von Betsy Bryan (siehe unten) wurde der Bezirk Mut für die Öffentlichkeit geöffnet.
Im Jahr 2006 präsentierte Betsy Bryan ihre Ergebnisse eines Festivals, bei dem es offensichtlich zu absichtlichem übermäßigem Alkoholgenuss kam.
Diese Funde wurden im Tempel von Mut gemacht, denn als Theben zu größerer Bedeutung gelangte, nahm Mut die Kriegergöttinnen Sekhmet und Bast als einige ihrer Aspekte auf.
In einem späteren Mythos, der sich rund um das alljährliche Sekhmet-Fest der Betrunkenen entwickelte, erschuf Ra, damals der Sonnengott Oberägyptens, sie aus einem feurigen Auge, das er von seiner Mutter erhalten hatte, um Sterbliche zu vernichten, die sich gegen ihn verschworen hatten (Unterägypten).
Der Luxor-Tempel ist ein großer altägyptischer Tempelkomplex am Ostufer des Nils in der heutigen Stadt Luxor (altes Theben) und wurde etwa 1400 v. Chr. erbaut.
Zu den vier wichtigsten Totentempeln, die von frühen Reisenden besucht wurden, gehören der Tempel von Sethos I. in Gurnah, der Tempel der Hatschepsut in Deir el Bahri, der Tempel von Ramses II. (d. h. Ramesseum) und der Tempel von Ramses III. in Medinet Habu.
Auf der Rückseite des Tempels befinden sich Kapellen, die von Amenophis III. aus der 18. Dynastie und Alexander erbaut wurden.
Dieser Sandstein wird als Nubischer Sandstein bezeichnet.
Alexander Badawy, „Illusionism in Egyptian Architecture“, Studies in the Ancient Oriental Civilization, 35 (1969): 23.
Entlang der Allee wurden Stationen für Zeremonien wie das Opet-Fest errichtet, die für den Tempel von Bedeutung waren.
Lalibela ist eine Stadt im Bezirk Lasta, North Wollo Zone, Region Amhara, Äthiopien.
Für Christen ist Lalibela nach Axum eine der heiligsten Städte Äthiopiens und ein Pilgerzentrum.
Die Namen mehrerer Orte in der modernen Stadt und die allgemeine Anordnung der in den Felsen gehauenen Kirchen selbst sollen Namen und Muster nachahmen, die Lalibela während seiner Zeit als Jugendlicher in Jerusalem und im Heiligen Land beobachtet hatte.
Der christliche Glaube inspiriert viele Elemente mit biblischen Namen – sogar der Fluss von Lalibela ist als Jordan bekannt.
Der portugiesische Priester Francisco Álvares (1465–1540) begleitete den portugiesischen Botschafter bei seinem Besuch in Dawit II. in den 1520er Jahren.
Der nächste gemeldete europäische Besucher in Lalibela war Michael de Castanhoso, der als Soldat unter Christopher da Gama diente und Äthiopien verließ
Seine Säulen wurden ebenfalls aus dem Berg gehauen.")
Es gibt einige Kontroversen darüber, wann einige der Kirchen gebaut wurden.
In seinem Bericht wurden zwei Arten einheimischer Wohnhäuser beschrieben, die in der Gegend gefunden wurden.
Das Katharinenkloster, offiziell Heiliges Kloster des gottbetretenen Berges Sinai, ist ein ostorthodoxes Kloster auf der Sinai-Halbinsel, an der Mündung einer Schlucht am Fuße des Berges Sinai, in der Nähe der Stadt St. Katharina in Ägypten.
Das Kloster der Heiligen Katharina liegt im Schatten einer Gruppe von drei Bergen; Ras Sufsafeh (möglicherweise „Berg Horeb“ ca. 1 km westlich), Jebel Arrenziyeb und Jebel Musa, der „biblische Berg Sinai“ (Gipfel ca. 2 km südlich).
Katharina selbst ordnete den Beginn der Hinrichtung an.
Das Kloster wurde im Auftrag von Kaiser Justinian I. (reg. 527–565) erbaut und umschließt die Kapelle des Brennenden Dornbuschs (auch bekannt als „Kapelle der Heiligen Helena“), deren Bau Kaiserin Gemahlin Helena, Mutter von Konstantin dem Großen, in Auftrag gegeben hatte der Ort, an dem Moses den brennenden Dornbusch gesehen haben soll.
Die Stätte ist dem Christentum, dem Islam und dem Judentum heilig.
Im siebten Jahrhundert wurden die isolierten christlichen Einsiedler des Sinai eliminiert; nur das befestigte Kloster blieb übrig.
Seit dem Ersten Kreuzzug weckte die Anwesenheit von Kreuzfahrern im Sinai bis 1270 das Interesse europäischer Christen und erhöhte die Zahl unerschrockener Pilger, die das Kloster besuchten.
Der genaue Verwaltungsstatus der Kirche innerhalb der Ostorthodoxen Kirche ist unklar: Einige, darunter auch die Kirche selbst, betrachten sie als autokephale Kirche, andere als autonome Kirche unter der Jurisdiktion der Griechisch-Orthodoxen Kirche von Jerusalem.
Doch im Jahr 2003 entdeckten russische Gelehrte die Schenkungsurkunde für das Manuskript, die am 13. November 1869 vom Konzil von Kairo Metochion und Erzbischof Kallistratus unterzeichnet worden war.
Palimpseste zeichnen sich dadurch aus, dass sie im Laufe der Jahrhunderte ein- oder mehrmals wiederverwendet wurden.
Das vollständige Scannen jeder Seite dauerte etwa acht Minuten.
Die große Ikonensammlung beginnt mit einigen wenigen Exemplaren aus dem 5. (möglicherweise) und 6. Jahrhundert, bei denen es sich um einzigartige Überbleibsel handelt; Das Kloster blieb vom byzantinischen Bildersturm verschont und wurde nie geplündert.
Die Erhaltung ihrer architektonischen Strukturen, Gemälde und Bücher macht einen großen Teil des Stiftungszwecks aus.
Seine Größe spiegelt den relativen Wohlstand der Zeit wider.
Er wurde an der Stelle eines früheren, kleineren Tempels errichtet, der ebenfalls Horus geweiht war, obwohl die vorherige Struktur eher in Ost-West-Richtung als in Nord-Süd-Richtung ausgerichtet war, wie am heutigen Standort.
Der Tempel von Edfu wurde nicht mehr als religiöses Denkmal genutzt, nachdem Theodosius I. im Jahr 391 Heiden verfolgt und nichtchristliche Gottesdienste im Römischen Reich verboten hatte.
Im Laufe der Jahrhunderte wurde der Tempel bis zu einer Tiefe von 12 Metern (39 Fuß) unter treibendem Wüstensand und vom Nil abgelagerten Flussschlammschichten begraben.
Im Jahr 1860 begann Auguste Mariette, ein französischer Ägyptologe, mit der Befreiung des Edfu-Tempels vom Sand.
Groß-Simbabwe ist eine mittelalterliche Stadt in den südöstlichen Hügeln Simbabwes in der Nähe des Mutirikwe-Sees und der Stadt Masvingo.
Es wird angenommen, dass Groß-Simbabwe dem örtlichen Monarchen als königlicher Palast diente.
Sie wurden ohne Mörtel (Trockenmauerwerk) errichtet.
Die ersten bestätigten Besuche von Europäern fanden im späten 19. Jahrhundert statt, wobei die Untersuchungen der Stätte im Jahr 1871 begannen.
Das Gebiet von Groß-Simbabwe wurde im 4. Jahrhundert n. Chr. besiedelt.
David Beach glaubt, dass die Stadt und ihr Staat, das Königreich Simbabwe, von 1200 bis 1500 blühten, obwohl eine Beschreibung, die Anfang des 16. Jahrhunderts an João de Barros übermittelt wurde, auf ein etwas früheres Datum für ihren Untergang schließen lässt.
Sie sind als Hill Complex, Valley Complex und Great Enclosure bekannt.
Der Talkomplex ist in die oberen und unteren Talruinen mit unterschiedlichen Besiedlungsperioden unterteilt.
Der Machtschwerpunkt verlagerte sich im 12. Jahrhundert vom Hill Complex auf die Great Enclosure, das Upper Valley und schließlich im frühen 16. Jahrhundert auf das Lower Valley.
Weitere Artefakte sind Specksteinfiguren (eine davon befindet sich im British Museum), Töpferwaren, Eisengongs, kunstvoll bearbeiteter Elfenbein-, Eisen- und Kupferdraht, Eisenhacken, Speerspitzen aus Bronze, Kupferbarren und -tiegel sowie Goldperlen, Armbänder, Anhänger und Scheiden .
Dieser internationale Handel erfolgte zusätzlich zum lokalen Agrarhandel, bei dem Vieh besonders wichtig war.
Portugiesische Händler hörten im frühen 16. Jahrhundert von den Überresten der mittelalterlichen Stadt, und es sind Aufzeichnungen über Interviews und Notizen einiger von ihnen erhalten, die Groß-Simbabwe mit der Goldproduktion und dem Fernhandel in Verbindung bringen.
Er behauptete, dass die Figur stattdessen offenbar aus der späteren ptolemäischen Ära (ca. 323–30 v. Chr.) stammte, als in Alexandria ansässige griechische Kaufleute ägyptische Antiquitäten und Pseudoantiquitäten in das südliche Afrika exportierten.
Bent verfügte über keine formale archäologische Ausbildung, war aber weit in Arabien, Griechenland und Kleinasien gereist.
Sie haben in ihrer männlichen Linie eine Tradition alter jüdischer oder südarabischer Abstammung.
Der Lemba-Anspruch wurde auch von einem William Bolts (im Jahr 1777 an die österreichischen habsburgischen Behörden) und von einem A.A. gemeldet. Anderson (schreibt über seine Reisen nördlich des Limpopo-Flusses im 19. Jahrhundert).
Sie hatte zunächst drei Testgruben in ehemalige Müllhaufen auf den oberen Terrassen des Hügelkomplexes gegraben und dabei eine Mischung aus unauffälliger Keramik und Eisenarbeiten hervorgebracht.
Caton Thompson stellte ihre Bantu-Ursprungstheorie sofort auf einem Treffen der British Association in Johannesburg vor.
Der Radiokarbonnachweis besteht aus einer Reihe von 28 Messungen, von denen alle bis auf die ersten vier, die aus den Anfängen der Anwendung dieser Methode stammen und heute als ungenau gelten, die Chronologie vom 12. bis 15. Jahrhundert stützen.
Die Entfernung von Gold und Artefakten bei Amateurgrabungen durch frühkoloniale Antiquare verursachte weitreichende Schäden, insbesondere bei den Grabungen von Richard Nicklin Hall.
Preben Kaarsholm schreibt, dass sowohl koloniale als auch schwarze nationalistische Gruppen sich auf die Vergangenheit Groß-Simbabwes beriefen, um ihre Vision von der Gegenwart des Landes durch die Medien der Populärgeschichte und der Fiktion zu untermauern.
Pikirayi und Kaarsholm vermuten, dass diese Präsentation von Groß-Simbabwe teilweise dazu gedacht war, die Ansiedlung und Investitionen in der Region zu fördern.
Im Jahr 1980 wurde die Stätte in das neue international anerkannte unabhängige Land umbenannt und die berühmten Vogelschnitzereien aus Speckstein wurden als nationales Symbol aus der Flagge und dem Wappen Rhodesiens übernommen und in der neuen Flagge Simbabwes abgebildet.
Ein Beispiel für Ersteres ist Ken Mufukas Booklet, obwohl das Werk heftig kritisiert wurde.
Es wurde geschaffen, um die reiche Geschichte dieses Landes zu bewahren, das aufgrund der Globalisierung vor einer dunklen Zukunft stand.
Die Stätte weist eine Vielzahl architektonischer Stile auf, die an Stile in Zentralmexiko und an die Puuc- und Chenes-Stile im nördlichen Maya-Tiefland erinnern.
Die Stadt hatte möglicherweise die vielfältigste Bevölkerung der Maya-Welt, ein Faktor, der zur Vielfalt der Architekturstile an diesem Ort beigetragen haben könnte.
Eine mögliche Übersetzung für Itza ist „Zauberer (oder Verzauberung) des Wassers“, von dessen (itz), „Zauberer“, und ha, „Wasser“.
Diese Form bewahrt die phonemische Unterscheidung zwischen chʼ und ch, da das Grundwort chʼeʼen (das in der Maya jedoch nicht betont wird) mit einem postalveolaren Ejektiv-Affrikat-Konsonanten beginnt.
Von diesen Cenoten ist die „Cenote Sagrado“ oder Heilige Cenote (auch bekannt als „Heiliger Brunnen“ oder „Brunnen des Opfers“) die berühmteste.
Stattdessen hätte die politische Organisation der Stadt durch ein „multipales“ System strukturiert sein können, das als Herrschaft durch einen Rat charakterisiert ist, der sich aus Mitgliedern elitärer Herrscherlinien zusammensetzt.
Doch erst gegen Ende der Spätklassik und zu Beginn der Endklassik entwickelte sich der Ort zu einer wichtigen regionalen Hauptstadt, die das politische, soziokulturelle, wirtschaftliche und ideologische Leben im nördlichen Maya-Tiefland zentralisierte und dominierte.
Hunac Ceel prophezeite angeblich seinen eigenen Aufstieg zur Macht.
Während es einige archäologische Beweise dafür gibt, dass Chichén Itzá einst geplündert und geplündert wurde, scheint es mehr Beweise dafür zu geben, dass es nicht von Mayapan stammen konnte, zumindest nicht, als Chichén Itzá ein aktives städtisches Zentrum war.
Nachdem die Aktivitäten der Elite in Chichén Itzá eingestellt wurden, wurde die Stadt möglicherweise nicht verlassen.
Montejo kehrte 1531 mit Verstärkung nach Yucatán zurück und errichtete seinen Hauptstützpunkt in Campeche an der Westküste.
Montejo der Jüngere kam schließlich in Chichen Itza an, das er in Ciudad Real umbenannte.
Monate vergingen, aber es traf keine Verstärkung ein.
Bis 1535 waren alle Spanier von der Halbinsel Yucatán vertrieben worden.
Im Jahr 1860 untersuchte Désiré Charnay Chichén Itzá und machte zahlreiche Fotos, die er in American Cities and Ruins (1863) veröffentlichte.
Augustus Le Plongeon nannte es „Chaacmol“ (später umbenannt in „Chac Mool“, mit dem alle in Mesoamerika vorkommenden Arten dieser Statuen beschrieben wurden).
Im Jahr 1894 kaufte der US-Konsul in Yucatán, Edward Herbert Thompson, die Hacienda Chichén, zu der auch die Ruinen von Chichen Itza gehörten.
Thompson ist vor allem für das Ausbaggern der Cenote Sagrado (heilige Cenote) von 1904 bis 1910 bekannt, wo er Artefakte aus Gold, Kupfer und geschnitzter Jade sowie die ersten Exemplare von vermutlich präkolumbianischen Maya-Tüchern und -Stoffen fand Holzwaffen.
Die mexikanische Revolution und die darauffolgende Instabilität der Regierung sowie der Erste Weltkrieg verzögerten das Projekt um ein Jahrzehnt.
Zur gleichen Zeit ließ die mexikanische Regierung El Castillo (Tempel von Kukulcán) und den Großen Ballplatz ausgraben und restaurieren.
Thompson, der sich zu dieser Zeit in den Vereinigten Staaten aufhielt, kehrte nie nach Yucatán zurück.
Im Jahr 1944 entschied der Oberste Gerichtshof Mexikos, dass Thompson keine Gesetze gebrochen hatte, und gab Chichen Itza an seine Erben zurück.
Die erste wurde vom National Geographic gesponsert, die zweite von privaten Interessen.
Die Stadt wurde auf gebrochenem Gelände errichtet, das künstlich eingeebnet wurde, um die wichtigsten Architekturgruppen zu errichten, wobei der größte Aufwand in die Einebnung der Bereiche für die Castillo-Pyramide sowie die Gruppen Las Monjas, Osario und Main Southwest investiert wurde.
Viele dieser Steingebäude waren ursprünglich in den Farben Rot, Grün, Blau und Lila gestrichen.
Genau wie bei gotischen Kathedralen in Europa vermittelten Farben ein größeres Gefühl der Vollständigkeit und trugen wesentlich zur symbolischen Wirkung der Gebäude bei.
Die Gebäude im Puuc-Stil weisen die für diesen Stil typischen, mit Mosaiken verzierten oberen Fassaden auf, unterscheiden sich jedoch von der Architektur des Puuc-Kernlandes durch ihre Mauerwerksmauern, im Gegensatz zu den feinen Furnieren der eigentlichen Puuc-Region.
Am Fuß der Balustraden der nordöstlichen Treppe befinden sich geschnitzte Schlangenköpfe.
Nach mehreren Fehlstarts entdeckten sie eine Treppe unter der Nordseite der Pyramide.
Die mexikanische Regierung grub einen Tunnel vom Fuß der Nordtreppe hinauf über die Treppe der früheren Pyramide zum versteckten Tempel und öffnete ihn für Touristen.
Auf einer Tafel wurde einer der Spieler enthauptet; Aus der Wunde strömen Blutströme in Form sich windender Schlangen.
Am südlichen Ende befindet sich ein weiterer, viel größerer Tempel, der jedoch in Trümmern liegt.
Im Inneren befindet sich ein großes, stark zerstörtes Wandgemälde, das eine Kampfszene darstellt.
Es ist in einer Kombination aus Maya- und Tolteken-Stil erbaut und verfügt über eine Treppe, die an jeder seiner vier Seiten hinaufführt.
In seinem Inneren entdeckten Archäologen eine Ansammlung großer, aus Stein gemeißelter Kegel, deren Zweck unbekannt ist.
Der Name stammt von einer Reihe von Altären an der Spitze des Bauwerks, die von kleinen geschnitzten Männerfiguren mit erhobenen Armen, den sogenannten „Atlanten“, getragen werden.
Dieser Komplex ähnelt Tempel B in der toltekischen Hauptstadt Tula und weist auf eine Form des kulturellen Kontakts zwischen den beiden Regionen hin.
Dieser Tempel umhüllt oder begräbt ein ehemaliges Bauwerk namens „Tempel des Chac Mool“.
Südlich der Gruppe der Tausend Säulen befindet sich eine Gruppe aus drei kleineren, miteinander verbundenen Gebäuden.
Vor dem Bauwerk ist ein Ausschnitt der oberen Fassade mit einem Motiv aus „X“ und „O“ zu sehen.
Der Tempel von Xtoloc ist ein kürzlich restaurierter Tempel außerhalb der Osario-Plattform.
Zwischen dem Xtoloc-Tempel und dem Osario befinden sich mehrere ausgerichtete Bauwerke: die Plattform der Venus, die im Design dem gleichnamigen Bauwerk neben Kukulkan (El Castillo) ähnelt, die Plattform der Gräber und ein kleines, rundes Bauwerk ist unbenannt.
Die Casa Colorada (spanisch für „Rotes Haus“) ist eines der am besten erhaltenen Gebäude in Chichen Itza.
Im Jahr 2009 restaurierte INAH einen kleinen Ballplatz, der an die Rückwand der Casa Colorada angrenzte.
Der Name dieses Gebäudes wird seit langem von den örtlichen Maya verwendet, und einige Autoren erwähnen, dass es nach einem Hirschgemälde über Stuck benannt wurde, das nicht mehr existiert.
Die Spanier nannten diesen Komplex Las Monjas („Die Nonnen“ oder „Das Nonnenkloster“), aber es war ein Regierungspalast.
In diesen Texten wird häufig ein Herrscher namens Kʼakʼupakal erwähnt.
Seinen Namen verdankt es der steinernen Wendeltreppe im Inneren.
Die lange, nach Westen ausgerichtete Fassade hat sieben Türen.
Das südliche Ende des Gebäudes verfügt über einen Eingang.
In einer der Kammern, nahe der Decke, befindet sich ein gemalter Handabdruck.
Der Standort der Höhle ist in der Neuzeit gut bekannt.
E. Wyllys Andrews IV erkundete die Höhle ebenfalls in den 1930er Jahren.
Am 15. September 1959 entdeckte José Humberto Gómez, ein lokaler Führer, eine falsche Wand in der Höhle.
Noch bevor das Buch veröffentlicht wurde, reisten Benjamin Norman und Baron Emanuel von Friedrichsthal nach einem Treffen mit Stephens nach Chichen und veröffentlichten beide die Ergebnisse ihrer Entdeckungen.
Im Jahr 1923 eröffnete Gouverneur Carrillo Puerto offiziell die Autobahn nach Chichen Itza.
1930 wurde das Mayaland Hotel nördlich der Hacienda Chichén eröffnet, die von der Carnegie Institution übernommen worden war.
Im Jahr 1972 erließ Mexiko das Ley Federal Sobre Monumentos y Zonas Arqueológicas, Artísticas e Históricas (Bundesgesetz über Denkmäler und archäologische, künstlerische und historische Stätten), das alle präkolumbianischen Denkmäler des Landes, einschließlich der in Chichen Itza, in Bundeseigentum übertrug.
Reiseführer demonstrieren außerdem einen einzigartigen akustischen Effekt in Chichen Itza: Ein Händeklatschen vor der Treppe der El Castillo-Pyramide erzeugt ein Echo, das dem Zwitschern eines Vogels ähnelt, ähnlich dem von Declercq untersuchten Quetzal .
INAH, das die Stätte verwaltet, hat eine Reihe von Denkmälern für die Öffentlichkeit gesperrt.
Ursprünglich ein Projekt des Immobilienentwicklers und ehemaligen Senators des Staates New York William H. Reynolds, wurde das Gebäude von Walter Chrysler, dem Chef der Chrysler Corporation, errichtet.
Ein Anbau wurde 1952 fertiggestellt und das Gebäude wurde im nächsten Jahr von der Familie Chrysler mit zahlreichen nachfolgenden Eigentümern verkauft.
Die Ära war von tiefgreifenden sozialen und technologischen Veränderungen geprägt.
Im folgenden Jahr wurde Chrysler vom Time Magazine zur „Person des Jahres“ gekürt.
Nach dem Ende des Ersten Weltkriegs betrachteten europäische und amerikanische Architekten vereinfachtes Design als Inbegriff der Moderne und Art-Déco-Wolkenkratzer als Symbol für Fortschritt, Innovation und Modernität.
Vor seiner Beteiligung an der Planung des Gebäudes war Reynolds vor allem für die Entwicklung des Vergnügungsparks Dreamland auf Coney Island bekannt.
Nach mehreren Jahren der Verzögerung beauftragte Reynolds 1927 den Architekten William Van Alen mit dem Entwurf eines vierzigstöckigen Gebäudes.
Van Alen und Severance ergänzten einander, wobei Van Alen ein origineller, einfallsreicher Architekt und Severance ein kluger Geschäftsmann war, der sich um die Finanzen des Unternehmens kümmerte.
Der Vorschlag wurde zwei Wochen später erneut geändert, mit offiziellen Plänen für ein 63-stöckiges Gebäude.
Das angrenzende 56-stöckige Chanin-Gebäude befand sich ebenfalls im Bau.
Diese Pläne wurden im Juni 1928 genehmigt.
Stattdessen entwarf er einen alternativen Entwurf für das Reynolds-Gebäude, der im August 1928 veröffentlicht wurde.
Der Auftrag wurde am 28. Oktober vergeben und der Abriss am 9. November abgeschlossen.
Von Ende 1928 bis Anfang 1929 wurden weiterhin Änderungen am Design der Kuppel vorgenommen.
Weiter unten wurde der Entwurf von Walter Chryslers Absicht beeinflusst, das Gebäude zum Hauptsitz der Chrysler Corporation zu machen, und daher wurden verschiedene architektonische Details den Chrysler-Automobilprodukten nachempfunden, wie zum Beispiel die Kühlerfiguren des Plymouth (siehe ).
Der Bau des eigentlichen Gebäudes begann am 21. Januar 1929.
Trotz eines hektischen Stahlbautempos von etwa vier Stockwerken pro Woche starben beim Bau des Stahlbaus des Wolkenkratzers keine Arbeiter.
40 Wall Street und das Chrysler Building konkurrieren um die Auszeichnung als „höchstes Gebäude der Welt“.
Am 23. Oktober 1929, eine Woche nachdem das Woolworth-Gebäude die Höhe überschritten hatte und einen Tag vor Beginn des katastrophalen Wall-Street-Crashs von 1929, wurde der Turm montiert.
Sogar die New York Herald Tribune, die praktisch kontinuierlich über den Bau des Turms berichtete, berichtete erst Tage nach der Turmerrichtung über die Installation des Turms.
In der Lobby des Gebäudes wurde eine Bronzetafel mit der Aufschrift „In Anerkennung von Herrn Chryslers Beitrag zum bürgerlichen Fortschritt“ enthüllt.
Das Chrysler Building wurde auf 14 Millionen US-Dollar geschätzt, war jedoch gemäß einem Gesetz aus dem Jahr 1859, das Grundstücke im Eigentum der Cooper Union Steuerbefreiungen vorsah, von der Stadtsteuer befreit.
Van Alens Zufriedenheit mit diesen Leistungen wurde wahrscheinlich dadurch gedämpft, dass Walter Chrysler sich später weigerte, den Restbetrag seines Architektenhonorars zu zahlen.
Allerdings beeinträchtigte die Klage gegen Chrysler Van Alens Ruf als Architekt deutlich, was zusammen mit den Auswirkungen der Weltwirtschaftskrise und negativer Kritik letztendlich seine Karriere ruinierte.
Im Jahr 1944 reichte das Unternehmen Pläne für den Bau eines 38-stöckigen Anbaus östlich des Gebäudes in der Third Avenue 666 ein.
Der Stein des ursprünglichen Gebäudes wurde nicht mehr hergestellt und musste speziell nachgebildet werden.
Die Familie verkaufte das Gebäude 1953 zum Schätzpreis von 18 Millionen US-Dollar an William Zeckendorf.
Damals soll es sich um den größten Immobilienverkauf in der Geschichte New Yorks gehandelt haben.
Im Jahr 1961 wurden die Edelstahlelemente des Gebäudes, darunter die Nadel, die Krone, die Wasserspeier und die Eingangstüren, zum ersten Mal poliert.
Das Unternehmen kaufte das Gebäude für 35 Millionen US-Dollar.
Der Turm wurde einer Restaurierung unterzogen, die 1995 abgeschlossen wurde.
Die Reinigung wurde 1997 mit dem Lucy G. Moses Preservation Award der New York Landmarks Conservancy ausgezeichnet.
Im Juni 2008 wurde berichtet, dass der Abu Dhabi Investment Council in Verhandlungen über den Kauf der 75-prozentigen wirtschaftlichen Beteiligung von TMW, einer 15-prozentigen Beteiligung von Tishman Speyer Properties an dem Gebäude und eines Anteils an der Trylons-Einzelhandelsstruktur nebenan für 800 Millionen US-Dollar stehe .
Dies führte zu einem Rückgang des Gesamtenergieverbrauchs des Gebäudes um 21 %, des Wasserverbrauchs um 64 % und einer Recyclingquote von 81 %.
Die Ethik der philosophischen Praxis.
Philosophie ist rational kritisches Denken mehr oder weniger systematischer Art über die allgemeine Natur der Welt (Metaphysik oder Existenztheorie), die Rechtfertigung des Glaubens (Erkenntnistheorie oder Erkenntnistheorie) und die Lebensführung (Ethik oder Theorie der Existenz). Wert).
Die Metaphysik ersetzt die in einer solchen Konzeption enthaltenen unbegründeten Annahmen durch eine rationale und organisierte Sammlung von Überzeugungen über die Welt als Ganzes.
Im 19. Jahrhundert führte das Wachstum moderner Forschungsuniversitäten zu einer Professionalisierung und Spezialisierung der akademischen Philosophie und anderer Disziplinen.
In „Gegen die Logiker“ beschrieb der pyrrhonistische Philosoph Sextus Empiricus die Vielfalt der Arten, in denen die antiken griechischen Philosophen die Philosophie unterteilt hatten, und stellte fest, dass dieser dreiteiligen Einteilung Platon, Aristoteles, Xenokrates und die Stoiker zugestimmt hatten.
Andere von Sokrates beeinflusste antike philosophische Traditionen waren Zynismus, Kyrenaismus, Stoizismus und akademischer Skeptizismus.
Zu den wichtigsten Denkern des Mittelalters zählen der heilige Augustinus, Thomas von Aquin, Boethius, Anselm und Roger Bacon.
Zu den wichtigsten modernen Philosophen zählen Spinoza, Leibniz, Locke, Berkeley, Hume und Kant.
Die babylonische Astronomie beinhaltete auch viele philosophische Spekulationen über die Kosmologie, die möglicherweise die alten Griechen beeinflusst haben.
Später geriet die jüdische Philosophie unter starke westliche intellektuelle Einflüsse und umfasst die Werke von Moses Mendelssohn, der die Haskalah (die jüdische Aufklärung), den jüdischen Existentialismus und das Reformjudentum einleitete.
Islamische Philosophie ist das philosophische Werk, das seinen Ursprung in der islamischen Tradition hat und größtenteils auf Arabisch verfasst wird.
Die frühislamische Philosophie entwickelte die griechischen philosophischen Traditionen in neue innovative Richtungen.
Das Werk des Aristoteles hatte großen Einfluss auf Philosophen wie Al-Kindi (9. Jahrhundert), Avicenna (980 – Juni 1037) und Averroes (12. Jahrhundert).
Ibn Khaldun war ein einflussreicher Denker der Geschichtsphilosophie.
Indische philosophische Traditionen teilen verschiedene Schlüsselkonzepte und Ideen, die auf unterschiedliche Weise definiert und von den verschiedenen Traditionen akzeptiert oder abgelehnt werden.
Die indische Philosophie wird üblicherweise nach ihrer Beziehung zu den Veden und den darin enthaltenen Ideen in Gruppen eingeteilt.
Die Schulen, die sich an den Gedanken der Upanishaden orientieren, die sogenannten „orthodoxen“ oder „hinduistischen“ Traditionen, werden oft in sechs Darśanas oder Philosophien eingeteilt: Sānkhya, Yoga, Nyāya, Vaisheshika, Mimāmsā und Vedānta.
Sie spiegeln auch eine Toleranz gegenüber einer Vielfalt philosophischer Interpretationen innerhalb des Hinduismus wider und teilen gleichzeitig die gleiche Grundlage.
Es gibt auch andere Denkschulen, die oft als „hinduistisch“ angesehen werden, obwohl sie nicht unbedingt orthodox sind (da sie möglicherweise andere Schriften als normativ akzeptieren, wie die Shaiva Agamas und Tantras), dazu gehören verschiedene Schulen des Shavismus wie Pashupata und Shaiva Siddhanta, nicht-dualer tantrischer Shavismus (d. h. Trika, Kaula usw.).
Die Leugnung, dass ein Mensch ein „Selbst“ oder eine „Seele“ besitzt, ist wahrscheinlich die berühmteste buddhistische Lehre.
Die Jain-Philosophie ist (neben dem Buddhismus) eine der beiden einzigen überlebenden „unorthodoxen“ Traditionen.
Das jainistische Denken geht davon aus, dass alle Existenz zyklisch, ewig und ungeschaffen ist.
In diesen Regionen entwickelte sich das buddhistische Denken zu verschiedenen philosophischen Traditionen, die verschiedene Sprachen verwendeten (wie Tibetisch, Chinesisch und Pali).
Die Philosophie der Theravada-Schule ist in südostasiatischen Ländern wie Sri Lanka, Burma und Thailand vorherrschend.
Nach dem Tod Buddhas begannen verschiedene Gruppen, seine Hauptlehren zu systematisieren und entwickelten schließlich umfassende philosophische Systeme namens Abhidharma.
Im alten und mittelalterlichen Indien gab es zahlreiche Schulen, Unterschulen und Traditionen der buddhistischen Philosophie.
Diese philosophischen Traditionen entwickelten metaphysische, politische und ethische Theorien wie Tao, Yin und Yang, Ren und Li.
Während der Song-Dynastie (960–1297) dominierte der Neokonfuzianismus das Bildungssystem, und seine Ideen dienten als philosophische Grundlage für die kaiserlichen Prüfungen für die offizielle Klasse der Gelehrten.
Während späterer chinesischer Dynastien wie der Ming-Dynastie (1368–1644) sowie in der koreanischen Joseon-Dynastie (1392–1897) wurde ein wiederauflebender Neokonfuzianismus unter der Führung von Denkern wie Wang Yangming (1472–1529) zur vorherrschenden Denkschule. und wurde vom kaiserlichen Staat gefördert.
In der Neuzeit haben chinesische Denker Ideen aus der westlichen Philosophie übernommen.
Beispielsweise hat der Neue Konfuzianismus, angeführt von Persönlichkeiten wie Xiong Shili, großen Einfluss erlangt.
Ein weiterer Trend in der modernen japanischen Philosophie war die Tradition der „Nationalstudien“ (Kokugaku).
Im 17. Jahrhundert entwickelte die äthiopische Philosophie eine starke literarische Tradition, wie Zera Yacob veranschaulicht.
Ein weiteres Merkmal der indigenen amerikanischen Weltanschauungen war die Ausweitung der Ethik auf nichtmenschliche Tiere und Pflanzen.
Die Theorie von Teotl kann als eine Form des Pantheismus angesehen werden.
Dennoch deuten Berichte des US-Bildungsministeriums aus den 1990er-Jahren darauf hin, dass nur wenige Frauen in die Philosophie eingestiegen sind und dass Philosophie eines der am wenigsten geschlechtsspezifischen Fachgebiete in den Geisteswissenschaften ist; Frauen machen demnach zwischen 17 % und 30 % der philosophischen Fakultäten aus zu einigen Studien.
Siehe auch „Merkmale und Einstellungen von Lehrkräften und Personal in den Geisteswissenschaften“.
Zu den Hauptuntersuchungen gehören die Frage, wie man ein gutes Leben führt und moralische Standards ermittelt.
Epistemologen untersuchen mutmaßliche Wissensquellen, einschließlich Wahrnehmungserfahrung, Vernunft, Erinnerung und Zeugnis.
Es entstand früh in der vorsokratischen Philosophie und wurde mit Pyrrho, dem Begründer der frühesten westlichen Schule des philosophischen Skeptizismus, formalisiert.
Der Empirismus legt den Schwerpunkt auf Beobachtungsbeweise über Sinneserfahrungen als Wissensquelle.
Rationalismus ist mit A-priori-Wissen verbunden, das unabhängig von Erfahrung ist (wie Logik und Mathematik).
Die Metaphysik umfasst die Kosmologie, das Studium der Welt in ihrer Gesamtheit, und die Ontologie, das Studium des Seins.
Essenz ist die Menge von Attributen, die ein Objekt zu dem machen, was es grundsätzlich ist, und ohne die es seine Identität verliert, während Zufall eine Eigenschaft des Objekts ist, ohne die das Objekt seine Identität behalten kann.
Da fundiertes Denken ein wesentliches Element aller Natur-, Sozial- und Geisteswissenschaften ist, wurde die Logik zu einer formalen Wissenschaft.
New York: Oxford University Press.
Die meisten Studenten der akademischen Philosophie leisten später jedoch Beiträge zu Recht, Journalismus, Religion, Naturwissenschaften, Politik, Wirtschaft oder verschiedenen Künsten.
In der analytischen Philosophie untersucht die Sprachphilosophie die Natur der Sprache, die Beziehungen zwischen Sprache, Sprachbenutzern und der Welt.
Diesen Autoren folgten Ludwig Wittgenstein (Tractatus Logico-Philosophicus), der Wiener Kreis sowie die logischen Positivisten und Willard Van Orman Quine.
Er kritisierte den Konventionalismus, weil er zu der bizarren Konsequenz führte, dass alles konventionell mit jedem beliebigen Namen bezeichnet werden kann.
Dazu wies er darauf hin, dass zusammengesetzte Wörter und Phrasen eine Reihe von Korrektheiten haben.
Am Ende des Kratylos hatte er jedoch zugegeben, dass auch einige soziale Konventionen eine Rolle spielten und dass die Vorstellung, dass Phoneme individuelle Bedeutungen hätten, fehlerhaft sei.
Er teilte alle Dinge in Arten und Gattungen ein.
Da Aristoteles diese Ähnlichkeiten jedoch in einer echten Gemeinsamkeit der Form begründete, wird er häufiger als Vertreter des „gemäßigten Realismus“ angesehen.
Dieses lektón war die Bedeutung (oder der Sinn) jedes Begriffs.
Im Mittelalter gab es mehrere bemerkenswerte Sprachphilosophen.
Die Scholastiker des Hochmittelalters wie Ockham und John Duns Scotus betrachteten die Logik als eine scientia sermocinalis (Sprachwissenschaft).
Die Phänomene der Vagheit und Mehrdeutigkeit wurden intensiv analysiert, was zu einem zunehmenden Interesse an Problemen im Zusammenhang mit der Verwendung synkategorematischer Wörter wie und, oder, nicht, wenn und jeder führte.
Die suppositio eines Begriffs ist die Interpretation, die ihm in einem bestimmten Kontext gegeben wird.
Ein solches Klassifikationsschema ist der Vorläufer moderner Unterscheidungen zwischen Gebrauch und Erwähnung sowie zwischen Sprache und Metasprache.
Ein Teil des allgemeinen Satzes ist das lexikalische Wort, das aus Substantiven, Verben und Adjektiven besteht.
Die philosophische Semantik konzentriert sich tendenziell auf das Prinzip der Kompositionalität, um die Beziehung zwischen bedeutungsvollen Teilen und ganzen Sätzen zu erklären.
Das Konzept der Funktionen kann nicht nur zur Beschreibung der Funktionsweise lexikalischer Bedeutungen verwendet werden: Sie können auch zur Beschreibung der Bedeutung eines Satzes verwendet werden.
Eine Aussagenfunktion ist eine Sprachoperation, die eine Entität (in diesem Fall das Pferd) als Eingabe verwendet und eine semantische Tatsache ausgibt (d. h. die Aussage, die durch „Das Pferd ist rot“ dargestellt wird).
Ist der Spracherwerb eine besondere Fähigkeit des Geistes?
Die erste ist die behavioristische Perspektive, die vorschreibt, dass nicht nur der Großteil der Sprache erlernt wird, sondern dass sie durch Konditionierung erlernt wird.
Nativistische Modelle behaupten, dass es im Gehirn spezielle Geräte gibt, die dem Spracherwerb dienen.
Die Linguisten Sapir und Whorf schlugen vor, dass die Sprache das Ausmaß begrenzt, in dem Mitglieder einer „Sprachgemeinschaft“ über bestimmte Themen nachdenken können (eine Hypothese, die in George Orwells Roman „Neunzehnhundertvierundachtzig“ eine Parallele findet).
Das krasse Gegenteil der Sapir-Whorf-Position ist die Vorstellung, dass das Denken (oder allgemeiner gesagt der mentale Inhalt) Vorrang vor der Sprache hat.
Ein weiteres Argument ist, dass es schwierig ist zu erklären, wie Zeichen und Symbole auf Papier etwas Bedeutungsvolles darstellen können, es sei denn, ihnen wird durch den Inhalt des Geistes eine bestimmte Bedeutung verliehen.
Eine andere Tradition von Philosophen hat versucht zu zeigen, dass Sprache und Denken koextensiv sind – dass es keine Möglichkeit gibt, das eine ohne das andere zu erklären.
In gewisser Weise deuten die theoretischen Grundlagen der kognitiven Semantik (einschließlich des Konzepts des semantischen Framings) auf den Einfluss der Sprache auf das Denken hin.
Es gibt Studien, die belegen, dass Sprachen die Art und Weise beeinflussen, wie Menschen Kausalität verstehen.
Spanisch- oder japanischsprachige Menschen würden jedoch eher sagen: „Die Vase ist von selbst zerbrochen.“
Spanisch- und Japanischsprachige erinnerten sich nicht so gut an die Auslöser zufälliger Ereignisse wie Englischsprachige.
In einer Studie wurden Deutsch- und Spanischsprecher gebeten, Objekte mit unterschiedlicher Geschlechtszuordnung in diesen beiden Sprachen zu beschreiben.
Um eine „Brücke“ zu beschreiben, die im Deutschen weiblich und im Spanischen männlich ist, sagten die deutschen Sprecher „schön“, „elegant“, „zerbrechlich“, „friedlich“, „hübsch“ und „schlank“, und die spanischen Sprecher sagten „groß“, „gefährlich“, „lang“, „stark“, „robust“ und „hoch aufragend“.
Ob jeder Außerirdische freundlich oder feindselig war, wurde durch bestimmte subtile Merkmale bestimmt, aber den Teilnehmern wurde nicht gesagt, um welche es sich dabei handelte.
Im Übrigen blieben die Außerirdischen namenlos.
Man kam zu dem Schluss, dass die Benennung von Objekten uns hilft, sie zu kategorisieren und uns einzuprägen.
Zu den Themen in diesem Bereich gehören: die Natur der Synonymie, die Ursprünge der Bedeutung selbst, unser Bedeutungsverständnis und die Natur der Zusammensetzung (die Frage, wie bedeutungsvolle Spracheinheiten aus kleineren bedeutungsvollen Teilen zusammengesetzt sind und wie die Bedeutung der Das Ganze ergibt sich aus der Bedeutung seiner Teile).
Die ideelle Bedeutungstheorie, die am häufigsten mit dem britischen Empiriker John Locke in Verbindung gebracht wird, besagt, dass Bedeutungen mentale Darstellungen sind, die durch Zeichen hervorgerufen werden.
(Siehe auch Wittgensteins Bildtheorie der Sprache.)
Cambridge, Massachusetts: Harvard University Press.
Die Referenztheorie der Bedeutung, allgemein auch als semantischer Externalismus bekannt, betrachtet Bedeutung als äquivalent zu den Dingen in der Welt, die tatsächlich mit Zeichen verbunden sind.
Die traditionelle Formulierung einer solchen Theorie ist, dass die Bedeutung eines Satzes seine Methode zur Verifizierung oder Falsifizierung ist.
In dieser Version besteht das Verständnis (und damit die Bedeutung) eines Satzes in der Fähigkeit des Hörers, den (mathematischen, empirischen oder anderen) Nachweis der Wahrheit des Satzes zu erkennen.
Eine pragmatische Bedeutungstheorie ist jede Theorie, in der die Bedeutung (oder das Verständnis) eines Satzes durch die Konsequenzen seiner Anwendung bestimmt wird.
Gottlob Frege war ein Verfechter einer vermittelten Referenztheorie.
Ein solcher Gedanke ist abstrakt, universell und objektiv.
Referenten sind die Objekte in der Welt, die Wörter auswählen.
Er betrachtete Eigennamen der oben beschriebenen Art als „abgekürzte eindeutige Beschreibungen“ (siehe Theorie der Beschreibungen).
Solche Phrasen bezeichnen in dem Sinne, dass es ein Objekt gibt, das der Beschreibung entspricht.
Nach Freges Ansicht hat jeder verweisende Ausdruck sowohl einen Sinn als auch einen Referenten.
Trotz der Unterschiede zwischen den Ansichten von Frege und Russell werden sie im Allgemeinen als Deskriptivisten über Eigennamen zusammengefasst.
Betrachten Sie den Namen Aristoteles und die Beschreibungen „der größte Schüler Platons“, „der Begründer der Logik“ und „der Lehrer Alexanders“.
Er könnte existiert haben und der Nachwelt überhaupt nicht bekannt geworden sein, oder er könnte im Kindesalter gestorben sein.
Aber das ist zutiefst kontraintuitiv.
Es stellen sich zwangsläufig Fragen zu den umliegenden Themen.
David Kellogg Lewis schlug eine würdige Antwort auf die erste Frage vor, indem er die Ansicht darlegte, dass eine Konvention eine rational sich selbst aufrechterhaltende Regelmäßigkeit im Verhalten sei.
Noam Chomsky schlug vor, dass das Studium der Sprache anhand der Ich-Sprache oder der inneren Sprache von Personen erfolgen könnte.
Eine fruchtbare Forschungsquelle ist die Untersuchung der sozialen Bedingungen, die Bedeutungen und Sprachen hervorbringen oder mit ihnen verbunden sind.
Die Annahmen, die jede theoretische Sichtweise stützen, sind für den Sprachphilosophen von Interesse.
Rhetorik ist das Studium der besonderen Wörter, die Menschen verwenden, um beim Zuhörer die richtige emotionale und rationale Wirkung zu erzielen, sei es, um zu überzeugen, zu provozieren, beliebt zu machen oder zu lehren.
Es findet auch Anwendung auf das Studium und die Auslegung des Rechts und hilft dabei, Einblick in das logische Konzept des Diskursbereichs zu geben.
Die Idee der Sprache wird oft mit der der Logik im griechischen Sinne als „logos“ in Verbindung gebracht, was Diskurs oder Dialektik bedeutet.
Heidegger verbindet Phänomenologie mit der Hermeneutik Wilhelm Diltheys.
Beispielsweise ist Sein (Sein), das Wort selbst, mit mehreren Bedeutungen gesättigt.
Heidegger behauptet, das Schreiben sei nur eine Ergänzung zum Sprechen, da selbst der Leser beim Lesen sein eigenes „Gespräch“ konstruiere oder beisteuere.
In Truth and Method beschreibt Gadamer Sprache als „das Medium, in dem substanzielles Verständnis und Übereinstimmung zwischen zwei Menschen stattfinden.“
Paul Ricœur hingegen schlug eine Hermeneutik vor, die sich wieder an die ursprüngliche griechische Bedeutung des Begriffs anlehnte und die Entdeckung verborgener Bedeutungen in den mehrdeutigen Begriffen (oder „Symbolen“) der gewöhnlichen Sprache betonte.
Es ermöglicht ihnen, die Außenwelt auszunutzen und effektiv zu manipulieren, um für sich selbst einen Sinn zu schaffen und diesen Sinn an andere weiterzugeben.
Einige wichtige Persönlichkeiten in der Geschichte der Semiotik sind Charles Sanders Peirce, Roland Barthes und Roman Jakobson.
Die Romantik des 19. Jahrhunderts betonte die menschliche Entscheidungsfreiheit und den freien Willen bei der Bedeutungskonstruktion.
Humanistische Ansichten werden durch biologische Sprachtheorien in Frage gestellt, die Sprachen als natürliche Phänomene betrachten.
Im Neodarwinismus betrachten Richard Dawkins und andere Befürworter kultureller Replikatortheorien Sprachen als Populationen von Geistesviren.
Einige haben gesagt, dass der Ausdruck für ein reales, abstraktes Universalität draußen in der Welt steht, das „Felsen“ genannt wird.
Die Problematik hier kann erklärt werden, wenn wir den Satz „Sokrates ist ein Mann“ untersuchen.
Diese beiden Dinge hängen in irgendeiner Weise zusammen oder überschneiden sich.
Eine andere Perspektive besteht darin, den „Menschen“ als eine Eigenschaft der Entität „Sokrates“ zu betrachten.
Zu den prominentesten Mitgliedern dieser Tradition der formalen Semantik zählen Tarski, Carnap, Richard Montague und Donald Davidson.
Sie glaubten nicht, dass die sozialen und praktischen Dimensionen der sprachlichen Bedeutung durch Formalisierungsversuche mit den Werkzeugen der Logik erfasst werden könnten.
Viele seiner Ideen wurden von Theoretikern wie Kent Bach, Robert Brandom, Paul Horwich und Stephen Neale übernommen.
In Word and Object bittet Quine die Leser, sich eine Situation vorzustellen, in der sie mit einer zuvor undokumentierten Gruppe indigener Völker konfrontiert werden und versuchen müssen, die Äußerungen und Gesten ihrer Mitglieder zu verstehen.
Alles, was getan werden kann, ist, die Äußerung als Teil des gesamten sprachlichen Verhaltens des Individuums zu untersuchen und diese Beobachtungen dann zu nutzen, um die Bedeutung aller anderen Äußerungen zu interpretieren.
Für Quine, wie auch für Wittgenstein und Austin, ist Bedeutung nicht etwas, das mit einem einzelnen Wort oder Satz verbunden ist, sondern vielmehr etwas, das, wenn überhaupt, nur einer ganzen Sprache zugeschrieben werden kann.
Die spezifischen Fälle von Unbestimmtheit, die Sprachphilosophen am meisten interessieren, sind solche, bei denen die Existenz von „Grenzfällen“ es scheinbar unmöglich macht, zu sagen, ob ein Prädikat wahr oder falsch ist.
Die Philosophie der Mathematik ist der Zweig der Philosophie, der sich mit den Annahmen, Grundlagen und Implikationen der Mathematik befasst.
Heutzutage streben einige Philosophen der Mathematik danach, diese Form der Forschung und ihre Ergebnisse so darzustellen, wie sie sind, während andere eine Rolle für sich selbst betonen, die über die einfache Interpretation hinaus bis hin zur kritischen Analyse geht.
Die griechische Mathematikphilosophie wurde stark durch ihr Studium der Geometrie beeinflusst.
Daher stellte beispielsweise 3 eine bestimmte Vielzahl von Einheiten dar und war somit keine „echte“ Zahl.
Diese früheren griechischen Zahlenvorstellungen wurden später durch die Entdeckung der Irrationalität der Quadratwurzel aus zwei auf den Kopf gestellt.
Der Legende nach waren andere Pythagoräer von dieser Entdeckung so traumatisiert, dass sie Hippasus ermordeten, um ihn an der Verbreitung seiner ketzerischen Idee zu hindern.
Es ist ein tiefes Rätsel, dass mathematische Wahrheiten einerseits eine zwingende Unvermeidlichkeit zu haben scheinen, andererseits aber die Quelle ihrer „Wahrhaftigkeit“ weiterhin schwer fassbar bleibt.
Zu dieser Zeit entstanden drei Schulen, der Formalismus, der Intuitionismus und der Logikismus, teilweise als Reaktion auf die zunehmend verbreitete Sorge, dass die Mathematik in ihrer jetzigen Form und insbesondere die Analyse nicht den angenommenen Maßstäben an Sicherheit und Genauigkeit entsprachen gewährt.
Im Laufe des Jahrhunderts weitete sich der anfängliche Fokus der Besorgnis auf eine offene Erforschung der grundlegenden Axiome der Mathematik aus, wobei der axiomatische Ansatz seit der Zeit Euklids um 300 v. Chr. als natürliche Grundlage der Mathematik als selbstverständlich angesehen wurde.
Sowohl in der Mathematik als auch in der Physik waren neue und unerwartete Ideen entstanden und es standen bedeutende Veränderungen bevor.
Ich glaube nicht, dass die Schwierigkeiten, auf die die Philosophie heute mit der klassischen Mathematik stößt, echte Schwierigkeiten sind; und ich denke, dass die philosophischen Interpretationen der Mathematik, die uns überall angeboten werden, falsch sind und dass „philosophische Interpretation“ genau das ist, was die Mathematik nicht braucht.
Viele arbeitende Mathematiker waren mathematische Realisten; Sie verstehen sich als Entdecker natürlich vorkommender Objekte.
Bestimmte Prinzipien (z. B. gibt es für zwei beliebige Objekte eine Sammlung von Objekten, die genau aus diesen beiden Objekten besteht) könnten direkt als wahr angesehen werden, aber die Kontinuumshypothese-Vermutung könnte sich allein auf der Grundlage solcher Prinzipien als unentscheidbar erweisen.
Sowohl Platons Höhle als auch der Platonismus haben bedeutungsvolle, nicht nur oberflächliche Zusammenhänge, denn Platons Ideen gingen den überaus populären Pythagoräern des antiken Griechenland voraus und wurden wahrscheinlich von ihnen beeinflusst, die glaubten, dass die Welt im wahrsten Sinne des Wortes durch Zahlen erzeugt werde.
Diese Ansicht hat Ähnlichkeiten mit vielen Dingen, die Husserl über die Mathematik gesagt hat, und unterstützt Kants Idee, dass Mathematik a priori synthetisch ist.)
Der Vollblut-Platonismus ist eine moderne Variante des Platonismus, die auf die Tatsache reagiert, dass je nach den verwendeten Axiomen und Schlussfolgerungsregeln (z. B. das Gesetz der ausgeschlossenen Mitte und das Gesetz der ausgeschlossenen Mitte) nachgewiesen werden kann, dass verschiedene Mengen mathematischer Entitäten existieren Axiom der Wahl).
Der mengentheoretische Realismus (auch mengentheoretischer Platonismus), eine von Penelope Maddy vertretene Position, ist die Ansicht, dass es in der Mengenlehre um ein einziges Universum von Mengen geht.
Sie führten das Paradoxon auf eine „bösartige Zirkularität“ zurück und entwickelten zu dessen Bewältigung die sogenannte verzweigte Typentheorie.
Sogar Russell sagte, dass dieses Axiom nicht wirklich zur Logik gehöre.
Frege forderte, dass das Grundgesetz V eine explizite Definition der Zahlen geben könne, aber alle Eigenschaften von Zahlen könnten aus Humes Prinzip abgeleitet werden.
Aber es ermöglicht dem arbeitenden Mathematiker, seine Arbeit fortzusetzen und solche Probleme dem Philosophen oder Wissenschaftler zu überlassen.
Hilbert wollte die Konsistenz mathematischer Systeme anhand der Annahme zeigen, dass die „finitäre Arithmetik“ (ein Subsystem der üblichen Arithmetik der positiven ganzen Zahlen, das als philosophisch unumstritten ausgewählt wurde) konsistent sei.
Um also zu zeigen, dass jedes axiomatische System der Mathematik tatsächlich konsistent ist, muss man zunächst die Konsistenz eines mathematischen Systems annehmen, das in gewisser Weise stärker ist als das System, dessen Konsistenz nachgewiesen werden soll.
Andere Formalisten wie Rudolf Carnap, Alfred Tarski und Haskell Curry betrachteten Mathematik als die Untersuchung formaler Axiomensysteme.
Je mehr Spiele wir studieren, desto besser.
Der Hauptkritikpunkt am Formalismus besteht darin, dass die tatsächlichen mathematischen Ideen, die Mathematiker beschäftigen, weit von den oben erwähnten String-Manipulationsspielen entfernt sind.
Brouwer, der Begründer der Bewegung, vertrat die Auffassung, dass mathematische Objekte aus den apriorischen Formen der Willensäußerungen entstehen, die die Wahrnehmung empirischer Objekte prägen.
Das Auswahlaxiom wird auch in den meisten intuitionistischen Mengenlehren abgelehnt, obwohl es in einigen Versionen akzeptiert wird.
Aus dieser Sicht ist Mathematik eine Übung der menschlichen Intuition und kein Spiel mit bedeutungslosen Symbolen.
Ebenso werden alle anderen ganzen Zahlen durch ihre Stellen in einer Struktur, dem Zahlenstrahl, definiert.
Die zentrale Behauptung bezieht sich jedoch nur darauf, was für eine Entität ein mathematisches Objekt ist, nicht auf die Art der Existenz mathematischer Objekte oder Strukturen (mit anderen Worten nicht auf ihre Ontologie).
Strukturen wird eine reale, aber abstrakte und immaterielle Existenz zugeschrieben.
Strukturen werden insofern als existent angesehen, als sie durch ein konkretes System veranschaulicht werden.
Wie der Nominalismus leugnet der Post-Rem-Ansatz die Existenz abstrakter mathematischer Objekte mit anderen Eigenschaften als ihrem Platz in einer relationalen Struktur.
Es wird davon ausgegangen, dass Mathematik nicht universell ist und in keinem wirklichen Sinne existiert, außer im menschlichen Gehirn.
Der menschliche Geist hat jedoch keinen besonderen Anspruch auf die Realität oder auf mathematisch aufgebaute Ansätze zu ihr.
Die zugänglichste, berühmteste und berüchtigtste Darstellung dieser Perspektive ist „Where Mathematics Comes From“ von George Lakoff und Rafael E. Núñez.
Franklin, James (2014), „An Aristotelian Realist Philosophy of Mathematics“, Palgrave Macmillan, Basingstoke; Franklin, James (2021), „Mathematik als Wissenschaft der nicht-abstrakten Realität: Aristotelische realistische Philosophien der Mathematik“, Foundations of Science 25.
Die von John Penn Mayberry in seinem Buch „The Foundations of Mathematics in the Theory of Sets“ entwickelte euklidische Arithmetik steht ebenfalls in der aristotelischen realistischen Tradition.
Edmund Husserl kritisierte im ersten Band seiner Logischen Untersuchungen mit dem Titel „Die Prolegomena der reinen Logik“ den Psychologismus gründlich und versuchte, sich von ihm zu distanzieren.
Das heißt, da die Physik über Elektronen sprechen muss, um zu sagen, warum sich Glühbirnen so verhalten, wie sie es tun, müssen Elektronen existieren.
Es argumentiert, dass die Existenz mathematischer Einheiten die beste Erklärung für Erfahrung sei, und beraubt damit die Mathematik ihrer Unterscheidungsfähigkeit gegenüber den anderen Wissenschaften.
Dies entstand aus der im späten 20. Jahrhundert immer beliebter werdenden Behauptung, es könne nie nachgewiesen werden, dass eine einzige Grundlage der Mathematik existiert.
Ein mathematisches Argument kann Falschheit von der Schlussfolgerung auf die Prämissen übertragen, ebenso wie es die Wahrheit von den Prämissen auf die Schlussfolgerung übertragen kann.
In New Directions lieferte er hierzu ausführliche Argumente.
Wenn die Mathematik genauso empirisch ist wie die anderen Wissenschaften, dann deutet dies darauf hin, dass ihre Ergebnisse genauso fehlbar und ebenso kontingent sind wie ihre.
Für eine Philosophie der Mathematik, die versucht, einige der Mängel der Ansätze von Quine und Gödel zu überwinden, indem sie Aspekte beider berücksichtigt, siehe Penelope Maddys Realism in Mathematics.
Er begann mit der „Zwischenheit“ von Hilberts Axiomen, um den Raum zu charakterisieren, ohne ihn zu koordinieren, und fügte dann zusätzliche Beziehungen zwischen Punkten hinzu, um die Arbeit zu erledigen, die früher Vektorfelder erledigten.
Nach dieser Darstellung gibt es keine metaphysischen oder erkenntnistheoretischen Probleme, die speziell für die Mathematik gelten.
Während aus empiristischer Sicht die Bewertung jedoch eine Art Vergleich mit der „Realität“ darstellt, betonen Sozialkonstruktivisten, dass die Richtung der mathematischen Forschung durch die Mode der sozialen Gruppe, die sie durchführt, oder durch die Bedürfnisse der Gesellschaft, die sie finanziert, bestimmt wird.
Sozialkonstruktivisten argumentieren jedoch, dass die Mathematik tatsächlich auf großer Unsicherheit beruht: Mit der Weiterentwicklung der mathematischen Praxis gerät der Status der bisherigen Mathematik in Zweifel und wird in dem Maße korrigiert, wie dies von der aktuellen Mathematikgemeinschaft gefordert oder gewünscht wird.
Der soziale Charakter der Mathematik wird in ihren Subkulturen hervorgehoben.
Sozialkonstruktivisten betrachten den Prozess des „Mathematikmachens“ als die tatsächliche Schaffung der Bedeutung, während Sozialrealisten einen Mangel entweder an der menschlichen Fähigkeit zur Abstraktion oder an der kognitiven Voreingenommenheit des Menschen oder an der kollektiven Intelligenz der Mathematiker als Hindernis für das Verständnis eines realen Universums sehen mathematische Objekte.
In jüngerer Zeit hat Paul Ernest ausdrücklich eine sozialkonstruktivistische Philosophie der Mathematik formuliert.
Beispielsweise werden die Werkzeuge der Linguistik im Allgemeinen nicht auf die Symbolsysteme der Mathematik angewendet, das heißt, Mathematik wird auf deutlich andere Weise studiert als andere Sprachen.
Die von Frege und Tarski für das Studium der mathematischen Sprache entwickelten Methoden wurden jedoch von Tarskis Schüler Richard Montague und anderen Linguisten, die sich mit formaler Semantik befassen, erheblich erweitert, um zu zeigen, dass der Unterschied zwischen mathematischer Sprache und natürlicher Sprache möglicherweise nicht so groß ist, wie es scheint .
Die Behauptung, dass „alle“ in wissenschaftlichen Theorien postulierten Entitäten, einschließlich Zahlen, als real akzeptiert werden sollten, wird durch den Bestätigungsholismus gerechtfertigt.
Field entwickelte seine Ansichten zum Fiktionalismus.
Das Argument basiert auf der Idee, dass eine zufriedenstellende naturalistische Darstellung von Denkprozessen anhand von Gehirnprozessen neben allem anderen auch für mathematisches Denken gegeben werden kann.
Eine weitere Verteidigungslinie besteht darin, zu behaupten, dass abstrakte Objekte für das mathematische Denken auf eine Weise relevant sind, die nicht kausal und nicht analog zur Wahrnehmung ist.
Sie liefern beispielhaft zwei Beweise für die Irrationalität von .
Paul Erdős war bekannt für seine Idee eines hypothetischen „Buches“, das die elegantesten und schönsten mathematischen Beweise enthielt.
Aus dem gleichen Grund haben Mathematikphilosophen jedoch versucht zu charakterisieren, was einen Beweis wünschenswerter macht als einen anderen, wenn beide logisch fundiert sind.
Die Philosophie des Geistes ist ein Zweig der Philosophie, der die Ontologie und Natur des Geistes und seine Beziehung zum Körper untersucht.
Dualismus und Monismus sind die beiden zentralen Denkschulen zum Geist-Körper-Problem, obwohl es differenzierte Ansichten gibt, die nicht genau in die eine oder andere Kategorie passen.
Hart, W.D. (1996) „Dualism“, in Samuel Guttenplan (org) A Companion to the Philosophy of Mind, Blackwell, Oxford, 265-7.
Pinel, J. Psychobiology, (1990) Prentice Hall, Inc. LeDoux, J. (2002) The Synaptic Self: How Our Brains Become Who We Are, New York: Viking Penguin.
Psychological Predicates“, in W. H. Capitan und D. D. Merrill, Hrsg.,
Zweitens ergeben absichtliche Bewusstseinszustände im nichtreduktiven Physikalismus keinen Sinn.
Der Wunsch einer Person zum Beispiel nach einem Stück Pizza führt dazu, dass diese Person ihren Körper auf eine bestimmte Art und Weise und in eine bestimmte Richtung bewegt, um das zu erreichen, was sie möchte.
Robinson, H. (1983): „Aristotelischer Dualismus“, Oxford Studies in Ancient Philosophy 1, 123–44.
Sie würden mit ziemlicher Sicherheit leugnen, dass der Geist einfach das Gehirn ist, oder umgekehrt, und die Idee, dass nur eine ontologische Einheit im Spiel ist, für zu mechanistisch oder unverständlich halten.
So kann man zum Beispiel durchaus fragen, wie sich ein verbrannter Finger anfühlt, wie ein blauer Himmel aussieht oder wie schöne Musik für einen Menschen klingt.
An diesen mentalen Ereignissen sind Qualia beteiligt, die besonders schwer auf etwas Physisches reduziert werden können.
Der Dualismus muss daher erklären, wie sich das Bewusstsein auf die physische Realität auswirkt.
Wissen wird jedoch durch Schlussfolgerungen vom Grund zur Folge erfasst.
Die Grundidee besteht darin, dass man sich seinen Körper vorstellen und sich daher die Existenz seines Körpers vorstellen kann, ohne dass mit diesem Körper bewusste Zustände verbunden sind.
Andere wie Dennett haben argumentiert, dass die Vorstellung eines philosophischen Zombies ein inkohärentes oder unwahrscheinliches Konzept sei.
Es ist die Ansicht, dass mentale Zustände wie Überzeugungen und Wünsche kausal mit physischen Zuständen interagieren.
Descartes‘ Argumentation basiert auf der Prämisse, dass das, was Seth in seinem Kopf für „klare und deutliche“ Ideen hält, notwendigerweise wahr ist.
Cambridge, MA: MIT Press (Bradford) Joseph Agassi weist beispielsweise darauf hin, dass mehrere wissenschaftliche Entdeckungen seit dem frühen 20. Jahrhundert die Idee des privilegierten Zugangs zu den eigenen Ideen untergraben haben.
Diese Ansicht wurde am prominentesten von Gottfried Leibniz verteidigt.
Diese entstehenden Eigenschaften haben einen unabhängigen ontologischen Status und können nicht auf das physische Substrat, aus dem sie hervorgehen, reduziert oder damit erklärt werden.
Epiphänomenalismus ist eine Lehre, die erstmals von Thomas Henry Huxley formuliert wurde.
Diese Ansicht wurde von Frank Jackson verteidigt.
Panpsychismus ist die Ansicht, dass alle Materie einen mentalen Aspekt hat oder, alternativ, alle Objekte ein einheitliches Erfahrungszentrum oder einen einheitlichen Standpunkt haben.
Ein Beispiel für diese unterschiedlichen Freiheitsgrade liefert Allan Wallace, der feststellt, dass es „erfahrungsgemäß offensichtlich ist, dass man sich körperlich unwohl fühlen kann – zum Beispiel während einer anstrengenden körperlichen Betätigung –, während man geistig fröhlich ist; umgekehrt kann man geistig verstört sein.“ während Sie körperliches Wohlbefinden erfahren.
Psychische Zustände können Veränderungen im körperlichen Zustand hervorrufen und umgekehrt.
Der Erfahrungsdualismus wird als konzeptioneller Rahmen des Madhyamaka-Buddhismus akzeptiert.
Indem die Madhyamaka-Sichtweise die unabhängige Selbstexistenz aller Phänomene leugnet, die die Welt unserer Erfahrung ausmachen, weicht sie sowohl vom Substanzdualismus von Descartes als auch vom Substanzmonismus – nämlich dem Physikalismus – ab, der für die moderne Wissenschaft charakteristisch ist.
Tatsächlich wird der Physikalismus oder die Vorstellung, dass Materie die einzige Grundsubstanz der Realität sei, vom Buddhismus ausdrücklich abgelehnt.
Während erstere üblicherweise über Masse, Ort, Geschwindigkeit, Form, Größe und zahlreiche andere physikalische Eigenschaften verfügen, sind diese im Allgemeinen nicht charakteristisch für mentale Phänomene.
Die grundsätzlich unterschiedliche Natur der Realität ist seit über zwei Jahrtausenden zentraler Bestandteil der Formen östlicher Philosophien.
Der physikalistische Monismus behauptet, dass die einzige existierende Substanz physikalisch ist, und zwar in gewisser Weise, die durch unsere beste Wissenschaft geklärt werden muss.
Obwohl reiner Idealismus wie der von George Berkeley in der zeitgenössischen westlichen Philosophie ungewöhnlich ist, vertreten einige Philosophen wie z als Alfred North Whitehead und David Ray Griffin.
Eine dritte Möglichkeit besteht darin, die Existenz einer Grundsubstanz zu akzeptieren, die weder körperlich noch geistig ist.
Introspektive Berichte über das eigene innere Seelenleben unterliegen keiner sorgfältigen Prüfung auf Richtigkeit und können nicht zur Bildung prädiktiver Verallgemeinerungen herangezogen werden.
Parallel zu diesen Entwicklungen in der Psychologie wurde ein philosophischer Behaviorismus (manchmal auch logischer Behaviorismus genannt) entwickelt.
Diese Philosophen kamen zu dem Schluss, dass mentale Zustände wahrscheinlich mit den inneren Zuständen des Gehirns identisch sind, wenn es sich bei mentalen Zuständen um etwas Materielles, aber nicht um Verhaltensweisen handelt.
Nach Token-Identitätstheorien bedeutet die Tatsache, dass ein bestimmter Gehirnzustand nur mit einem mentalen Zustand einer Person verbunden ist, nicht, dass eine absolute Korrelation zwischen mentalen Zustandstypen und Gehirnzustandstypen besteht.
Schließlich führte Wittgensteins Idee von Bedeutung als Gebrauch zu einer Version des Funktionalismus als Bedeutungstheorie, die von Wilfrid Sellars und Gilbert Harman weiterentwickelt wurde.
Daher stellt sich die Frage, ob es noch einen nichtreduktiven Physikalismus geben kann.
Davidson verwendet die These der Supervenienz: Mentale Zustände überlagern physische Zustände, sind aber nicht auf diese reduzierbar. "
Das Gehirn geht von einem Moment zum anderen weiter; Das Gehirn hat somit über die Zeit hinweg Identität.
Eine Analogie zum Selbst oder zum „Ich“ wäre die Flamme einer Kerze.
Die Flamme weist eine Art Kontinuität auf, da die Kerze während des Brennens nicht erlischt, aber es gibt keine wirkliche Identität der Flamme von einem Moment zum anderen im Laufe der Zeit.
Ebenso ist es eine Illusion, dass es sich um dieselbe Person handelt, die heute Morgen in den Unterricht kam.
Dies ist analog zu den physikalischen Eigenschaften des Gehirns, die einen mentalen Zustand hervorrufen.
Die Churchlands berufen sich häufig auf das Schicksal anderer, fehlerhafter populärer Theorien und Ontologien, die im Laufe der Geschichte entstanden sind.
Einige Philosophen argumentieren, dass dies auf eine zugrunde liegende konzeptionelle Verwirrung zurückzuführen sei.
Vielmehr sollte einfach akzeptiert werden, dass menschliche Erfahrungen auf unterschiedliche Weise beschrieben werden können – beispielsweise in einem mentalen und in einem biologischen Vokabular.
Das Gehirn ist einfach der falsche Kontext für die Verwendung von mentalem Vokabular – die Suche nach mentalen Zuständen des Gehirns ist daher ein Kategorienfehler oder eine Art Denkfehler.
Und es ist charakteristisch für einen mentalen Zustand, dass er eine gewisse Erfahrungsqualität hat, z.B. vom Schmerz, dass es wehtut.
Die Existenz zerebraler Ereignisse allein kann nicht erklären, warum sie von entsprechenden qualitativen Erfahrungen begleitet werden.
Dies folgt aus einer Annahme über die Möglichkeit reduktiver Erklärungen.
Der deutsche Philosoph Martin Heidegger aus dem 20. Jahrhundert kritisierte die ontologischen Annahmen, die einem solchen reduktiven Modell zugrunde liegen, und behauptete, dass es unmöglich sei, Erfahrungen auf diese Weise zu verstehen.
Dieses Problem der Erklärung introspektiver Ich-Aspekte von mentalen Zuständen und Bewusstsein im Allgemeinen im Sinne der quantitativen Neurowissenschaft der Dritten Person wird als Erklärungslücke bezeichnet.
Es handelt sich dabei um zwei getrennte Kategorien, von denen die eine nicht auf die andere reduziert werden kann.
Für Nagel ist die Wissenschaft noch nicht in der Lage, subjektive Erfahrungen zu erklären, weil sie noch nicht auf dem erforderlichen Niveau oder der Art von Wissen angekommen ist.
Diese Eigenschaft mentaler Zustände bringt mit sich, dass sie über Inhalte und semantische Bezüge verfügen und ihnen daher Wahrheitswerte zugeschrieben werden können.
Aber mentale Ideen oder Urteile sind wahr oder falsch. Wie können dann mentale Zustände (Ideen oder Urteile) natürliche Prozesse sein?
Wenn die Tatsache wahr ist, dann ist die Idee wahr; andernfalls ist es falsch.
Da mentale Prozesse eng mit körperlichen Prozessen verbunden sind, spielen die Beschreibungen, die die Naturwissenschaften über den Menschen liefern, eine wichtige Rolle in der Philosophie des Geistes.
Innerhalb der Neurobiologie gibt es viele Teildisziplinen, die sich mit den Zusammenhängen zwischen psychischen und körperlichen Zuständen und Prozessen befassen: Die sensorische Neurophysiologie untersucht den Zusammenhang zwischen Wahrnehmungs- und Stimulationsprozessen.
Schließlich untersucht die Evolutionsbiologie den Ursprung und die Entwicklung des menschlichen Nervensystems und beschreibt, sofern dieses die Grundlage des Geistes ist, auch die ontogenetische und phylogenetische Entwicklung geistiger Phänomene ab ihren primitivsten Stadien.
Ein einfaches Beispiel ist die Multiplikation.
Diese Frage ist aufgrund von Untersuchungen auf dem Gebiet der künstlichen Intelligenz (KI) in den Vordergrund vieler philosophischer Debatten gerückt.
Das Ziel einer starken KI hingegen ist ein Computer mit einem Bewusstsein, das dem des Menschen ähnelt.
Der Turing-Test wurde vielfach kritisiert, die bekannteste ist wohl das von Searle formulierte chinesische Raumgedankenexperiment.
Die Psychologie erforscht die Gesetze, die diese Geisteszustände miteinander bzw. mit Ein- und Ausgängen in den menschlichen Organismus verbinden.
Ein Gesetz der Formenpsychologie besagt, dass Objekte, die sich in die gleiche Richtung bewegen, als zueinander in Beziehung stehend wahrgenommen werden.
Es umfasst Forschung zu Intelligenz und Verhalten, wobei der Schwerpunkt insbesondere auf der Art und Weise liegt, wie Informationen in Nervensystemen (Menschen oder anderen Tieren) und Maschinen (z. B. Computern) dargestellt, verarbeitet und umgewandelt werden (in Fähigkeiten wie Wahrnehmung, Sprache, Gedächtnis, Denken und Emotionen). ).
Dennoch unterscheidet sich Hegels Werk radikal vom Stil der angloamerikanischen Geistesphilosophie.
Die von Edmund Husserl gegründete Phänomenologie konzentriert sich auf die Inhalte des menschlichen Geistes (siehe Noema) und darauf, wie Prozesse unsere Erfahrungen prägen.
Dies ist bei materialistischen Deterministen der Fall.
Manche gehen noch einen Schritt weiter: Menschen können nicht selbst bestimmen, was sie wollen und was sie tun.
Diejenigen, die diese Position vertreten, legen nahe, dass die Frage „Sind wir frei?“ gestellt wird.
Es ist nicht angebracht, Freiheit mit Unbestimmtheit gleichzusetzen.
Der bedeutendste Kompatibilist in der Geschichte der Philosophie war David Hume.
Diese Philosophen behaupten, dass der Lauf der Welt entweder a) nicht vollständig durch das Naturrecht bestimmt wird, wenn das Naturrecht durch eine physikalisch unabhängige Instanz abgefangen wird, b) nur durch das indeterministische Naturrecht bestimmt wird oder c) durch das indeterministische Naturrecht im Einklang mit dem Subjektiven bestimmt wird Anstrengung physikalisch nicht reduzierbarer Handlungsfähigkeit.
Sie argumentieren wie folgt: Wenn unser Wille durch nichts bestimmt wird, dann wünschen wir uns das, was wir uns wünschen, rein zufällig.
Die Vorstellung eines Selbst als unveränderlicher Wesenskern leitet sich von der Vorstellung einer immateriellen Seele ab.
Mantranga, das wichtigste Regierungsorgan dieser Staaten, bestand aus dem König, dem Premierminister, dem Oberbefehlshaber der Armee und dem Oberpriester des Königs.
Das Arthashastra bietet einen Bericht über die Wissenschaft der Politik eines weisen Herrschers, die Außen- und Kriegspolitik, das System eines Spionagestaates sowie die Überwachung und wirtschaftliche Stabilität des Staates.
Die wichtigsten Philosophien dieser Zeit – Konfuzianismus, Legalismus, Mohismus, Agrarismus und Taoismus – hatten jeweils einen politischen Aspekt in ihren philosophischen Schulen.
Der Legalismus befürwortete eine äußerst autoritäre Regierung, die auf drakonischen Strafen und Gesetzen beruhte.
In der Spätantike hatte jedoch die „traditionalistische“ asharitische Sichtweise des Islam im Allgemeinen gesiegt.
Im westlichen Denken geht man jedoch allgemein davon aus, dass es sich hierbei um einen spezifischen Bereich handelte, der ausschließlich den großen Philosophen des Islam vorbehalten war: al-Kindi (Alkindus), al-Farabi (Abunaser), İbn Sina (Avicenna), Ibn Bajjah (Avempace). und Ibn Rushd (Averroes).
Beispielsweise gelten die Vorstellungen der Khawarij in den frühen Jahren der islamischen Geschichte zu Kalifat und Ummah oder die des schiitischen Islam zum Konzept der Imamah als Belege für politisches Denken.
Der Aristotelismus blühte auf, als im islamischen Goldenen Zeitalter eine Fortsetzung der peripatetischen Philosophen entstand, die die Ideen des Aristoteles im Kontext der islamischen Welt umsetzten.
Zu den anderen bemerkenswerten politischen Philosophen dieser Zeit gehört Nizam al-Mulk, ein persischer Gelehrter und Wesir des Seldschukenreichs, der das Siyasatnama, auf Englisch das „Buch der Regierung“, verfasste.
Der vielleicht einflussreichste politische Philosoph des mittelalterlichen Europas war der heilige Thomas von Aquin, der dazu beitrug, die Werke des Aristoteles wieder einzuführen, die zusammen mit den Kommentaren von Averroes nur über das muslimische Spanien in das katholische Europa gelangt waren.
Andere, wie Nicole Oresme in ihrem Livre de Politiques, lehnten dieses Recht, einen ungerechten Herrscher zu stürzen, kategorisch ab.
Dieses Werk sowie „The Discourses“, eine gründliche Analyse der klassischen Antike, trugen wesentlich dazu bei, das moderne politische Denken im Westen zu beeinflussen.
Auf jeden Fall präsentiert Machiavelli eine pragmatische und in gewisser Weise konsequentialistische Sichtweise der Politik, in der Gut und Böse lediglich Mittel sind, die dazu dienen, ein Ziel herbeizuführen – d. h. den Erwerb und die Aufrechterhaltung absoluter Macht.
Diese Theoretiker wurden von zwei grundlegenden Fragen angetrieben: Erstens, aufgrund welches Rechts oder Bedürfnisses Menschen Staaten bilden; und zweitens, wie die beste Form für einen Staat aussehen könnte.
Der Begriff „Regierung“ würde sich auf eine bestimmte Gruppe von Menschen beziehen, die die Institutionen des Staates innehaben und die Gesetze und Verordnungen erlassen, an die das Volk, einschließlich sich selbst, gebunden wäre.
Es kann auch als die Idee des freien Marktes verstanden werden, die auf den internationalen Handel angewendet wird.
Der schärfste Kritiker der Kirche in Frankreich war François Marie Arouet de Voltaire, eine repräsentative Persönlichkeit der Aufklärung.
Mein einziges Bedauern im Tod ist, dass ich Ihnen bei diesem edlen Unterfangen, dem schönsten und respektabelsten, das der menschliche Geist aufzeigen kann, nicht helfen kann.
Locke widerlegte Sir Robert Filmers väterlicherseits begründete politische Theorie zugunsten eines natürlichen Systems, das auf der Natur in einem bestimmten gegebenen System basiert.
Im Gegensatz zu Thomas von Aquins vorherrschender Ansicht über die Erlösung der Seele von der Erbsünde glaubt Locke, dass der Geist des Menschen als tabula rasa auf diese Welt kommt.
Auch wenn man sich über Freiheitseinschränkungen durch wohlwollende Monarchen oder Aristokraten Sorgen machen könnte, besteht die traditionelle Sorge darin, dass Herrscher, wenn sie gegenüber den Regierten politisch nicht rechenschaftspflichtig sind, in ihren eigenen Interessen regieren und nicht im Interesse der Regierten.
Gerechtigkeit beinhaltet Pflichten, die vollkommene Pflichten sind – das heißt Pflichten, die mit Rechten korreliert sind.
Er verwendet On Liberty, um die Gleichstellung der Geschlechter in der Gesellschaft zu diskutieren.
Die Freiheit der Alten war eine partizipative republikanische Freiheit, die den Bürgern das Recht gab, durch Debatten und Abstimmungen in der öffentlichen Versammlung direkten Einfluss auf die Politik zu nehmen.
Die antike Freiheit war auch auf relativ kleine und homogene Gesellschaften beschränkt, in denen die Menschen bequem an einem Ort versammelt werden konnten, um öffentliche Angelegenheiten zu erledigen.
Stattdessen würden die Wähler Vertreter wählen, die im Namen des Volkes im Parlament beraten würden und den Bürgern die Notwendigkeit einer täglichen politischen Beteiligung ersparen würden.
In Leviathan legte Hobbes seine Lehre von der Gründung von Staaten und legitimen Regierungen und der Schaffung einer objektiven Wissenschaft der Moral dar.
In diesem Zustand hätte jede Person ein Recht oder eine Lizenz für alles auf der Welt.
Es wurde 1762 veröffentlicht und wurde zu einem der einflussreichsten Werke der politischen Philosophie in der westlichen Tradition.
Diejenigen, die sich für die Herren anderer halten, sind in der Tat größere Sklaven als sie selbst.“
Die industrielle Revolution führte zu einer parallelen Revolution im politischen Denken.
Mitte des 19. Jahrhunderts entwickelte sich der Marxismus, und der Sozialismus im Allgemeinen gewann zunehmend an Unterstützung in der Bevölkerung, vor allem bei der städtischen Arbeiterklasse.
Im Gegensatz zu Marx, der an den historischen Materialismus glaubte, glaubte Hegel an die Phänomenologie des Geistes.
In der angloamerikanischen Welt begannen Antiimperialismus und Pluralismus um die Wende zum 20. Jahrhundert an Bedeutung zu gewinnen.
Dies war die Zeit von Jean-Paul Sartre und Louis Althusser, und die Siege von Mao Zedong in China und Fidel Castro in Kuba sowie die Ereignisse vom Mai 1968 führten zu einem verstärkten Interesse an revolutionärer Ideologie, insbesondere bei der Neuen Linken.
Kolonialismus und Rassismus waren wichtige Themen, die aufkamen.
Der Aufstieg des Feminismus, der sozialen LGBT-Bewegungen und das Ende der Kolonialherrschaft sowie der politische Ausschluss von Minderheiten wie Afroamerikanern und sexuellen Minderheiten in der entwickelten Welt haben dazu geführt, dass feministisches, postkoloniales und multikulturelles Denken an Bedeutung gewonnen hat.
Rawls nutzte ein Gedankenexperiment, die ursprüngliche Position, bei der repräsentative Parteien hinter einem Schleier der Unwissenheit Prinzipien der Gerechtigkeit für die Grundstruktur der Gesellschaft auswählen.
Zeitgleich mit dem Aufstieg der analytischen Ethik im angloamerikanischen Denken entstanden in Europa zwischen den 1950er und 1980er Jahren mehrere neue Philosophierichtungen, die auf die Kritik bestehender Gesellschaften abzielten.
In etwas anderer Richtung legten eine Reihe anderer kontinentaler Denker – die immer noch weitgehend vom Marxismus beeinflusst waren – neue Schwerpunkte auf den Strukturalismus und auf eine „Rückkehr zu Hegel“.
Eine weitere Debatte entwickelte sich um die (deutliche) Kritik von Michael Walzer, Michael Sandel und Charles Taylor an der liberalen politischen Theorie.
Kommunitaristen neigen dazu, eine stärkere lokale Kontrolle sowie eine Wirtschafts- und Sozialpolitik zu unterstützen, die das Wachstum des Sozialkapitals fördert.
Ein Paar sich überschneidender politischer Perspektiven, die gegen Ende des 20. Jahrhunderts aufkamen, sind der Republikanismus (oder Neo- oder Bürgerrepublikanismus) und der Capability-Ansatz.
Für einen Republikaner ist der bloße Status als Sklave, unabhängig davon, wie dieser Sklave behandelt wird, anstößig.
Sowohl der Fähigkeitsansatz als auch der Republikanismus betrachten Wahlmöglichkeiten als etwas, das mit Ressourcen ausgestattet werden muss.
Bemerkenswert für die Theorien, dass Menschen soziale Tiere sind und dass die Polis (der altgriechische Stadtstaat) existierte, um diesen Tieren ein gutes Leben zu ermöglichen.
Burke war einer der größten Unterstützer der Amerikanischen Revolution.
Chomsky ist ein führender Kritiker der US-Außenpolitik, des Neoliberalismus und des zeitgenössischen Staatskapitalismus, des israelisch-palästinensischen Konflikts und der Mainstream-Nachrichtenmedien.
William E. Connolly: Hat dazu beigetragen, die postmoderne Philosophie in die politische Theorie einzuführen und neue Theorien des Pluralismus und der agonistischen Demokratie voranzutreiben.
Thomas Hill Green: Moderner liberaler Denker und früher Befürworter der positiven Freiheit.
Sein Frühwerk war stark von der Frankfurter Schule beeinflusst.
Er befürwortete den Kapitalismus des freien Marktes, in dem die Hauptaufgabe des Staates darin besteht, die Rechtsstaatlichkeit aufrechtzuerhalten und die Entwicklung spontaner Ordnung zu ermöglichen.
David Hume: Hume kritisierte die Gesellschaftsvertragstheorie von John Locke und anderen, die auf dem Mythos einer tatsächlichen Vereinbarung beruhe.
Am bekanntesten für die Unabhängigkeitserklärung der Vereinigten Staaten.
Argumentierte, dass eine internationale Organisation notwendig sei, um den Weltfrieden zu wahren.
Er wich von Hobbes ab, indem er ausgehend von der Annahme einer Gesellschaft, in der moralische Werte unabhängig von staatlicher Autorität sind und weithin geteilt werden, für eine Regierung plädierte, deren Macht auf den Schutz des persönlichen Eigentums beschränkt ist.
Einer der Begründer des westlichen Marxismus.
Beschrieb Staatskunst aus realistischer Sicht, anstatt sich auf Idealismus zu verlassen.
Als politischer Theoretiker glaubte er an die Gewaltenteilung und schlug ein umfassendes System von Kontrollmechanismen vor, die notwendig sind, um die Rechte des Einzelnen vor der Tyrannei der Mehrheit zu schützen.
Einführung des Konzepts der „repressiven Desublimation“, bei dem soziale Kontrolle nicht nur durch direkte Kontrolle, sondern auch durch Manipulation des Verlangens erfolgen kann.
Erstellte den Begriff der Ideologie im Sinne von (wahren oder falschen) Überzeugungen, die soziales Handeln prägen und kontrollieren.
Menzius: Als einer der bedeutendsten Denker der konfuzianischen Schule ist er der erste Theoretiker, der ein kohärentes Argument für die Verpflichtung der Herrscher gegenüber den Beherrschten vorbringt.
Montesquieu: Analysierter Schutz des Volkes durch ein „Gleichgewicht der Kräfte“ in den Gliederungen eines Staates.
Seine Dolmetscher haben den Inhalt seiner politischen Philosophie diskutiert.
Platon: Schrieb einen langen Dialog „Die Republik“, in dem er seine politische Philosophie darlegte: Die Bürger sollten in drei Kategorien eingeteilt werden.
Ayn Rand: Begründerin des Objektivismus und treibende Kraft der objektivistischen und libertären Bewegungen im Amerika der Mitte des 20. Jahrhunderts.
Die Regierung sollte auf die gleiche Weise von der Wirtschaft getrennt werden, und aus den gleichen Gründen wurde sie von der Religion getrennt.
Adam Smith: Wird oft als Begründer der modernen Wirtschaftswissenschaften bezeichnet; erklärte die Entstehung wirtschaftlicher Vorteile aus dem eigennützigen Verhalten („der unsichtbaren Hand“) von Handwerkern und Händlern.
Sokrates: Wird aufgrund seines sprachlichen Einflusses auf athenische Zeitgenossen weithin als Begründer der westlichen politischen Philosophie angesehen; Da Sokrates nie etwas geschrieben hat, stammt vieles, was wir über ihn und seine Lehren wissen, von seinem berühmtesten Schüler, Platon.
Max Stirner: Wichtiger Denker innerhalb des Anarchismus und Hauptvertreter der anarchistischen Strömung, die als individualistischer Anarchismus bekannt ist.
Andere Formen der Sozialphilosophie umfassen die politische Philosophie und die Rechtswissenschaft, die sich hauptsächlich mit den Staats- und Regierungsgesellschaften und deren Funktionsweise befassen.
Die vorsokratische Philosophie, auch frühgriechische Philosophie genannt, ist die antike griechische Philosophie vor Sokrates.
Ihre Arbeit und ihr Schreiben sind fast vollständig verloren gegangen.
Die vorsokratische Philosophie begann im 6. Jahrhundert v. Chr. mit den drei Milesianern: Thales, Anaximander und Anaximenes.
Xenophanes ist für seine Kritik am Anthropomorphismus der Götter bekannt.
Die eleatische Schule (Parmenides, Zenon von Elea und Melissos) folgte im 5. Jahrhundert v. Chr.
Anaxagoras und Empedokles lieferten einen pluralistischen Bericht über die Entstehung des Universums.
Es wurde erstmals vom deutschen Philosophen J.A. verwendet. Eberhard als „vorsokratische Philosophie“ im späten 18. Jahrhundert.
Der Begriff hat Nachteile, da einige der Vorsokratiker ein großes Interesse an Ethik und der Frage hatten, wie man das beste Leben führt.
Laut James Warren wird der Unterschied zwischen den vorsokratischen Philosophen und den Philosophen der klassischen Ära nicht so sehr durch Sokrates, sondern durch die Geographie und die erhaltenen Texte bestimmt.
Der Gelehrte André Laks unterscheidet zwei Traditionen der Trennung von Vorsokratikern und Sokratikern, die bis in die klassische Ära zurückreichen und bis in die heutige Zeit reichen.
Viele der Werke tragen den Titel „Peri Physeos“ oder „Über die Natur“, ein Titel, der wahrscheinlich später von anderen Autoren vergeben wurde.
Die unklare Sprache, die sie verwendeten, erschwert ihre Interpretation zusätzlich.
Theophrastus, der Nachfolger von Aristoteles, schrieb ein enzyklopädisches Buch „Meinung der Physiker“, das in der Antike das Standardwerk über die Vorsokratiker war.
Wissenschaftler verwenden dieses Buch nun, um die Fragmente mithilfe eines Codierungsschemas namens Diels-Kranz-Nummerierung zu referenzieren.
Danach folgt ein Code, der angibt, ob es sich bei dem Fragment um eine Testimonia handelt, codiert als „A“ oder „B“, wenn es sich um ein direktes Zitat des Philosophen handelt.
Die vorsokratische Ära dauerte etwa zwei Jahrhunderte, in denen sich das persische Achämenidenreich nach Westen ausdehnte, während die Griechen auf Handels- und Seewegen vordrangen und Zypern und Syrien erreichten.
Die Griechen empörten sich 499 v. Chr., wurden aber schließlich 494 v. Chr. besiegt.
Mehrere Faktoren trugen zur Entstehung der vorsokratischen Philosophie im antiken Griechenland bei.
Ein weiterer Faktor war die Leichtigkeit und Häufigkeit innergriechischer Reisen, die zu einer Vermischung und einem Vergleich von Ideen führte.
Auch das demokratische politische System unabhängiger Poleis trug zum Aufstieg der Philosophie bei.
Die Ideen der Philosophen waren gewissermaßen Antworten auf Fragen, die im Werk von Homer und Hesiod subtil präsent waren.
Sie gelten als Vorläufer der Vorsokratiker, da sie sich mit dem Ursprung der Welt auseinandersetzen und traditionelle Folklore und Legenden systematisch ordnen wollen.
Die ersten vorsokratischen Philosophen reisten auch ausgiebig in andere Länder, was bedeutet, dass das vorsokratische Denken sowohl im Ausland als auch im Inland Wurzeln hatte.
Die vorsokratischen Philosophen teilten die Intuition, dass es eine einzige Erklärung gab, die sowohl die Pluralität als auch die Singularität des Ganzen erklären konnte – und dass diese Erklärung nicht auf direkten Handlungen der Götter beruhen würde.
Viele suchten nach dem materiellen Prinzip (arche) der Dinge und nach der Methode ihres Entstehens und Verschwindens.
In ihrem Bemühen, den Kosmos zu verstehen, prägten sie neue Begriffe und Konzepte wie Rhythmus, Symmetrie, Analogie, Deduktionismus, Reduktionismus, Mathematisierung der Natur und andere.
Es könnte den Anfang oder Ursprung bedeuten, mit dem Unterton, dass es eine Auswirkung auf die folgenden Dinge gibt.
Dies könnte an einem Mangel an Instrumenten liegen oder an der Tendenz, die Welt als Einheit und nicht dekonstruierbar zu betrachten, so dass es für ein äußeres Auge unmöglich wäre, winzige Teile der Natur unter experimenteller Kontrolle zu beobachten.
Systematisch, weil sie versuchten, ihre Erkenntnisse zu universalisieren.
Die Vorsokratiker waren keine Atheisten; Sie minimierten jedoch das Ausmaß der Beteiligung der Götter an Naturphänomenen wie Donner oder beseitigten die Götter vollständig aus der natürlichen Welt.
Die erste Phase der vorsokratischen Philosophie, vor allem der Milesianer, Xenophanes und Heraklit, bestand darin, die traditionelle Kosmogonie abzulehnen und zu versuchen, die Natur auf der Grundlage empirischer Beobachtungen und Interpretationen zu erklären.
Die Eleaten waren auch Monisten (sie glaubten, dass nur eine Sache existiert und alles andere nur eine Transformation davon ist).
Er gilt als der erste westliche Philosoph, da er als erster Vernunft, Beweise und Verallgemeinerungen anwandte.
Thales könnte phönizischer Abstammung gewesen sein.
Thales brachte jedoch die Geometrie voran, indem er mit seinem abstrakten deduktiven Denken universelle Verallgemeinerungen erreichte.
Thales besuchte Sardes, wie damals viele Griechen, wo astronomische Aufzeichnungen geführt wurden und astronomische Beobachtungen für praktische Zwecke (Ölgewinnung) genutzt wurden.
Er führte den Ursprung der Welt auf ein Element und nicht auf ein göttliches Wesen zurück.
Er war Mitglied der Elite von Milet, wohlhabend und ein Staatsmann.
Als Reaktion auf Thales postulierte er als erstes Prinzip eine undefinierte, unbegrenzte Substanz ohne Eigenschaften (apeiron), aus der sich die primären Gegensätze heiß und kalt, feucht und trocken differenzierten.
Er ist auch dafür bekannt, über den Ursprung der Menschheit zu spekulieren.
Er schrieb auch ein Buch über die Natur in Prosa.
Er war ein weitgereister Dichter, dessen Hauptinteressen Theologie und Erkenntnistheorie waren.
Er sagte bekanntlich: Wenn Ochsen, Pferde oder Löwen zeichnen könnten, würden sie ihre Götter als Ochsen, Pferde oder Löwen zeichnen.
Xenophanes bot auch naturalistische Erklärungen für Phänomene wie die Sonne, den Regenbogen und das Elmsfeuer.
Während Xenophanes ein Pessimist hinsichtlich der Fähigkeit des Menschen war, Wissen zu erlangen, glaubte er auch an einen allmählichen Fortschritt durch kritisches Denken.
Heraklit ging davon aus, dass alle Dinge in der Natur einem ständigen Wandel unterliegen.
Feuer wird zu Wasser und Erde und umgekehrt.
Dort behauptet Heraklit, wir dürfen nicht zweimal in denselben Fluss steigen, eine Position, die mit dem Slogan „ta panta rhei“ (Alles fließt) zusammengefasst wird.
Ein weiteres Schlüsselkonzept von Heraklit ist, dass Gegensätze sich irgendwie spiegeln, eine Lehre, die als Einheit der Gegensätze bezeichnet wird.
Heraklits Lehre von der Einheit der Gegensätze legt nahe, dass die Einheit der Welt und ihrer verschiedenen Teile durch die durch die Gegensätze erzeugte Spannung aufrechterhalten wird.
Ein Grundgedanke bei Heraklit ist Logos, ein altgriechisches Wort mit verschiedenen Bedeutungen; Heraklit könnte in seinem Buch bei jeder Verwendung eine andere Bedeutung des Wortes verwendet haben.
Einige Jahrzehnte später musste er Kroton verlassen und nach Metapontum umsiedeln.
Sie brachten seine Ideen voran und kamen zu der Behauptung, dass alles aus Zahlen bestehe, das Universum aus Zahlen bestehe und alles ein Spiegelbild von Analogien und geometrischen Beziehungen sei.
Ihre Lebensweise war asketisch und sie verzichteten auf verschiedene Vergnügungen und Nahrungsmittel.
Andere vorsokratische Philosophen verspotteten Pythagoras wegen seines Glaubens an die Reinkarnation.
Der Pythagoräismus beeinflusste spätere christliche Strömungen wie den Neuplatonismus, und seine pädagogischen Methoden wurden von Platon übernommen.
Laut Aristoteles und Diogenes Laertius war Xenophanes der Lehrer von Parmenides, und es wird diskutiert, ob Xenophanes auch als Eleatiker angesehen werden sollte.
Er war der erste, der folgerte, dass die Erde kugelförmig ist.
Parmenides schrieb ein schwer zu interpretierendes Gedicht mit dem Titel „Über die Natur“ oder „Über das Was-ist“, das die spätere griechische Philosophie wesentlich beeinflusste.
Das Gedicht besteht aus drei Teilen: dem Proem (d. h. Vorwort), dem Weg der Wahrheit und dem Weg der Meinung.
Der Weg der Wahrheit wurde damals und noch heute als weitaus wichtiger angesehen.
Daher sind alle Dinge, die wir für wahr halten, auch wir selbst, falsche Darstellungen.
Die Göttin lehrt Kouros, seine Argumentation zu nutzen, um zu verstehen, ob verschiedene Behauptungen wahr oder falsch sind, und verwirft Sinne als trügerisch.
Zeno und Melissus führten Parmenides‘ Gedanken zur Kosmologie fort.
Er versuchte zu erklären, warum wir glauben, dass verschiedene nicht existierende Objekte existieren.
Anaxagoras wurde in Ionien geboren, war aber der erste große Philosoph, der nach Athen auswanderte.
Anaxagoras hatte auch großen Einfluss auf Sokrates.
Es gibt unterschiedliche Interpretationen darüber, was er meinte.
Alle Objekte waren Mischungen verschiedener Elemente wie Luft, Wasser und anderen.
Nous galt auch als Baustein des Kosmos, kommt aber nur in lebenden Objekten vor.
Anaxagoras entwickelte das Milesianische Denken zur Erkenntnistheorie weiter und strebte danach, eine Erklärung zu finden, die für alle Naturphänomene gültig sein könnte.
Laut Diogenes Laertius schrieb Empedokles zwei Bücher in Form von Gedichten: Peri Physeos (Über die Natur) und Katharmoi (Reinigungen).
Er führt auch Anaxagoras‘ Gedanken über die vier „Wurzeln“ (d. h. klassische Elemente) fort, die durch ihre Vermischung alle Dinge um uns herum erschaffen.
Diese beiden Kräfte sind gegensätzlich und vereinen sich durch Einwirkung auf das Material der vier Wurzeln in Harmonie oder reißen die vier Wurzeln auseinander, wobei die resultierende Mischung alle existierenden Dinge darstellt.
Sie sind vor allem für ihre atomare Kosmologie bekannt, obwohl ihre Gedanken viele andere Bereiche der Philosophie umfassten, wie Ethik, Mathematik, Ästhetik, Politik und sogar Embryologie.
Demokrit und Leukipp waren Skeptiker hinsichtlich der Zuverlässigkeit unserer Sinne, aber sie waren zuversichtlich, dass Bewegung existiert.
Atome bewegen sich in der Leere, interagieren miteinander und bilden auf rein mechanische Weise die Pluralität der Welt, in der wir leben.
Demokrit kam zu dem Schluss, dass einige unserer Sinne nicht real, sondern konventionell sind, da alles aus Atomen und Leere besteht.
Sie griffen das traditionelle Denken an, von den Göttern bis zur Moral, und ebneten den Weg für weitere Fortschritte in der Philosophie und anderen Disziplinen wie Theater, Sozialwissenschaften, Mathematik und Geschichte.
Die Sophisten lehrten Rhetorik und wie man Probleme aus mehreren Blickwinkeln angeht.
Gorgias schrieb ein Buch mit dem Titel „Über die Natur“, in dem er die Konzepte der Eleatiker von „Was ist“ und „Was ist nicht“ angriff.
Antiphon stellte das Naturrecht dem Gesetz der Stadt entgegen.
Er versuchte, sowohl die Vielfalt als auch die Einheit des Kosmos zu erklären.
Diogenes von Apollonia kehrte zum Milesischen Monismus zurück, jedoch mit einem etwas eleganteren Gedanken.
Während Pythagoras und Empedokles ihre selbsternannte Weisheit mit ihrem göttlich inspirierten Status verknüpften, versuchten sie, die Sterblichen zu lehren oder zu drängen, die Wahrheit über den natürlichen Bereich zu suchen – Pythagoras durch Mathematik und Geometrie und Empedokles durch die Auseinandersetzung mit Erfahrungen.
Sie griffen die traditionellen Götterdarstellungen an, die Homer und Hesiod etabliert hatten, stellten die griechische Volksreligion auf den Prüfstand und leiteten so die Spaltung zwischen Naturphilosophie und Theologie ein.
Der theologische Gedanke beginnt mit den Milesian-Philosophen.
Xenophanes stellte drei Voraussetzungen für Gott: Er musste vollkommen gut und unsterblich sein und in seinem Aussehen nicht dem Menschen ähneln, was einen großen Einfluss auf das westliche religiöse Denken hatte.
Anaxagoras behauptete, dass die kosmische Intelligenz (nous) den Dingen Leben verleiht.
Es war Hippokrates (oft als Vater der Medizin gefeiert), der die beiden Domänen – aber nicht vollständig – trennte.
Die sich ständig verändernde Natur wird durch Heraklits Axiom panta rhei (alles ist im Fluss) zusammengefasst.
Die Vorsokratiker versuchten, die verschiedenen Aspekte der Natur durch Rationalismus, Beobachtungen und Erklärungen zu verstehen, die als wissenschaftlich angesehen werden konnten, und brachten so den westlichen Rationalismus hervor.
Anaximander brachte das Prinzip der hinreichenden Vernunft vor, ein revolutionäres Argument, das auch zu dem Prinzip führen würde, dass nichts aus nichts entsteht.
Xenophanes brachte auch eine Kritik der anthropomorphen Religion vor, indem er auf rationale Weise die Inkonsistenz der Götterdarstellungen in der griechischen Volksreligion hervorhob.
Andere Vorsokratiker versuchten ebenfalls, die Frage der Arche zu beantworten und boten verschiedene Antworten an, aber der erste Schritt zum wissenschaftlichen Denken war bereits getan.
Das philosophische Denken der Vorsokratiker hatte großen Einfluss auf spätere Philosophen, Historiker und Dramatiker.
Die Naturforscher beeindruckten den jungen Sokrates und er interessierte sich für die Suche nach der Substanz des Kosmos, doch sein Interesse ließ nach, als er sich zunehmend mehr auf Erkenntnistheorie, Tugend und Ethik als auf die natürliche Welt konzentrierte.
Cicero analysierte seine Ansichten über die Vorsokratiker in seinen Tusculanae Disputationes, indem er die theoretische Natur des vorsokratischen Denkens von früheren „Weisen“ unterschied, die an eher praktischen Fragen interessiert waren.
Aristoteles besprach die Vorsokratiker im ersten Buch der Metaphysik als Einführung in seine eigene Philosophie und die Suche nach Arche.
Francis Bacon, ein Philosoph des 16. Jahrhunderts, der für die Weiterentwicklung der wissenschaftlichen Methode bekannt war, war wahrscheinlich der erste Philosoph der Neuzeit, der in seinen Texten ausgiebig vorsokratische Axiome verwendete.
Friedrich Nietzsche verehrte die Vorsokratiker zutiefst und nannte sie „Tyrannen des Geistes“, um ihren Gegensatz und seine Bevorzugung gegenüber Sokrates und seinen Nachfolgern zum Ausdruck zu bringen.
Seiner Erzählung zufolge, die in vielen seiner Bücher dargelegt wird, war die vorsokratische Ära die glorreiche Ära Griechenlands, während das darauf folgende sogenannte Goldene Zeitalter laut Nietzsche ein Zeitalter des Verfalls war.
Auch wenn diese Periode – in ihrem früheren Teil als Frühlings- und Herbstperiode und als Periode der Streitenden Reiche bekannt – in ihrem späteren Teil von Chaos und blutigen Schlachten geprägt war, wird sie aufgrund einer Vielzahl von Ereignissen auch als das Goldene Zeitalter der chinesischen Philosophie bezeichnet Gedanken und Ideen wurden frei entwickelt und diskutiert.
Taoismus (auch Daoismus genannt), eine Philosophie, die die drei Juwelen des Tao betont: Mitgefühl, Mäßigung und Demut, während sich das taoistische Denken im Allgemeinen auf die Natur, die Beziehung zwischen der Menschheit und dem Kosmos, konzentriert; Gesundheit und Langlebigkeit; und Wu Wei (Handlung durch Untätigkeit).
Agrarismus oder die Schule des Agrarismus, die den bäuerlichen utopischen Kommunalismus und Egalitarismus befürwortete.
Gelehrte dieser Schule waren gute Redner, Debattierer und Taktiker.
Die Schule der „Minor-Talks“, die keine einzigartige Denkschule war, sondern eine Philosophie, die sich aus allen Gedanken zusammensetzte, die von normalen Menschen auf der Straße diskutiert wurden und von ihnen stammten.
Der Konfuzianismus war während der Han-Dynastie besonders stark ausgeprägt, deren größter Denker Dong Zhongshu war, der den Konfuzianismus mit den Gedanken der Zhongshu-Schule und der Theorie der Fünf Elemente verband.
Insbesondere widerlegten sie die Annahme, dass Konfuzius eine gottähnliche Figur sei, und betrachteten ihn als den größten Weisen, aber lediglich als Menschen und Sterblichen.
Der Buddhismus kam um das 1. Jahrhundert n. Chr. nach China, erlangte jedoch erst in der nördlichen und südlichen Sui- und Tang-Dynastie erheblichen Einfluss und Anerkennung.
Dies führt zur Frage nach dem einen Wesen, das der Vielfalt empirischer Phänomene und dem Ursprung aller Dinge zugrunde liegt.
Sieben Rishis – Atri, Bharadwaja, Gautama, Jamadagni, Kasyapa, Vasistha, Vishwamitra.
Die antike griechische Philosophie entstand im 6. Jahrhundert v. Chr. und markierte das Ende des griechischen Mittelalters.
Es befasste sich mit einer Vielzahl von Themen, darunter Astronomie, Erkenntnistheorie, Mathematik, politische Philosophie, Ethik, Metaphysik, Ontologie, Logik, Biologie, Rhetorik und Ästhetik.
Klare, ununterbrochene Einflusslinien führen von antiken griechischen und hellenistischen Philosophen über die römische Philosophie, die frühislamische Philosophie, die mittelalterliche Scholastik, die europäische Renaissance und das Zeitalter der Aufklärung.
Aber sie brachten sich selbst das Vernunft bei.
Thales inspirierte die Milesische Philosophieschule und wurde von Anaximander gefolgt, der argumentierte, dass das Substrat oder die Arche nicht Wasser oder eines der klassischen Elemente sein könne, sondern stattdessen etwas „Unbegrenztes“ oder „Unbestimmtes“ (auf Griechisch das Apeiron) sei.
Im Gegensatz zur Milesian-Schule, die ein stabiles Element als Arche postuliert, lehrte Heraklit, dass panta rhei („alles fließt“), wobei das diesem ewigen Fluss am nächsten stehende Element das Feuer ist.
Er argumentierte, dass Sein per Definition Ewigkeit impliziert, während nur das, was ist, gedacht werden kann; ein Ding, das darüber hinaus ist, kann nicht mehr oder weniger sein, und so ist die Verdünnung und Verdichtung der Milesianer hinsichtlich des Seins unmöglich; Schließlich erfordert die Bewegung, dass neben dem sich bewegenden Ding etwas existiert (z.
Um dies zu untermauern, versuchte Parmenides‘ Schüler Zenon von Elea zu beweisen, dass der Begriff der Bewegung absurd sei und Bewegung als solche nicht existiere.
Leukipp schlug auch einen ontologischen Pluralismus mit einer Kosmogonie vor, die auf zwei Hauptelementen basiert: dem Vakuum und den Atomen.
Während Philosophie schon vor Sokrates eine etablierte Beschäftigung war, bezeichnet Cicero ihn als „den ersten, der die Philosophie vom Himmel herabholte, sie in den Städten verbreitete, sie in Familien einführte und sie dazu zwang, Leben und Moral sowie Gut und Böse zu untersuchen“. "
Die Tatsache, dass viele Gespräche über Sokrates (wie von Platon und Xenophon erzählt) enden, ohne zu einem festen Abschluss gekommen zu sein oder aporetisch, hat eine Debatte über die Bedeutung der sokratischen Methode angeregt.
Sokrates lehrte, dass niemand das Schlechte wünscht. Wenn also jemand etwas tut, das wirklich schlecht ist, muss dies unfreiwillig oder aus Unwissenheit geschehen. Folglich ist jede Tugend Wissen.
Der große Staatsmann Perikles war jedoch eng mit dieser neuen Gelehrsamkeit verbunden und ein Freund von Anaxagoras, und seine politischen Gegner griffen ihn an, indem sie eine konservative Reaktion gegen die Philosophen ausnutzten; Es wurde zu einem Verbrechen, die Dinge über dem Himmel oder unter der Erde zu untersuchen, Themen, die als gottlos galten.
Sokrates ist jedoch der einzige Angeklagte, der nach diesem Gesetz angeklagt, verurteilt und im Jahr 399 v. Chr. zum Tode verurteilt wurde (siehe Prozess gegen Sokrates).
Platon macht Sokrates zum Hauptgesprächspartner seiner Dialoge und leitet daraus die Grundlage des Platonismus (und damit auch des Neuplatonismus) ab.
Zenon von Citium wiederum adaptierte die Ethik des Zynismus, um den Stoizismus zu artikulieren.
Neben Xenophon ist Platon die wichtigste Informationsquelle über das Leben und den Glauben von Sokrates, und es ist nicht immer einfach, zwischen den beiden zu unterscheiden.
Obwohl die Herrschaft eines Weisen der Herrschaft durch das Gesetz vorzuziehen wäre, können die Weisen nicht umhin, von den Unweisen beurteilt zu werden, und so wird in der Praxis die Herrschaft durch das Gesetz als notwendig erachtet.
Platons Dialoge haben auch metaphysische Themen, das berühmteste davon ist seine Formenlehre.
Es vergleicht die meisten Menschen mit in einer Höhle gefesselten Menschen, die nur auf die Schatten an den Wänden blicken und keine andere Vorstellung von der Realität haben.
Würden diese Reisenden dann wieder die Höhle betreten, wären die Menschen im Inneren (die noch immer nur mit den Schatten vertraut sind) nicht in der Lage, den Berichten über diese „Außenwelt“ zu glauben.
Bertrand Russell, Eine Geschichte der westlichen Philosophie (New York: Simon & Schuster, 1972).
Er kritisiert die in Platons „Republik und Gesetze“ beschriebenen Regime und bezeichnet die Formentheorie als „leere Worte und poetische Metaphern“.
Antisthenes ließ sich von der Askese des Sokrates inspirieren und beschuldigte Platon des Stolzes und der Einbildung.
Es wurde von Euklides von Megara, einem der Schüler von Sokrates, gegründet.
Der Pyrrhonismus betrachtet das Erreichen von Ataraxie (einem Zustand des Gleichmuts) als den Weg zur Eudaimonie.
Seine Ethik basierte auf „dem Streben nach Vergnügen und der Vermeidung von Schmerz“.
Ihre logischen Beiträge finden sich immer noch in der zeitgenössischen Aussagenrechnung.
Diese skeptische Periode des antiken Platonismus, von Arcesilaos bis Philo von Larissa, wurde als Neue Akademie bekannt, obwohl einige antike Autoren weitere Unterteilungen hinzufügten, beispielsweise eine Mittlere Akademie.
Während das Ziel der Pyrrhonisten darin bestand, die Ataraxie zu erreichen, hielten die akademischen Skeptiker nach Arcesilaos die Ataraxie nicht mehr für das zentrale Ziel.
Im Byzantinischen Reich wurden griechische Ideen bewahrt und studiert, und nicht lange nach der ersten großen Ausbreitung des Islam genehmigten die abbasidischen Kalifen jedoch die Sammlung griechischer Manuskripte und stellten Übersetzer ein, um ihr Ansehen zu steigern.
Unter mittelalterlicher Philosophie versteht man die Philosophie, die das gesamte Mittelalter hindurch existierte, die Zeitspanne, die ungefähr vom Untergang des Weströmischen Reiches im 5. Jahrhundert bis zur Renaissance im 15. Jahrhundert reichte.
Mit der möglichen Ausnahme von Avicenna und Averroes betrachteten sich mittelalterliche Denker überhaupt nicht als Philosophen: Für sie waren die Philosophen die antiken heidnischen Schriftsteller wie Platon und Aristoteles.
Eines der am heftigsten diskutierten Themen dieser Zeit war das Thema Glaube vs. Vernunft.
Es besteht allgemein Einigkeit darüber, dass es mit Augustinus (354–430) beginnt, der streng zur klassischen Periode gehört, und mit der dauerhaften Wiederbelebung der Gelehrsamkeit im späten 11. Jahrhundert, zu Beginn des Hochmittelalters, endet.
In späteren Zeiten wurden Mönche für die Ausbildung von Administratoren und Kirchenmännern eingesetzt.
Ein Großteil des Werks des Aristoteles war zu dieser Zeit im Westen unbekannt.
Augustinus gilt als der größte der Kirchenväter.
Über tausend Jahre lang gab es kaum ein lateinisches theologisches oder philosophisches Werk, das nicht seine Schriften zitierte oder sich nicht auf seine Autorität berief.
Er wurde 510 Konsul im Königreich der Ostgoten.
Er verfasste Kommentare zu diesen Werken und zur Isagoge von Porphyrius (ein Kommentar zu den Kategorien).
In dieser Zeit kam es zu mehreren Lehrkontroversen, beispielsweise zu der Frage, ob Gott einige für die Erlösung und andere für die Verdammnis vorherbestimmt hatte.
Ist die Hostie dieselbe wie der historische Körper Christi?
In dieser Zeit kam es auch zu einer Wiederbelebung der Wissenschaft.
Später erlebte Fleury unter dem heiligen Abt von Fleury (Abt 988–1004), dem Leiter der reformierten Abteischule, ein zweites goldenes Zeitalter.
Das frühe 13. Jahrhundert markierte den Höhepunkt der Erholung der griechischen Philosophie.
Mächtige normannische Könige versammelten als Zeichen ihres Ansehens gebildete Männer aus Italien und anderen Gebieten an ihren Höfen.
In dieser Zeit entstanden in den großen Städten Europas Universitäten, und rivalisierende Geistliche Orden innerhalb der Kirche begannen um die politische und intellektuelle Kontrolle über diese Zentren des Bildungslebens zu kämpfen.
Die großen Vertreter des dominikanischen Denkens in dieser Zeit waren Albertus Magnus und (insbesondere) Thomas von Aquin, dessen kunstvolle Synthese aus griechischem Rationalismus und christlicher Lehre schließlich die katholische Philosophie definierte.
Thomas von Aquin zeigte, wie es möglich war, einen Großteil der Philosophie des Aristoteles zu übernehmen, ohne in die „Irrtümer“ des Kommentators Averroes zu verfallen.
Das Problem des Bösen: Die klassischen Philosophen hatten über die Natur des Bösen spekuliert, aber das Problem, wie ein allmächtiger, allwissender und liebender Gott ein System der Dinge schaffen könnte, in dem das Böse existiert, entstand erstmals im Mittelalter.
Ab dem 14. Jahrhundert bereitete jedoch die zunehmende Verwendung mathematischer Argumente in der Naturphilosophie den Weg für den Aufstieg der Wissenschaft in der frühen Neuzeit.
In der früheren Periode verfassten Schriftsteller wie Peter Abaelard Kommentare zu den Werken der Alten Logik (Aristoteles‘ Kategorien, Über die Interpretation und die Isagoge des Porphyrius).
(Das Wort „Intentionalität“ wurde von Franz Brentano wiederbelebt, der den mittelalterlichen Sprachgebrauch widerspiegeln wollte.)
Die Bezeichnung „Renaissance-Philosophie“ wird von Wissenschaftlern der Geistesgeschichte verwendet, um sich auf das Denken der Zeit zu beziehen, die in Europa etwa zwischen 1355 und 1650 verlief (die Daten verschieben sich für Mittel- und Nordeuropa sowie für Gebiete wie Spanisch-Amerika, Indien und Japan nach vorne). und China unter europäischem Einfluss).
Die Annahme, dass die Werke des Aristoteles die Grundlage für das Verständnis der Philosophie bildeten, verlor auch in der Renaissance nicht an Bedeutung, als neue Übersetzungen, Kommentare und andere Interpretationen seiner Werke sowohl in lateinischer als auch in der Volkssprache aufblühten.
Letztere untersuchten, ähnlich wie moderne Debatten, die Vor- und Nachteile bestimmter philosophischer Positionen oder Interpretationen.
Platon, der im Mittelalter nur durch zweieinhalb Dialoge direkt bekannt war, wurde im Italien des 15. Jahrhunderts durch zahlreiche lateinische Übersetzungen bekannt, die ihren Höhepunkt in der äußerst einflussreichen Übersetzung seiner Gesamtwerke durch Marsilio Ficino in Florenz im Jahr 1484 fanden.
Nicht alle Renaissance-Humanisten folgten in allen Belangen seinem Beispiel, aber Petrarca trug zu einer Erweiterung des „Kanons“ seiner Zeit bei (heidnische Poesie galt früher als frivol und gefährlich), was auch in der Philosophie geschah.
Auch andere Bewegungen aus der antiken Philosophie fanden wieder Eingang in den Mainstream.
Diese Position geriet in der Renaissance zunehmend unter Druck, da verschiedene Denker behaupteten, Thomas‘ Klassifikationen seien ungenau und Ethik sei der wichtigste Teil der Moral.
Wie wir gesehen haben, glaubten sie, dass die Philosophie unter die Fittiche der Rhetorik gebracht werden könne.
In den Jahren 1416–1417 übersetzte Leonardo Bruni, der herausragende Humanist seiner Zeit und Kanzler von Florenz, die Ethik des Aristoteles in ein fließenderes, idiomatischeres und klassischeres Latein.
Die treibende Überzeugung war, dass die Philosophie von ihrem Fachjargon befreit werden sollte, damit mehr Menschen sie lesen könnten.
Desiderius Erasmus, der große niederländische Humanist, verfasste sogar eine griechische Ausgabe des Aristoteles, und schließlich mussten diejenigen, die an den Universitäten Philosophie lehrten, zumindest so tun, als könnten sie Griechisch.
Nachdem jedoch festgestellt worden war, dass Italienisch eine Sprache mit literarischem Wert war und das Gewicht der philosophischen Diskussion tragen konnte, gab es zahlreiche Bemühungen in dieser Richtung, insbesondere ab den 1540er Jahren.
Wir wissen, dass Debatten über die Freiheit des Willens immer wieder aufflammten (zum Beispiel im berühmten Austausch zwischen Erasmus und Martin Luther), dass spanische Denker zunehmend von der Vorstellung des Adels besessen waren und dass das Duell eine Praxis war, die großen Aufschwung hervorrief Literatur im 16. Jahrhundert (war das erlaubt oder nicht?).
Wir dürfen nicht vergessen, dass die meisten Philosophen dieser Zeit zumindest nominelle, wenn nicht sogar gläubige Christen waren, dass im 16. Jahrhundert sowohl die protestantische als auch die katholische Reformation stattfand und dass die Philosophie der Renaissance in der Zeit des Dreißigjährigen Krieges (1618) ihren Höhepunkt erreichte –1648).
Zusammenfassend lässt sich sagen, dass die Renaissance-Philosophie wie jeder andere Moment in der Geschichte des Denkens weder etwas völlig Neues hervorgebracht hat noch über Jahrhunderte hinweg die Schlussfolgerungen ihrer Vorgänger wiederholt hat.
Moderne Philosophie ist eine in der Neuzeit entwickelte und mit der Moderne verbundene Philosophie.
Im 17. und 18. Jahrhundert wurden die Hauptfiguren der Geistesphilosophie, der Erkenntnistheorie und der Metaphysik grob in zwei Hauptgruppen eingeteilt.
Im Gegensatz dazu waren die „Empiristen“ der Ansicht, dass Wissen mit Sinneserfahrungen beginnen muss.
Weitere wichtige Persönlichkeiten der politischen Philosophie sind Thomas Hobbes und Jean-Jacques Rousseau.
Kant löste im frühen 19. Jahrhundert in Deutschland einen Sturm philosophischer Arbeit aus, beginnend mit dem deutschen Idealismus.
Karl Marx eignete sich sowohl Hegels Geschichtsphilosophie als auch die in Großbritannien vorherrschende empirische Ethik an, überführte Hegels Ideen in eine streng materialistische Form und legte so den Grundstein für die Entwicklung einer Gesellschaftswissenschaft.
Arthur Schopenhauer kam aus dem Idealismus zu dem Schluss, dass die Welt nichts anderes sei als das vergebliche, endlose Zusammenspiel von Bildern und Wünschen, und befürwortete Atheismus und Pessimismus.
Descartes argumentierte, dass viele vorherrschende metaphysische Lehren der Scholastik bedeutungslos oder falsch seien.
Er versucht, so viel wie möglich von all seinen Überzeugungen beiseite zu legen, um herauszufinden, was, wenn überhaupt, er mit Sicherheit weiß.
Auf dieser Basis baut er sein Wissen wieder auf.
Während der Historismus auch die Rolle der Erfahrung anerkennt, unterscheidet er sich vom Empirismus durch die Annahme, dass sensorische Daten nicht verstanden werden können, ohne die historischen und kulturellen Umstände zu berücksichtigen, unter denen Beobachtungen gemacht werden.
Daher ist der Empirismus in erster Linie durch das Ideal gekennzeichnet, Beobachtungsdaten „für sich selbst sprechen“ zu lassen, während die konkurrierenden Ansichten diesem Ideal entgegenstehen.
Mit anderen Worten: Empirismus als Konzept muss zusammen mit anderen Konzepten konstruiert werden, die es zusammen ermöglichen, wichtige Unterscheidungen zwischen verschiedenen Idealen zu treffen, die der zeitgenössischen Wissenschaft zugrunde liegen.
Epistemologisch manifestiert sich Idealismus als Skepsis gegenüber der Möglichkeit, irgendein geistesunabhängiges Ding zu wissen.
Es beschreibt einen Prozess, bei dem Theorie aus der Praxis extrahiert und wieder auf die Praxis angewendet wird, um eine sogenannte intelligente Praxis zu bilden.
Brian Leiter (2006) Webseite „Analytische“ und „kontinentale“ Philosophie.
Als zeitgenössische Philosophie bezeichnet man die gegenwärtige Periode in der Geschichte der westlichen Philosophie, beginnend im frühen 20. Jahrhundert mit der zunehmenden Professionalisierung der Disziplin und dem Aufkommen der analytischen und kontinentalen Philosophie.
Deutschland war das erste Land, das die Philosophie professionalisierte.
Darüber hinaus richten sich Werke professioneller Philosophen im Gegensatz zu vielen Wissenschaften, für die es mittlerweile eine gesunde Industrie von Büchern, Zeitschriften und Fernsehsendungen gibt, die darauf abzielen, die Wissenschaft populär zu machen und die technischen Ergebnisse eines wissenschaftlichen Fachgebiets der breiten Öffentlichkeit zu vermitteln, an eine berufsfremdes Publikum bleibt rar.
Jede Abteilung organisiert eine große jährliche Konferenz.
Neben vielen anderen Aufgaben ist der Verband für die Verwaltung zahlreicher höchster Auszeichnungen des Berufsstandes verantwortlich.
Diese Entwicklung erfolgte ungefähr zeitgleich mit der Arbeit von Gottlob Frege und Bertrand Russell, die eine neue philosophische Methode einführten, die auf der Analyse der Sprache mittels moderner Logik beruhte (daher der Begriff „analytische Philosophie“).
Einige Philosophen wie Richard Rorty und Simon Glendinning argumentieren, dass diese „analytisch-kontinentale“ Kluft der Disziplin als Ganzes abträglich sei.
Danach unterscheiden sich analytische und kontinentale Philosophen hinsichtlich der Bedeutung und des Einflusses nachfolgender Philosophen auf ihre jeweiligen Traditionen.
Obwohl die analytische und die kontinentale Philosophie sehr unterschiedliche Ansichten über die Philosophie nach Kant haben, wird die kontinentale Philosophie oft auch in einem erweiterten Sinne verstanden und umfasst alle Philosophen oder Bewegungen nach Kant, die für die kontinentale Philosophie wichtig sind, jedoch nicht die analytische Philosophie.
Daher tendiert die kontinentale Philosophie zum Historismus, während die analytische Philosophie dazu neigt, Philosophie im Hinblick auf diskrete Probleme zu behandeln, die unabhängig von ihren historischen Ursprüngen analysiert werden können.
Die großen orthodoxen Schulen entstanden irgendwann zwischen dem Beginn der Neuzeit und dem Gupta-Reich.
Diese religiös-philosophischen Traditionen wurden später unter der Bezeichnung Hinduismus zusammengefasst.
Westliche Gelehrte betrachten den Hinduismus als eine Verschmelzung oder Synthese verschiedener indischer Kulturen und Traditionen mit unterschiedlichen Wurzeln und keinem einzigen Gründer.
Indische Philosophen entwickelten ein System des erkenntnistheoretischen Denkens (Pramana) und der Logik und untersuchten Themen wie Ontologie (Metaphysik, Brahman-Atman, Sunyata-Anatta), verlässliche Wissensmethoden (Erkenntnistheorie, Pramanas), Wertesystem (Axiologie) und andere Themen.
Spätere Entwicklungen umfassen die Entwicklung von Tantra und iranisch-islamischen Einflüssen.
Nyāya akzeptiert traditionell vier Pramanas als zuverlässige Mittel zur Wissensgewinnung: Pratyakṣa (Wahrnehmung), Anumāṇa (Schlussfolgerung), Upamāṇa (Vergleich und Analogie) und Śabda (Wort, Aussage früherer oder gegenwärtiger zuverlässiger Experten).
Diese Philosophie ging davon aus, dass das Universum auf Paramāṇu (Atome) reduzierbar sei, die unzerstörbar (anitya) und unteilbar seien und eine besondere Art von Dimension hätten, die „klein“ (aṇu) genannt werde.
Später fügten Vaiśeṣikas (Śrīdhara und Udayana und Śivāditya) eine weitere Kategorie Abhava (Nichtexistenz) hinzu.
Aufgrund ihres Schwerpunkts auf dem Studium und der Interpretation von Texten entwickelten Mīmāṃsā auch Theorien der Philologie und der Sprachphilosophie, die andere indische Schulen beeinflussten.
Zu den charakteristischen Merkmalen der Jain-Philosophie gehören ein Geist-Körper-Dualismus, die Leugnung eines kreativen und allmächtigen Gottes, Karma, ein ewiges und ungeschaffenes Universum, Gewaltlosigkeit, die Theorie der vielfältigen Facetten der Wahrheit und eine Moral, die auf der Befreiung der Seele basiert .
Es wurde auch als Modell des philosophischen Liberalismus bezeichnet, weil es darauf beharrt, dass die Wahrheit relativ und vielschichtig ist, und weil es bereit ist, alle möglichen Standpunkte der konkurrierenden Philosophien zu berücksichtigen.
Cārvāka-Philosophen wie Brihaspati standen anderen Philosophieschulen dieser Zeit äußerst kritisch gegenüber.
Es ist die vorherrschende philosophische Tradition in Tibet und südostasiatischen Ländern wie Sri Lanka und Burma.
Spätere buddhistische philosophische Traditionen entwickelten komplexe phänomenologische Psychologien, die als „Abhidharma“ bezeichnet werden.
Diese Tradition trug zu einer sogenannten „erkenntnistheoretischen Wende“ in der indischen Philosophie bei.
Wichtige Vertreter der buddhistischen Moderne sind Anagarika Dharmapala (1864–1933) und der amerikanische Konvertit Henry Steel Olcott, die chinesischen Modernisten Taixu (1890–1947) und Yin Shun (1906–2005), der Zen-Gelehrte D.T. Suzuki und der Tibeter Gendün Chöphel ( 1903–1951).
Anthropologie ist die wissenschaftliche Erforschung der Menschheit und befasst sich mit menschlichem Verhalten, menschlicher Biologie, Kulturen und Gesellschaften sowohl in der Gegenwart als auch in der Vergangenheit, einschließlich früherer menschlicher Spezies.
Die biologische oder physikalische Anthropologie untersucht die biologische Entwicklung des Menschen.
Es hatten sich bereits verschiedene kurzlebige Organisationen von Anthropologen gebildet.
Als 1848 in Frankreich die Sklaverei abgeschafft wurde, wurde die Société aufgegeben.
Für sie war die Veröffentlichung von Charles Darwins „Über die Entstehung der Arten“ die Offenbarung all dessen, was sie zu vermuten begannen.
Es gab sofort einen Ansturm darauf, es in die Sozialwissenschaften zu integrieren.
Seine Definition lautete nun „das Studium der menschlichen Gruppe, betrachtet als Ganzes, in ihren Einzelheiten und in Bezug auf den Rest der Natur“.
Er entdeckte das Sprachzentrum des menschlichen Gehirns, das heute nach ihm Broca-Areal genannt wird.
Die letzten beiden Bände wurden posthum veröffentlicht.
Er betont, dass die Vergleichsdaten empirisch sein und durch Experimente gesammelt werden müssen.
Waitz war einflussreich unter den britischen Ethnologen.
Vertreter der französischen Société waren anwesend, Broca jedoch nicht.
Zuvor hatte Edward sich selbst als Ethnologen bezeichnet; anschließend Anthropologe.
Eine bemerkenswerte Ausnahme war die Berliner Gesellschaft für Anthropologie, Ethnologie und Urgeschichte (1869), die von Rudolph Virchow gegründet wurde, der für seine schmähenden Angriffe auf die Evolutionisten bekannt war.
Die wichtigsten Theoretiker gehörten diesen Organisationen an.
Die praktische Anthropologie, der Einsatz anthropologischer Kenntnisse und Techniken zur Lösung spezifischer Probleme, ist angekommen; Beispielsweise könnte die Anwesenheit begrabener Opfer den Einsatz eines forensischen Archäologen zur Nachbildung der Schlussszene anregen.
Dies ist in den Vereinigten Staaten besonders deutlich zu erkennen, von Boas‘ Argumenten gegen die Rassenideologie des 19. Jahrhunderts über Margaret Meads Eintreten für Geschlechtergleichheit und sexuelle Befreiung bis hin zu aktueller Kritik an postkolonialer Unterdrückung und der Förderung des Multikulturalismus.
In Großbritannien und den Commonwealth-Ländern dominiert tendenziell die britische Tradition der Sozialanthropologie.
Kulturanthropologie ist die vergleichende Untersuchung der vielfältigen Arten, wie Menschen die Welt um sich herum verstehen, während Sozialanthropologie die Untersuchung der Beziehungen zwischen Einzelpersonen und Gruppen ist.
Es gibt keine eindeutige Unterscheidung zwischen ihnen und diese Kategorien überschneiden sich in erheblichem Maße.
Dieses Projekt wird häufig im Bereich der Ethnographie angesiedelt.
Die teilnehmende Beobachtung ist eine der grundlegenden Methoden der Sozial- und Kulturanthropologie.
Die Erforschung von Verwandtschaft und sozialer Organisation ist ein zentraler Schwerpunkt der soziokulturellen Anthropologie, da Verwandtschaft ein universelles menschliches Wesen ist.
Für die Ethnographie sind Erfahrungen aus erster Hand und der soziale Kontext wichtig.
Ethnomusikologie kann in den unterschiedlichsten Bereichen eingesetzt werden, etwa in der Lehre, in der Politik, in der Kulturanthropologie usw.
Die Wirtschaftsanthropologie konzentriert sich weiterhin größtenteils auf den Austausch.
Der erste dieser Bereiche befasste sich mit den „vorkapitalistischen“ Gesellschaften, die evolutionären „Stammes“-Stereotypen unterworfen waren.
Warum sind diejenigen, die in der Entwicklung arbeiten, so bereitwillig, die Geschichte und die Lehren, die sie daraus ziehen könnte, zu ignorieren?
Innerhalb der Verwandtschaft gibt es zwei verschiedene Familien.
Anthropologie beschäftigt sich oft mit Feministinnen aus nicht-westlichen Traditionen, deren Perspektiven und Erfahrungen sich von denen weißer Feministinnen in Europa, Amerika und anderswo unterscheiden können.
Die politische Anthropologie entwickelte sich zu einer Disziplin, die sich hauptsächlich mit der Politik in staatenlosen Gesellschaften befasste. Eine neue Entwicklung begann in den 1960er Jahren und ist noch immer im Gange: Anthropologen begannen zunehmend, „komplexere“ soziale Zusammenhänge zu untersuchen, in denen die Präsenz von Staaten, Bürokratien und Märkten Einzug hielt Ethnografische Berichte und Analyse lokaler Phänomene.
Zweitens entwickelten Anthropologen langsam eine disziplinäre Beschäftigung mit Staaten und ihren Institutionen (und mit der Beziehung zwischen formellen und informellen politischen Institutionen).
Manchmal wird es der soziokulturellen Anthropologie zugeordnet und manchmal als Teil der materiellen Kultur betrachtet.
Es ist auch das Studium der Geschichte verschiedener ethnischer Gruppen, die heute möglicherweise existieren oder nicht.
Verschiedene gesellschaftliche Prozesse sowohl in der westlichen Welt als auch in der „Dritten Welt“ (Letztere steht gewohnheitsmäßig im Mittelpunkt der Aufmerksamkeit von Anthropologen) rückten die Aufmerksamkeit von „Spezialisten für „andere Kulturen““ näher an ihre Heimat heran.
Es handelt sich um ein interdisziplinäres Fachgebiet, das sich mit einer Reihe anderer Disziplinen überschneidet, darunter Anthropologie, Ethologie, Medizin, Psychologie, Veterinärmedizin und Zoologie.
Es handelt sich um die Erforschung der Urmenschen, wie sie in fossilen Hominidenfunden wie versteinerten Knochen und Fußabdrücken zu finden sind.
Im Jahr 1989 gründete eine Gruppe europäischer und amerikanischer Wissenschaftler auf dem Gebiet der Anthropologie die European Association of Social Anthropologists (EASA), die als wichtige Berufsorganisation für in Europa tätige Anthropologen dient.
Dies ist die Vorstellung, dass Kulturen nicht anhand der Werte oder Standpunkte anderer beurteilt werden sollten, sondern leidenschaftslos nach ihren eigenen Gesichtspunkten untersucht werden sollten.
Franz Boas protestierte öffentlich gegen die Beteiligung der USA am Ersten Weltkrieg und veröffentlichte nach dem Krieg eine kurze Enthüllung und Verurteilung der Beteiligung mehrerer amerikanischer Archäologen an der Spionage in Mexiko unter ihrem Deckmantel als Wissenschaftler.
Gleichzeitig liefert die Arbeit von David H. Price über die amerikanische Anthropologie während des Kalten Krieges detaillierte Berichte über die Verfolgung und Entlassung mehrerer Anthropologen aus kommunistischen Sympathien.
Auf den Jahrestagungen der American Anthropological Association (AAA) wurden zahlreiche Resolutionen verabschiedet, die den Krieg in all seinen Aspekten verurteilten.
Die Association of Social Anthropologists of the UK and Commonwealth (ASA) hat bestimmte Wissenschaften als ethisch gefährlich bezeichnet.
Eines der zentralen Merkmale ist, dass die Anthropologie tendenziell eine vergleichsweise ganzheitlichere Darstellung von Phänomenen liefert und tendenziell stark empirisch ist.
Diese dynamischen Beziehungen zwischen dem, was vor Ort beobachtet werden kann, und dem, was durch die Zusammenstellung vieler lokaler Beobachtungen beobachtet werden kann, bleiben für jede Art von Anthropologie, ob kulturell, biologisch, sprachlich oder archäologisch, von grundlegender Bedeutung.
Auf der biologischen oder physikalischen Seite können menschliche Messungen, genetische Proben und Ernährungsdaten gesammelt und als Artikel oder Monographien veröffentlicht werden.
Weitere kulturelle Unterteilungen nach Werkzeugtypen wie Olduwan, Mousterian oder Levalloisian helfen Archäologen und anderen Anthropologen dabei, wichtige Trends in der menschlichen Vergangenheit zu verstehen.
Eine kulturelle Norm kodifiziert akzeptables Verhalten in der Gesellschaft; Es dient als Leitfaden für Verhalten, Kleidung, Sprache und Auftreten in einer Situation und dient als Vorlage für Erwartungen in einer sozialen Gruppe.
Dazu gehören Ausdrucksformen wie Kunst, Musik, Tanz, Rituale, Religion und Technologien wie Werkzeuggebrauch, Kochen, Unterkunft und Kleidung.
Der Grad der kulturellen Verfeinerung wurde manchmal auch verwendet, um Zivilisationen von weniger komplexen Gesellschaften zu unterscheiden.
Unter Massenkultur versteht man die massenproduzierten und massenvermittelten Formen der Konsumkultur, die im 20. Jahrhundert entstanden sind.
In den breiteren Sozialwissenschaften geht die theoretische Perspektive des Kulturmaterialismus davon aus, dass die menschliche symbolische Kultur aus den materiellen Bedingungen des menschlichen Lebens entsteht, da der Mensch die Bedingungen für das physische Überleben schafft, und dass die Grundlage der Kultur in entwickelten biologischen Dispositionen liegt.
In diesem Sinne schätzt der Multikulturalismus das friedliche Zusammenleben und den gegenseitigen Respekt zwischen verschiedenen Kulturen, die auf demselben Planeten leben.
Im Jahr 1986 schrieb der Philosoph Edward S. Casey: „Das Wort Kultur selbst bedeutete im Mittelenglischen ‚Ort bebaut‘, und das gleiche Wort geht auf das Lateinische colere, ‚bewohnen, pflegen, bestellen, anbeten‘ und cultus, ‚A.‘ zurück Kult, insbesondere ein religiöser.'
Daher wird bei diesen Autoren meist ein Kontrast zwischen „Kultur“ und „Zivilisation“ impliziert, auch wenn dieser nicht so ausgedrückt wird.
Diese Fähigkeit entstand mit der Entwicklung der Verhaltensmodernität beim Menschen vor etwa 50.000 Jahren und wird oft als einzigartig für den Menschen angesehen.
Rein Raud hat, aufbauend auf der Arbeit von Umberto Eco, Pierre Bourdieu und Jeffrey C. Alexander, ein Modell des kulturellen Wandels vorgeschlagen, das auf Ansprüchen und Angeboten basiert, die anhand ihrer kognitiven Angemessenheit beurteilt und durch die symbolische Autorität des Unternehmens bestätigt oder nicht bestätigt werden betreffende Kulturgemeinschaft.
Kulturelle Neupositionierung bedeutet die Rekonstruktion des kulturellen Konzepts einer Gesellschaft.
Soziale Konflikte und die Entwicklung von Technologien können zu Veränderungen innerhalb einer Gesellschaft führen, indem sie soziale Dynamiken verändern, neue kulturelle Modelle fördern und generatives Handeln anregen oder ermöglichen.
Als Faktoren können auch Umweltbedingungen eine Rolle spielen.
Krieg oder Konkurrenz um Ressourcen können sich auf die technologische Entwicklung oder die soziale Dynamik auswirken.
Westliche Restaurantketten und kulinarische Marken erweckten beispielsweise bei den Chinesen Neugier und Faszination, als China Ende des 20. Jahrhunderts seine Wirtschaft für den internationalen Handel öffnete.
Er argumentierte, dass diese Unreife nicht auf mangelndes Verständnis, sondern auf mangelnden Mut zum unabhängigen Denken zurückzuführen sei.
Darüber hinaus schlug Herder eine kollektive Form der Bildung vor: „Für Herder war Bildung die Gesamtheit der Erfahrungen, die einem Volk eine kohärente Identität und ein Gefühl für das gemeinsame Schicksal verleihen.“
Nach dieser Denkrichtung hat jede ethnische Gruppe eine eigene Weltanschauung, die mit der Weltanschauung anderer Gruppen nicht vergleichbar ist.
Er schlug vor, dass ein wissenschaftlicher Vergleich aller menschlichen Gesellschaften zeigen würde, dass unterschiedliche Weltanschauungen aus denselben Grundelementen bestehen.
eine bestimmte Lebensweise, sei es eines Volkes, einer Epoche oder einer Gruppe.
Mit anderen Worten: Die Idee der „Kultur“, die sich im 18. und frühen 19. Jahrhundert in Europa entwickelte, spiegelte Ungleichheiten innerhalb der europäischen Gesellschaften wider.
Nach dieser Denkweise könnte man einige Länder und Nationen als zivilisierter als andere und einige Menschen als kultivierter als andere einstufen.
Andere Kritiker des 19. Jahrhunderts, die Rousseau folgten, akzeptierten diese Unterscheidung zwischen höherer und niedrigerer Kultur, betrachteten die Verfeinerung und Verfeinerung der Hochkultur jedoch als korrumpierende und unnatürliche Entwicklungen, die die wesentliche Natur der Menschen verschleiern und verzerren.
Im Jahr 1870 wandte der Anthropologe Edward Tylor (1832–1917) diese Vorstellungen von höherer und niedrigerer Kultur an, um eine Theorie zur Entwicklung der Religion vorzuschlagen.
Für den Soziologen Georg Simmel (1858–1918) bezeichnete Kultur „die Kultivierung von Individuen durch äußere, im Laufe der Geschichte objektivierte Formen“.
Immaterielle Kultur bezieht sich auf die nicht-physischen Vorstellungen, die Individuen über ihre Kultur haben, einschließlich Werte, Glaubenssysteme, Regeln, Normen, Moral, Sprache, Organisationen und Institutionen, während materielle Kultur der physische Beweis einer Kultur in den Objekten ist und Architektur, die sie machen oder gemacht haben.
Die Kultursoziologie wurde dann im englischsprachigen Raum als Produkt des „Cultural Turn“ der 1960er Jahre „neu erfunden“, der strukturalistische und postmoderne Ansätze in der Sozialwissenschaft einleitete.
„Kultur“ ist seitdem in vielen Bereichen der Soziologie zu einem wichtigen Konzept geworden, einschließlich eindeutig wissenschaftlicher Bereiche wie der sozialen Schichtung und der Analyse sozialer Netzwerke.
Sie sahen Konsum- und Freizeitmuster als durch Produktionsverhältnisse bestimmt, was dazu führte, dass sie sich auf Klassenverhältnisse und die Organisation der Produktion konzentrierten.
Seitdem ist es eng mit Stuart Hall verbunden, der die Nachfolge von Hoggart als Direktor antrat.
Diese Praktiken umfassen die Art und Weise, wie Menschen in einer bestimmten Kultur bestimmte Dinge tun (z. B. fernsehen oder auswärts essen).
Fernsehen, um eine öffentliche Perspektive auf ein historisches Ereignis zu sehen, sollte nicht als Kultur betrachtet werden, es sei denn, es bezieht sich auf das Medium Fernsehen selbst, das möglicherweise kulturell ausgewählt wurde; Schulkinder, die nach der Schule mit ihren Freunden fernsehen, um sich anzupassen, sind jedoch durchaus geeignet, da es keinen begründeten Grund für die Teilnahme an dieser Praxis gibt.
„Kultur“ umfasst für einen Kulturwissenschaftler nicht nur die traditionelle Hochkultur (die Kultur herrschender sozialer Gruppen) und die Populärkultur, sondern auch alltägliche Bedeutungen und Praktiken.
Wissenschaftler im Vereinigten Königreich und in den Vereinigten Staaten entwickelten nach den späten 1970er Jahren etwas unterschiedliche Versionen der Kulturwissenschaften.
Die Unterscheidung zwischen amerikanischen und britischen Strömungen ist jedoch verblasst.
Das Hauptaugenmerk eines orthodoxen marxistischen Ansatzes liegt auf der Erzeugung von Bedeutung.
Andere kulturwissenschaftliche Ansätze, etwa die feministischen Kulturwissenschaften und spätere amerikanische Entwicklungen des Fachgebiets, distanzieren sich von dieser Sichtweise.
Kulturpsychologen begannen, die Beziehung zwischen Emotionen und Kultur zu erforschen und zu beantworten, ob der menschliche Geist unabhängig von der Kultur ist.
Andererseits versuchen einige Forscher, nach Unterschieden zwischen den Persönlichkeiten von Menschen in verschiedenen Kulturen zu suchen.
Beispielsweise wird Menschen, die in einer Kultur mit Abakus aufgewachsen sind, ein ausgeprägter Denkstil beigebracht.
Im Wesentlichen befassen sich die Haager Konvention zum Schutz von Kulturgut bei bewaffneten Konflikten und das UNESCO-Übereinkommen zum Schutz der kulturellen Vielfalt mit dem Schutz der Kultur.
Nach internationalem Recht versuchen die UN und die UNESCO, hierfür Regeln aufzustellen und durchzusetzen.
Ziel des Angriffs ist die Identität des Gegners, weshalb symbolische Kulturgüter zum Hauptziel werden.
Ein Fest ist ein Ereignis, das normalerweise von einer Gemeinschaft gefeiert wird und sich auf einen charakteristischen Aspekt dieser Gemeinschaft und ihrer Religion oder Kultur konzentriert.
Neben Religion und Folklore ist die Landwirtschaft ein wichtiger Ursprung.
Feste dienen oft der Erfüllung bestimmter gemeinschaftlicher Zwecke, insbesondere des Gedenkens oder des Dankes an Götter, Göttinnen oder Heilige: Sie werden Patronatsfeste genannt.
Im antiken Griechenland und Rom waren Feste wie die Saturnalien eng mit der sozialen Organisation und politischen Prozessen sowie der Religion verbunden.
Im Mittelenglischen war ein „Festival Dai“ ein religiöser Feiertag.
Der Begriff „Fest“ wird im weltlichen Sprachgebrauch auch als Synonym für jede große oder aufwändige Mahlzeit verwendet.
Die wichtigsten religiösen Feste wie Weihnachten, Rosch Haschana, Diwali, Eid al-Fitr und Eid al-Adha markieren das Jahr.
Ein frühes Beispiel ist das Fest, das der altägyptische Pharao Ramses III. zur Feier seines Sieges über die Libyer ins Leben rief.
Es gibt zahlreiche Arten von Festen auf der Welt und die meisten Länder feiern wichtige Ereignisse oder Traditionen mit traditionellen kulturellen Veranstaltungen und Aktivitäten.
Feste im alten Ägypten konnten entweder religiöser oder politischer Natur sein.
Das Sed-Fest zum Beispiel feierte das dreißigste Jahr der Herrschaft eines ägyptischen Pharaos und danach alle drei (oder in einem Fall vier) Jahre.
Im christlichen liturgischen Kalender gibt es zwei Hauptfeste, die eigentlich als das Fest der Geburt unseres Herrn (Weihnachten) und das Fest der Auferstehung (Ostern) bekannt sind, aber in fast allen werden kleinere Feste zu Ehren der örtlichen Schutzheiligen gefeiert Länder, die vom Christentum beeinflusst sind.
In Sri Lanka und Thailand finden buddhistische religiöse Feste wie Esala Perahera statt.
Filmfestivals beinhalten die Vorführung verschiedener Filme und finden in der Regel jährlich statt.
Es gibt auch spezielle Getränkefeste, wie zum Beispiel das berühmte Oktoberfest in Deutschland für Bier.
Die alten Ägypter verließen sich auf die saisonale Überschwemmung durch den Nil, eine Form der Bewässerung, die fruchtbares Land für den Anbau lieferte.
Das Dree-Fest der Apatanis, die im Distrikt Lower Subansiri in Arunachal Pradesh leben, wird jedes Jahr vom 4. bis 7. Juli gefeiert, indem für eine Rekordernte gebetet wird.
Ein Feiertag ist ein durch Brauch oder Gesetz festgelegter Tag, an dem normale Aktivitäten, insbesondere geschäftliche oder berufliche Aktivitäten einschließlich der Schule, ausgesetzt oder eingeschränkt sind.
Das Ausmaß, in dem normale Aktivitäten durch einen Feiertag eingeschränkt werden, kann von den örtlichen Gesetzen, Gepflogenheiten, der Art der ausgeübten Tätigkeit oder persönlichen Entscheidungen abhängen.
In den meisten modernen Gesellschaften dienen Feiertage jedoch ebenso einer Erholungsfunktion wie alle anderen Wochenendtage oder Aktivitäten.
In manchen Fällen kann ein Feiertag nur nominell eingehalten werden.
Die moderne Nutzung variiert geografisch.
Beispielsweise wird der Monkey Day am 14. Dezember gefeiert, der International Talk Like a Pirate Day am 19. September und der Blasphemy Day am 30. September.
Jehovas Zeugen begehen jährlich das „Denkmal an den Tod Jesu Christi“, feiern jedoch keine anderen Feiertage mit religiöser Bedeutung wie Ostern, Weihnachten oder Neujahr.
Ahmadi-Muslime feiern außerdem den Tag des versprochenen Messias, den Tag des versprochenen Reformators und den Tag des Kalifats, aber entgegen der landläufigen Meinung gelten beide nicht als Feiertage.
Keltische, nordische und neuheidnische Feiertage folgen der Reihenfolge des Jahresrads.
Forscher in der Bioarchäologie kombinieren die Kompetenzen der menschlichen Osteologie, Paläopathologie und Archäologie und berücksichtigen häufig den kulturellen und bestattenden Kontext der Überreste.
Evolutionspsychologie ist die Untersuchung psychologischer Strukturen aus einer modernen evolutionären Perspektive.
Unter menschlicher Verhaltensökologie versteht man die Untersuchung von Verhaltensanpassungen (Nahrungssuche, Fortpflanzung, Ontogenese) aus evolutionärer und ökologischer Sicht (siehe Verhaltensökologie).
Paläoanthropologie ist die Untersuchung fossiler Beweise für die menschliche Evolution. Dabei werden hauptsächlich Überreste ausgestorbener Homininen und anderer Primatenarten verwendet, um die morphologischen und Verhaltensänderungen in der menschlichen Abstammungslinie sowie in der Umgebung, in der die menschliche Evolution stattfand, zu bestimmen.
Der Name ist sogar relativ neu, da er seit über einem Jahrhundert für „physische Anthropologie“ steht und einige Praktiker diesen Begriff immer noch verwenden.
Einige Herausgeber (siehe unten) haben das Fachgebiet noch tiefer verwurzelt als die formale Wissenschaft.
Dies wurde für die nächsten etwa 2.000 Jahre zum Hauptsystem, mit dem Gelehrte über die Natur nachdachten.
Er schrieb auch über Physiognomie, eine Idee, die aus Schriften im Hippokratischen Korpus stammt.
Im 19. Jahrhundert konzentrierten sich französische physikalische Anthropologen unter der Leitung von Paul Broca (1824–1880) auf die Kraniometrie, während die deutsche Tradition unter der Leitung von Rudolf Virchow (1821–1902) den Einfluss von Umwelt und Krankheiten auf den menschlichen Körper betonte.
Er verlagerte den Schwerpunkt von der Rassentypologie auf das Studium der menschlichen Evolution und bewegte sich weg von der Klassifizierung hin zum Evolutionsprozess.
Eine Rasse ist eine Gruppierung von Menschen auf der Grundlage gemeinsamer körperlicher oder sozialer Eigenschaften in Kategorien, die von der Gesellschaft allgemein als unterschiedlich angesehen werden.
Die moderne Wissenschaft betrachtet Rasse als ein soziales Konstrukt, eine Identität, die auf der Grundlage gesellschaftlicher Regeln vergeben wird.
Wieder andere argumentieren, dass die Rasse beim Menschen keine taxonomische Bedeutung habe, da alle lebenden Menschen derselben Unterart angehören, dem Homo sapiens sapiens.
In Südafrika erkannte das Bevölkerungsregistrierungsgesetz von 1950 nur Weiße, Schwarze und Farbige an, später kamen auch Inder hinzu.
Das United States Census Bureau schlug Pläne zur Aufnahme einer neuen Kategorie zur Klassifizierung von Völkern des Nahen Ostens und Nordafrikas in der US-Volkszählung 2020 vor, zog sie dann aber zurück, da es Streit darüber gab, ob diese Klassifizierung als weiße ethnische Zugehörigkeit oder als separate Rasse betrachtet werden sollte.
Die Festlegung von Rassengrenzen geht oft mit der Unterwerfung von Gruppen einher, die als rassisch minderwertig definiert werden, wie in der One-Drop-Regel, die in den Vereinigten Staaten des 19 ".
Laut dem Genetiker David Reich „mag Rasse ein soziales Konstrukt sein, doch Unterschiede in der genetischen Abstammung, die zufällig mit vielen der heutigen Rassenkonstrukte korrelieren, sind real.“
Andere Dimensionen rassischer Gruppierungen umfassen gemeinsame Geschichte, Traditionen und Sprache.
Sozioökonomische Faktoren haben in Kombination mit frühen, aber anhaltenden Ansichten über Rasse zu erheblichem Leid innerhalb benachteiligter Rassengruppen geführt.
Rassismus hat zu vielen Tragödien geführt, darunter Sklaverei und Völkermord.
Da in manchen Gesellschaften die Rassengruppierung eng mit den Mustern der sozialen Schichtung übereinstimmt, kann die Rasse für Sozialwissenschaftler, die sich mit sozialer Ungleichheit befassen, eine wichtige Variable sein.
Beispielsweise plädierte John Hartigan Jr. im Jahr 2008 für eine Sichtweise der Rasse, die sich in erster Linie auf die Kultur konzentrierte, aber die potenzielle Relevanz von Biologie oder Genetik nicht außer Acht ließ.
Auf diese Weise entstand die Idee der Rasse, wie wir sie heute verstehen, während des historischen Prozesses der Erkundung und Eroberung, der die Europäer mit Gruppen aus verschiedenen Kontinenten in Kontakt brachte, und der Ideologie der Klassifizierung und Typologie in den Naturwissenschaften.
Eine Reihe von Volksglauben setzte sich durch, die ererbte physische Unterschiede zwischen Gruppen mit vererbten intellektuellen, Verhaltens- und moralischen Qualitäten in Verbindung brachten.
Die Klassifikation von Carl Linnaeus, Erfinder der zoologischen Taxonomie, aus dem Jahr 1735 teilte die menschliche Spezies Homo sapiens in die kontinentalen Varianten Europaeus, Asiaticus, Americanus und Afer ein, die jeweils mit einem anderen Humor verbunden sind: Sanguiniker, Melancholiker, Choleriker und Phlegmatiker.
Blumenbach bemerkte auch den abgestuften Übergang der Erscheinungen von einer Gruppe zu benachbarten Gruppen und schlug vor, dass „eine Vielfalt der Menschheit so sinnvoll in die andere übergeht, dass man die Grenzen zwischen ihnen nicht erkennen kann“.
Darüber hinaus wurde argumentiert, dass einige Gruppen das Ergebnis einer Vermischung ehemals unterschiedlicher Populationen sein könnten, dass jedoch sorgfältige Untersuchungen die angestammten Rassen unterscheiden könnten, die sich zu gemischten Gruppen zusammengeschlossen hatten.
Neue Kulturstudien und das junge Gebiet der Populationsgenetik untergruben den wissenschaftlichen Stellenwert des Rassenessenzialismus und führten dazu, dass Rassenanthropologen ihre Schlussfolgerungen über die Quellen phänotypischer Variation revidierten.
Studien zur menschlichen genetischen Variation zeigen, dass menschliche Populationen nicht geografisch isoliert sind und ihre genetischen Unterschiede weitaus geringer sind als die zwischen vergleichbaren Unterarten.
Andreasen zitierte von Luigi Cavalli-Sforza veröffentlichte Baumdiagramme relativer genetischer Abstände zwischen Populationen als Grundlage für einen phylogenetischen Stammbaum menschlicher Rassen (S. 661).
Marks, Templeton und Cavalli-Sforza kommen alle zu dem Schluss, dass die Genetik keinen Beweis für menschliche Rassen liefert.
In Bezug auf die Hautfarbe in Europa und Afrika schreibt Brace beispielsweise: „Bis heute verändert sich die Hautfarbe auf unmerkliche Weise von Europa südwärts um das östliche Ende des Mittelmeers herum und den Nil hinauf nach Afrika.“
Er argumentierte weiter, dass man den Begriff Rasse verwenden könne, wenn man zwischen „Rassenunterschieden“ und „dem Rassenkonzept“ unterscheiden würde.
Kurz gesagt, Livingstone und Dobzhansky stimmen darin überein, dass es genetische Unterschiede zwischen Menschen gibt; Sie stimmen auch darin überein, dass die Verwendung des Rassenkonzepts zur Klassifizierung von Menschen und die Art und Weise, wie das Rassenkonzept verwendet wird, eine Frage gesellschaftlicher Konventionen ist.
Wie die Anthropologen Leonard Lieberman und Fatimah Linda Jackson feststellten: „Disharmonische Heterogenitätsmuster verfälschen jede Beschreibung einer Population, als ob sie genotypisch oder sogar phänotypisch homogen wäre.“
Der Anthropologe William C. Boyd definierte Mitte des 20. Jahrhunderts Rasse als: „Eine Population, die sich hinsichtlich der Häufigkeit eines oder mehrerer der Gene, die sie besitzt, erheblich von anderen Populationen unterscheidet.“
Darüber hinaus hat der Anthropologe Stephen Molnar vorgeschlagen, dass die Diskrepanz zwischen den Abstammungslinien unweigerlich zu einer Vervielfachung der Rassen führt, die das Konzept selbst nutzlos macht.
Joanna Mountain und Neil Risch warnten davor, dass genetische Cluster zwar eines Tages möglicherweise mit phänotypischen Variationen zwischen Gruppen korrespondieren, solche Annahmen jedoch verfrüht seien, da die Beziehung zwischen Genen und komplexen Merkmalen noch immer kaum verstanden sei.
Jede Kategorie, die Sie sich ausdenken, wird unvollkommen sein, aber das hindert Sie nicht daran, sie zu verwenden oder dass sie nützlich ist.
Dies ging von drei Bevölkerungsgruppen aus, die durch große geografische Gebiete getrennt waren (europäisch, afrikanisch und ostasiatisch).
Anthropologen wie C. Loring Brace, die Philosophen Jonathan Kaplan und Rasmus Winther sowie der Genetiker Joseph Graves haben argumentiert, dass es zwar durchaus möglich ist, biologische und genetische Variationen zu finden, die in etwa den Gruppierungen entsprechen, die normalerweise als „kontinentale Rassen“ definiert werden. Dies gilt für fast alle geografisch unterschiedlichen Populationen.
Weiss und Fullerton haben festgestellt, dass sich, wenn man nur Isländer, Mayas und Maoris beproben würde, drei verschiedene Cluster bilden würden und alle anderen Populationen klinisch als aus Mischungen genetischer Materialien der Maori, Isländer und Mayas zusammengesetzt beschrieben werden könnten.
Darüber hinaus bestimmen die Genomdaten nicht, ob man Unterteilungen (d. h. Splitter) oder ein Kontinuum (d. h. Klumpen) sehen möchte.
Neben empirischen und konzeptionellen Problemen mit „Rasse“ waren sich Evolutions- und Sozialwissenschaftler nach dem Zweiten Weltkrieg sehr bewusst, wie Rassenvorstellungen zur Rechtfertigung von Diskriminierung, Apartheid, Sklaverei und Völkermord genutzt wurden.
Craig Venter und Francis Collins vom National Institute of Health kündigten im Jahr 2000 gemeinsam die Kartierung des menschlichen Genoms an.
Es ist keine wissenschaftliche.
Der Anthropologe Stephan Palmié hat argumentiert, dass Rasse „keine Sache, sondern eine soziale Beziehung“ sei; oder, in den Worten von Katya Gibel Mevorach, „ein Metonym“, „eine menschliche Erfindung, deren Unterscheidungskriterien weder universell noch festgelegt sind, sondern immer zur Bewältigung von Unterschieden verwendet wurden.“
Dort wurde die Rassenidentität nicht wie in den Vereinigten Staaten durch eine strenge Abstammungsregel wie die One-Drop-Regel geregelt.
Diese Typen gehen wie die Farben des Spektrums ineinander über, und keine Kategorie ist wesentlich vom Rest isoliert.
New Jersey: Prentice Hall Inc, 1984.
Im europäischen Kontext unterstreicht die historische Resonanz des Begriffs „Rasse“ seinen problematischen Charakter.
Das Konzept der rassischen Herkunft beruht auf der Vorstellung, dass Menschen in biologisch unterschiedliche „Rassen“ unterteilt werden können, eine Idee, die von der wissenschaftlichen Gemeinschaft allgemein abgelehnt wird.
In den Vereinigten Staaten haben die meisten Menschen, die sich selbst als Afroamerikaner identifizieren, europäische Vorfahren, während viele Menschen, die sich als Europäer identifizieren, afrikanische oder indianische Vorfahren haben.
Die Kriterien für die Mitgliedschaft in diesen Rassen unterschieden sich im späten 19. Jahrhundert.
Indianer werden weiterhin durch einen bestimmten Prozentsatz an „Indianerblut“ (Blutquantum genannt) definiert.
Diese Regel bedeutete, dass diejenigen, die gemischter Abstammung waren, aber eine erkennbare afrikanische Abstammung aufwiesen, als schwarz definiert wurden.
Der Begriff „Hispanoamerikaner“ als Ethnonym entstand im 20. Jahrhundert mit der zunehmenden Migration von Arbeitskräften aus den spanischsprachigen Ländern Lateinamerikas in die Vereinigten Staaten.
Es wurde festgestellt, dass drei Faktoren, das Land der akademischen Ausbildung, die Disziplin und das Alter, für die Differenzierung der Antworten von Bedeutung waren.
Im Jahr 2007 interviewte Ann Morning über 40 amerikanische Biologen und Anthropologen und stellte erhebliche Meinungsverschiedenheiten über die Natur der Rasse fest, wobei in keiner der beiden Gruppen ein Standpunkt die Mehrheit vertrat.
Obwohl er gute Argumente für beide Seiten sieht, scheint die völlige Ablehnung der gegensätzlichen Beweise „weitgehend auf gesellschaftspolitischer Motivation und überhaupt nicht auf wissenschaftlichen Erkenntnissen zu beruhen“.
In einer teilweisen Antwort auf Gills Aussage argumentiert C. Loring Brace, Professor für biologische Anthropologie, dass der Grund, warum Laien und biologische Anthropologen die geografische Abstammung eines Individuums bestimmen können, durch die Tatsache erklärt werden kann, dass biologische Merkmale klinisch über den Planeten verteilt sind, und das ist auch der Fall nicht in das Konzept der Rasse übersetzen.
In Texten zur physikalischen Anthropologie wurde argumentiert, dass biologische Rassen bis in die 1970er Jahre existierten, als sie begannen zu argumentieren, dass es keine Rassen gäbe.
Im Februar 2001 forderten die Herausgeber von Archives of Pediatrics and Adolescent Medicine „die Autoren auf, Rasse und ethnische Zugehörigkeit nicht zu verwenden, wenn dafür kein biologischer, wissenschaftlicher oder soziologischer Grund besteht“.
Morning (2008) untersuchte Biologielehrbücher an weiterführenden Schulen aus der Zeit von 1952 bis 2002 und stellte zunächst ein ähnliches Muster fest: Im Zeitraum von 1983 bis 1992 diskutierten nur 35 % direkt über Rasse, während anfangs 92 % dies taten.
Im Allgemeinen hat sich das Material zur Rasse von oberflächlichen Merkmalen hin zu Genetik und Evolutionsgeschichte verlagert.
Sie stellt fest: „Bestenfalls kann man zu dem Schluss kommen, dass Biologen und Anthropologen in ihren Ansichten über die Natur der Rasse mittlerweile gleichermaßen geteilter Meinung zu sein scheinen.“
In einer Studie aus dem Jahr 2008 wurden 33 Gesundheitsforscher aus verschiedenen geografischen Regionen befragt.
Viele Soziologen konzentrierten sich auf Afroamerikaner, die damals Neger genannt wurden, und behaupteten, sie seien den Weißen unterlegen.
Seine Lösung basierte größtenteils auf der Arbeit von Al-Khwarizmi.
Allerdings beginnt die quadratische Formel aufgrund von Rundungsfehlern irgendwann an Genauigkeit zu verlieren, während die Näherungsmethode immer besser wird.
Es gab Methoden zur numerischen Approximation, Prosthaphaerese genannt, die Abkürzungen für zeitaufwändige Operationen wie Multiplikation und Potenz- und Wurzelbildung boten.
Rechenalgorithmen zum Finden von Lösungen sind ein wichtiger Bestandteil der numerischen linearen Algebra und spielen eine herausragende Rolle in den Ingenieurwissenschaften, der Physik, der Chemie, der Informatik und den Wirtschaftswissenschaften.
Für Lösungen in einem Integralbereich wie dem Ring der ganzen Zahlen oder in anderen algebraischen Strukturen wurden andere Theorien entwickelt, siehe Lineare Gleichung über einem Ring.
Dadurch kann die gesamte Sprache und Theorie von Vektorräumen (oder allgemeiner von Modulen) zum Tragen kommen.
Ein solches System wird als unterbestimmtes System bezeichnet.
Das zweite System hat eine einzige eindeutige Lösung, nämlich den Schnittpunkt der beiden Geraden.
Zwei beliebige dieser Gleichungen haben eine gemeinsame Lösung.
Ein Gleichungssystem, dessen linke Seiten linear unabhängig sind, ist immer konsistent.
Dies ergibt ein Gleichungssystem mit einer Gleichung weniger und einer Unbekannten weniger.
Typ 3: Addiere zu einer Zeile ein skalares Vielfaches einer anderen.
Beispielsweise können Systeme mit einer symmetrischen positiv definiten Matrix mit der Cholesky-Zerlegung doppelt so schnell gelöst werden.
Bei sehr großen Systemen, die sonst zu viel Zeit oder Speicher beanspruchen würden, geht man oft ganz anders vor.
Dies führt zur Klasse der iterativen Methoden.
In der Mathematik ist eine Reihe grob gesagt eine Beschreibung des Vorgangs, bei dem unendlich viele Größen nacheinander zu einer gegebenen Ausgangsgröße addiert werden.
Zusätzlich zu ihrer Allgegenwärtigkeit in der Mathematik werden Unendlichkeitsreihen auch in anderen quantitativen Disziplinen wie der Physik, der Informatik, der Statistik und der Finanzwissenschaft häufig verwendet.
Zenos Paradoxon von Achilles und der Schildkröte veranschaulicht diese kontraintuitive Eigenschaft unendlicher Summen: Achilles rennt einer Schildkröte nach, aber als er zu Beginn des Rennens die Position der Schildkröte erreicht, hat die Schildkröte eine zweite Position erreicht; wenn er diese zweite Position erreicht, befindet sich die Schildkröte an einer dritten Position und so weiter.
Dieses Argument beweist nicht, dass die Summe gleich 2 ist (obwohl es so ist), aber es beweist, dass sie höchstens 2 ist.
Zu den Tests für gleichmäßige Konvergenz gehören der M-Test von Weierstrass, der gleichmäßige Konvergenztest von Abel, der Dini-Test und das Cauchy-Kriterium.
Die Konvergenz ist auf geschlossenen und begrenzten (d. h. kompakten) Teilmengen des Inneren der Konvergenzscheibe gleichmäßig: Das heißt, sie ist auf kompakten Mengen gleichmäßig konvergent.
Die Hilbert-Poincaré-Reihe ist eine formale Potenzreihe zur Untersuchung abgestufter Algebren.
Im 17. Jahrhundert arbeitete James Gregory im neuen Dezimalsystem an unendlichen Reihen und veröffentlichte mehrere Maclaurin-Reihen.
Cauchy (1821) bestand auf strengen Konvergenztests; Er zeigte, dass das Produkt zweier Reihen nicht unbedingt konvergent ist, und mit ihm beginnt die Entdeckung wirksamer Kriterien.
Eine Summierbarkeitsmethode ist eine solche Zuweisung eines Grenzwerts zu einer Teilmenge der Menge divergenter Reihen, die den klassischen Begriff der Konvergenz angemessen erweitert.
Indische Gelehrte verwenden Faktorialformeln mindestens seit dem 12. Jahrhundert.
In funktionalen Sprachen wird die rekursive Definition häufig direkt implementiert, um rekursive Funktionen zu veranschaulichen.
Andere Implementierungen (z. B. Computersoftware wie Tabellenkalkulationsprogramme) können häufig größere Werte verarbeiten.
Im Vergleich zur Pickover-Definition des Superfaktoriellen wächst das Hyperfaktorielle relativ langsam.
Relativ gesehen gibt es keine so einfachen Lösungen für Fakultäten; Keine endliche Kombination von Summen, Produkten, Potenzen, Exponentialfunktionen oder Logarithmen wird ausreichen, um auszudrücken; Es ist jedoch möglich, eine allgemeine Formel für Fakultäten zu finden, indem man Werkzeuge wie Integrale und Grenzwerte aus der Analysis verwendet.
Bei den bisher besprochenen Integralen handelt es sich um transzendente Funktionen, aber die Gammafunktion entsteht auch aus Integralen rein algebraischer Funktionen.
Durch die Festlegung von Grenzwerten können bestimmte rationale Produkte mit unendlich vielen Faktoren auch anhand der Gammafunktion bewertet werden.
Ihre Geschichte, die insbesondere von Philip J. Davis in einem Artikel dokumentiert wurde, der ihm 1963 den Chauvenet-Preis einbrachte, spiegelt viele der wichtigsten Entwicklungen in der Mathematik seit dem 18. Jahrhundert wider.
Anstatt für jede Formel einen speziellen Beweis zu finden, wäre es wünschenswert, eine allgemeine Methode zur Identifizierung der Gammafunktion zu haben.
Allerdings scheint die Gammafunktion keine einfache Differentialgleichung zu erfüllen.
Das Bohr-Mollerup-Theorem ist nützlich, da es relativ einfach ist, die logarithmische Konvexität für jede der verschiedenen Formeln zu beweisen, die zur Definition der Gammafunktion verwendet werden.
Als in den 1950er Jahren elektronische Computer für die Erstellung von Tabellen verfügbar wurden, wurden mehrere umfangreiche Tabellen für die komplexe Gammafunktion veröffentlicht, um der Nachfrage gerecht zu werden, darunter eine Tabelle mit einer Genauigkeit von 12 Dezimalstellen vom US-amerikanischen National Bureau of Standards.
In der Wissenschaft ist eine Formel eine prägnante Möglichkeit, Informationen symbolisch auszudrücken, beispielsweise in einer mathematischen Formel oder einer chemischen Formel.
In der Mathematik bezieht sich eine Formel im Allgemeinen auf eine Identität, die einen mathematischen Ausdruck einem anderen gleichsetzt, wobei die wichtigsten mathematischen Theoreme sind.
Diese Konvention ist zwar bei relativ einfachen Formeln weniger wichtig, bedeutet aber, dass Mathematiker Formeln, die größer und komplexer sind, schneller manipulieren können.
H2O ist beispielsweise die chemische Formel für Wasser, die besagt, dass jedes Molekül aus zwei Wasserstoffatomen (H) und einem Sauerstoffatom (O) besteht.
In empirischen Formeln beginnen diese Proportionen mit einem Schlüsselelement und weisen dann die Anzahl der Atome der anderen Elemente in der Verbindung zu – als Verhältnisse zum Schlüsselelement.
Einige Arten ionischer Verbindungen können jedoch nicht als empirische Formeln geschrieben werden, die nur die ganzen Zahlen enthalten.
Es gibt verschiedene Arten dieser Formeln, darunter Summenformeln und Kurzformeln.
Funktionen waren ursprünglich die Idealisierung der Abhängigkeit einer variierenden Größe von einer anderen Größe.
Diese Definition von „Graph“ bezieht sich auf eine Menge von Objektpaaren.
Wenn die Domäne und die Kodomäne Mengen reeller Zahlen sind, kann man sich jedes dieser Paare als kartesische Koordinaten eines Punktes in der Ebene vorstellen.
Gelegentlich kann es mit der Funktion identifiziert werden, dies verbirgt jedoch die übliche Interpretation einer Funktion als Prozess.
Eine Karte kann eine beliebige Menge als Kodomäne haben, während in manchen Zusammenhängen, typischerweise in älteren Büchern, die Kodomäne einer Funktion speziell die Menge reeller oder komplexer Zahlen ist.
Ein weiteres häufiges Beispiel ist die Fehlerfunktion.
Potenzreihen können verwendet werden, um Funktionen auf dem Gebiet zu definieren, in dem sie konvergieren.
Dann kann die Potenzreihe verwendet werden, um den Definitionsbereich der Funktion zu erweitern.
Aus Teilen davon kann ein Diagramm entstehen, das (Teile) der Funktion darstellt.
Dies ist die kanonische Faktorisierung von .
Damals wurden nur reellwertige Funktionen einer reellen Variablen berücksichtigt und alle Funktionen wurden als glatt angenommen.
Funktionen werden mittlerweile in allen Bereichen der Mathematik verwendet.
Auf diese Weise werden inverse trigonometrische Funktionen als trigonometrische Funktionen definiert, wobei die trigonometrischen Funktionen monoton sind.
Die Nützlichkeit des Konzepts mehrwertiger Funktionen wird deutlicher, wenn man komplexe Funktionen, typischerweise analytische Funktionen, betrachtet.
Eine solche Funktion wird als Hauptwert der Funktion bezeichnet.
Funktionale Programmierung ist das Programmierparadigma, das darin besteht, Programme nur unter Verwendung von Unterprogrammen zu erstellen, die sich wie mathematische Funktionen verhalten.
Mit Ausnahme der computersprachlichen Terminologie hat „Funktion“ die in der Informatik übliche mathematische Bedeutung.
Begriffe werden durch einige Regeln manipuliert (die -Äquivalenz, die -Reduktion und die -Umwandlung), die die Axiome der Theorie sind und als Berechnungsregeln interpretiert werden können.
Nicolas Chuquet verwendete im 15. Jahrhundert eine Form der Exponentialschreibweise, die später im 16. Jahrhundert von Henricus Grammateus und Michael Stifel verwendet wurde.
So würden sie Polynome beispielsweise als schreiben.
Das Ergebnis ist immer eine positive reelle Zahl, und die oben für ganzzahlige Exponenten gezeigten Identitäten und Eigenschaften bleiben mit diesen Definitionen für reelle Exponenten wahr.
Diese Funktion entspricht der üblichen Wurzel für positive reelle Radikanden.
Dies ist der Ausgangspunkt der mathematischen Theorie der Halbgruppen.
Wir können die Menge N wieder durch eine Kardinalzahl n ersetzen, um Vn zu erhalten, obwohl diese ohne die Wahl einer bestimmten Standardmenge mit der Kardinalität n nur bis zur Isomorphie definiert ist.
Nicolas Bourbaki, Elemente der Mathematik, Mengenlehre, Springer-Verlag, 2004, III.§3.5.
Die iterierende Tetration führt zu einer weiteren Operation und so weiter, einem Konzept namens Hyperoperation.
In angewandten Einstellungen modellieren Exponentialfunktionen eine Beziehung, bei der eine konstante Änderung der unabhängigen Variablen die gleiche proportionale Änderung (d. h. prozentuale Zunahme oder Abnahme) der abhängigen Variablen ergibt.
Diese Funktionseigenschaft führt zu exponentiellem Wachstum oder exponentiellem Abfall.
Ebenso ist die Zusammensetzung von on (surjektiven) Funktionen immer on.
Dann kann man Ketten aus zusammengefügten Transformationen bilden, wie zum Beispiel .
Diese alternative Notation wird Postfix-Notation genannt.
Die Kategorie der Mengen mit Funktionen als Morphismen ist die prototypische Kategorie.
Beispielsweise ist das Dezibel (dB) eine Einheit, die verwendet wird, um das Verhältnis als Logarithmus auszudrücken, hauptsächlich für die Signalstärke und -amplitude (wobei der Schalldruck ein häufiges Beispiel ist).
Sie helfen bei der Beschreibung von Frequenzverhältnissen musikalischer Intervalle, kommen in Formeln zum Zählen von Primzahlen oder Approximationen von Fakultäten vor, informieren einige Modelle in der Psychophysik und können bei der forensischen Rechnungslegung hilfreich sein.
Die nächste ganze Zahl ist 4, was der Anzahl der Stellen von 1430 entspricht.
Vor Napiers Erfindung gab es andere Techniken mit ähnlichem Umfang, wie die Prosthaphärese oder die Verwendung von Progressionstabellen, die von Jost Bürgi um 1600 umfassend entwickelt wurden.
Die Aussage, dass eine Zahl so viele Ziffern erfordert, ist eine grobe Anspielung auf den gewöhnlichen Logarithmus und wurde von Archimedes als „Ordnung einer Zahl“ bezeichnet.
Solche Methoden werden Prosthaphaerese genannt.
Beispielsweise ist jede Kammer der Schale einer Nautilus eine ungefähre Kopie der nächsten, skaliert mit einem konstanten Faktor.
Logarithmen sind auch mit Selbstähnlichkeit verbunden.
Es wird zur Quantifizierung des Spannungsverlusts bei der Übertragung elektrischer Signale, zur Beschreibung der Schallleistung in der Akustik und der Lichtabsorption in den Bereichen Spektrometrie und Optik verwendet.
Essig hat typischerweise einen pH-Wert von etwa 3.
Dieses „Gesetz“ ist jedoch weniger realistisch als neuere Modelle, wie etwa das Potenzgesetz von Stevens.)
Wenn der Logarithmus einer Zufallsvariablen normalverteilt ist, spricht man von einer logarithmischen Normalverteilung.
Für ein solches Modell hängt die Wahrscheinlichkeitsfunktion von mindestens einem Parameter ab, der geschätzt werden muss.
In ähnlicher Weise sortiert der Zusammenführungssortierungsalgorithmus eine unsortierte Liste, indem er die Liste in zwei Hälften teilt und diese zuerst sortiert, bevor die Ergebnisse zusammengeführt werden.
Lyapunov-Exponenten verwenden Logarithmen, um den Grad der Chaotizität eines dynamischen Systems zu messen.
Das Sierpinski-Dreieck (im Bild) kann durch drei Kopien von sich selbst abgedeckt werden, wobei jede Seite die halbe Originallänge hat.
Ein weiteres Beispiel ist der p-adische Logarithmus, die Umkehrfunktion der p-adischen Exponentialfunktion.
Die Potenzierung lässt sich effizient durchführen, der diskrete Logarithmus dürfte jedoch in manchen Gruppen sehr schwer zu berechnen sein.
Quadratwurzeln negativer Zahlen können im Rahmen komplexer Zahlen diskutiert werden.
Im alten Indien war das Wissen über theoretische und angewandte Aspekte der Quadrat- und Quadratwurzel mindestens so alt wie die Sulba-Sutras, die auf etwa 800–500 v. Chr. datiert wurden (möglicherweise viel früher).
Der Buchstabe jīm ähnelt der heutigen Quadratwurzelform.
Es definiert ein wichtiges Konzept der Standardabweichung, das in der Wahrscheinlichkeitstheorie und Statistik verwendet wird.
Die meisten Taschenrechner verfügen über einen Quadratwurzelschlüssel.
Der Zeitaufwand für die Berechnung einer Quadratwurzel mit n-stelliger Genauigkeit entspricht dem der Multiplikation zweier n-stelliger Zahlen.
Hilberts Probleme sind 23 Probleme der Mathematik, die 1900 vom deutschen Mathematiker David Hilbert veröffentlicht wurden.
Für andere Probleme wie das fünfte haben sich Experten traditionell auf eine einzige Interpretation geeinigt und eine Lösung für die akzeptierte Interpretation gegeben, es gibt jedoch eng damit verbundene ungelöste Probleme.
Es gibt zwei Probleme, die nicht nur ungelöst sind, sondern nach modernen Maßstäben möglicherweise sogar unlösbar sind.
Die anderen einundzwanzig Probleme haben alle große Aufmerksamkeit erhalten, und bis ins 20. Jahrhundert hinein wurde die Arbeit an diesen Problemen immer noch als von größter Bedeutung angesehen.
Hilbert lebte noch 12 Jahre nach der Veröffentlichung seines Theorems durch Kurt Gödel, scheint aber keine formelle Antwort auf Gödels Werk verfasst zu haben.
Bei der Erörterung seiner Meinung, dass jedes mathematische Problem eine Lösung haben sollte, lässt Hilbert die Möglichkeit zu, dass die Lösung ein Beweis dafür sein könnte, dass das ursprüngliche Problem unmöglich ist.
Die erste davon wurde von Bernard Dwork bewiesen; Ein völlig anderer Beweis der ersten beiden, mittels ℓ-adischer Kohomologie, wurde von Alexander Grothendieck geliefert.
Allerdings ähnelten die Weil-Vermutungen in ihrem Umfang eher einem einzelnen Hilbert-Problem, und Weil hatte sie nie als Programm für die gesamte Mathematik gedacht.
Erdős bot oft Geldprämien an; Die Höhe der Belohnung hing von der wahrgenommenen Schwierigkeit des Problems ab.
Zumindest in den Mainstream-Medien ist die Liste der sieben Millennium-Preis-Probleme, die im Jahr 2000 vom Clay Mathematics Institute ausgewählt wurden, de facto das Analogon zu Hilberts Problemen im 21. Jahrhundert.
Die Riemann-Hypothese ist bemerkenswert für ihr Erscheinen auf der Liste der Hilbert-Probleme, der Smale-Liste, der Liste der Millennium-Preis-Probleme und sogar den Weil-Vermutungen in ihrer geometrischen Gestalt.
1931, 1936 3. Ist es bei zwei beliebigen Polyedern mit gleichem Volumen immer möglich, das erste in endlich viele Polyederstücke zu zerlegen, die sich wieder zusammensetzen lassen, um das zweite zu ergeben?
— 12. Erweitern Sie den Kronecker-Weber-Satz über abelsche Erweiterungen der rationalen Zahlen auf jeden Basiszahlkörper.
1959 15. Strenge Begründung von Schuberts Aufzählungsrechnung.
1927 18. (a) Gibt es ein Polyeder, das nur eine anisoedrische Kachelung in drei Dimensionen zulässt? (b) Was ist die dichteste Kugelpackung?
Eine Zahl ist ein mathematisches Objekt, das zum Zählen, Messen und Beschriften verwendet wird.
Allgemeiner ausgedrückt können einzelne Zahlen durch Symbole, sogenannte Ziffern, dargestellt werden; „5“ ist beispielsweise eine Zahl, die die Zahl fünf darstellt.
Berechnungen mit Zahlen werden mit arithmetischen Operationen durchgeführt. Die bekanntesten sind Addition, Subtraktion, Multiplikation, Division und Potenzierung.
Gilsdorf, Thomas E. Introduction to Cultural Mathematics: With Case Studies in the Otomies and Incas, John Wiley & Sons, 24. Februar 2012. Restivo, S. Mathematics in Society and History, Springer Science & Business Media, 30. November 1992.
Im 19. Jahrhundert begannen Mathematiker, viele verschiedene Abstraktionen zu entwickeln, die bestimmte Eigenschaften von Zahlen gemeinsam hatten und als Erweiterung des Konzepts angesehen werden können.
Ein Zählsystem kennt kein Stellenwertkonzept (wie in der modernen Dezimalschreibweise), was die Darstellung großer Zahlen einschränkt.
Brahmaguptas Brāhmasphuṭasiddhānta ist das erste Buch, das Null als Zahl erwähnt, daher wird Brahmagupta normalerweise als der erste angesehen, der das Konzept der Null formuliert.
In ähnlicher Weise verwendete Pāṇini (5. Jahrhundert v. Chr.) den Nulloperator im Ashtadhyayi, einem frühen Beispiel einer algebraischen Grammatik für die Sanskrit-Sprache (siehe auch Pingala).
Um 130 n. Chr. verwendete Ptolemaios, beeinflusst von Hipparchos und den Babyloniern, ein Symbol für 0 (einen kleinen Kreis mit einem langen Überstrich) innerhalb eines sexagesimalen Zahlensystems, ansonsten verwendete er alphabetische griechische Ziffern.
Diophantus' frühere Referenz wurde vom indischen Mathematiker Brahmagupta in Brāhmasphuṭasiddhānta im Jahr 628 ausführlicher erörtert, der negative Zahlen verwendete, um die allgemeine quadratische Formel zu erstellen, die bis heute verwendet wird.
Gleichzeitig zeigten die Chinesen negative Zahlen an, indem sie einen diagonalen Strich durch die am weitesten rechts stehende Ziffer ungleich Null der entsprechenden positiven Zahl zogen.
Klassische griechische und indische Mathematiker untersuchten die Theorie der rationalen Zahlen als Teil des allgemeinen Studiums der Zahlentheorie.
Das Konzept der Dezimalbrüche ist eng mit der dezimalen Stellenwertschreibweise verknüpft; Die beiden scheinen sich parallel entwickelt zu haben.
Pythagoras glaubte jedoch an die Absolutheit der Zahlen und konnte die Existenz irrationaler Zahlen nicht akzeptieren.
Im 17. Jahrhundert verwendeten Mathematiker im Allgemeinen Dezimalbrüche mit moderner Schreibweise.
Im Jahr 1872 erfolgte die Veröffentlichung der Theorien von Karl Weierstraß (durch seinen Schüler E. Kossak), Eduard Heine, Georg Cantor und Richard Dedekind.
Weierstrass, Cantor und Heine basieren ihre Theorien auf unendlichen Reihen, während Dedekind seine auf der Idee eines Schnitts (Schnitt) im System der reellen Zahlen gründet, der alle rationalen Zahlen in zwei Gruppen mit bestimmten charakteristischen Eigenschaften aufteilt.
Daher war es notwendig, die breitere Menge algebraischer Zahlen (alle Lösungen von Polynomgleichungen) zu berücksichtigen.
Aristoteles definierte die traditionelle westliche Vorstellung der mathematischen Unendlichkeit.
Aber der nächste große Fortschritt in der Theorie wurde von Georg Cantor gemacht; 1895 veröffentlichte er ein Buch über seine neue Mengenlehre, in dem er unter anderem transfinite Zahlen einführte und die Kontinuumshypothese formulierte.
Eine moderne geometrische Version der Unendlichkeit ist die projektive Geometrie, die „ideale Punkte im Unendlichen“ einführt, einen für jede Raumrichtung.
Die Idee der grafischen Darstellung komplexer Zahlen tauchte jedoch bereits 1685 in Wallis‘ De algebra tractatus auf.
Im Jahr 240 v. Chr. nutzte Eratosthenes das Sieb des Eratosthenes, um Primzahlen schnell zu isolieren.
Weitere Ergebnisse zur Verteilung der Primzahlen sind Eulers Beweis, dass die Summe der Kehrwerte der Primzahlen divergiert, und die Goldbach-Vermutung, die besagt, dass jede ausreichend große gerade Zahl die Summe zweier Primzahlen ist.
Traditionell begann die Folge der natürlichen Zahlen mit 1 (0 galt bei den alten Griechen noch nicht einmal als Zahl).
In diesem System zur Basis 10 hat die Ziffer ganz rechts einer natürlichen Zahl den Stellenwert 1, und jede zweite Ziffer hat einen Stellenwert, der zehnmal so groß ist wie der Stellenwert der Ziffer rechts davon.
Negative Zahlen werden normalerweise mit einem negativen Vorzeichen (Minuszeichen) geschrieben.
Hier kommt der Buchstabe Z.
Brüche können größer, kleiner oder gleich 1 sein und können auch positiv, negativ oder 0 sein.
Der folgende Absatz konzentriert sich hauptsächlich auf positive reelle Zahlen.
So ist beispielsweise eine Hälfte 0,5, ein Fünftel 0,2, ein Zehntel 0,1 und ein Fünfzigstel 0,02.
Nicht nur diese prominenten Beispiele, sondern fast alle reellen Zahlen sind irrational und haben daher keine sich wiederholenden Muster und daher keine entsprechende Dezimalzahl.
Da nicht einmal die zweite Nachkommastelle erhalten bleibt, sind die folgenden Ziffern nicht von Bedeutung.
Beispielsweise stellen 0,999..., 1,0, 1,00, 1,000, ... alle die natürliche Zahl 1 dar.
Wenn schließlich alle Ziffern einer Zahl 0 sind, ist die Zahl 0, und wenn alle Ziffern einer Zahl eine endlose Folge von Neunen sind, können Sie die Neunen rechts von der Dezimalstelle weglassen und eine hinzufügen zur Folge von Neunen links von der Dezimalstelle.
Somit sind die reellen Zahlen eine Teilmenge der komplexen Zahlen.
Der Grundsatz der Algebra besagt, dass die komplexen Zahlen ein algebraisch geschlossenes Feld bilden, was bedeutet, dass jedes Polynom mit komplexen Koeffizienten eine Wurzel in den komplexen Zahlen hat.
Die Primzahlen werden seit mehr als 2000 Jahren umfassend untersucht und haben zu vielen Fragen geführt, von denen nur einige beantwortet wurden.
Reelle Zahlen, die keine rationalen Zahlen sind, werden irrationale Zahlen genannt.
Die berechenbaren Zahlen sind für alle üblichen Rechenoperationen, einschließlich der Berechnung der Wurzeln eines Polynoms, stabil und bilden somit einen reellen geschlossenen Körper, der die reellen algebraischen Zahlen enthält.
Ein Grund dafür ist, dass es keinen Algorithmus zum Testen der Gleichheit zweier berechenbarer Zahlen gibt.
Das resultierende Zahlensystem hängt davon ab, welche Basis für die Ziffern verwendet wird: Jede Basis ist möglich, aber eine Primzahlbasis bietet die besten mathematischen Eigenschaften.
Ersteres gibt die Reihenfolge der Menge an, während letzteres ihre Größe angibt.
Diese Standardbasis macht die komplexen Zahlen zu einer kartesischen Ebene, die als komplexe Ebene bezeichnet wird.
Die komplexen Zahlen mit dem Betrag eins bilden den Einheitskreis.
Bei der Domänenfärbung werden die Ausgabedimensionen durch Farbe bzw. Helligkeit dargestellt.
Die Arbeit am Problem der allgemeinen Polynome führte schließlich zum Fundamentalsatz der Algebra, der zeigt, dass es bei komplexen Zahlen für jede Polynomgleichung vom Grad eins oder höher eine Lösung gibt.
Wessels Memoiren erschienen in den Proceedings of the Copenhagen Academy, blieben jedoch weitgehend unbeachtet.
Zu den späteren klassischen Autoren der allgemeinen Theorie zählen Richard Dedekind, Otto Hölder, Felix Klein, Henri Poincaré, Hermann Schwarz, Karl Weierstrass und viele andere.
Die Verwendung imaginärer Zahlen wurde erst durch die Arbeiten von Leonhard Euler (1707–1783) und Carl Friedrich Gauß (1777–1855) allgemein akzeptiert.
Die ganzen Zahlen bilden die kleinste Gruppe und den kleinsten Ring, der die natürlichen Zahlen enthält.
Es ist der Prototyp aller Objekte einer solchen algebraischen Struktur.
In mehreren Programmiersprachen (z. B. Algol68, C, Java, Delphi usw.) werden Integer-Approximationsdatentypen (oder Teilmengen) mit fester Länge als int oder Integer bezeichnet.
Dies sind beweisbare Eigenschaften rationaler Zahlen und Positionszahlensysteme und werden in der Mathematik nicht als Definitionen verwendet.
Da das Dreieck gleichschenklig ist, gilt a u003d b).
Da c gerade ist, ergibt die Division von c durch 2 eine ganze Zahl.
Das Ersetzen von c2 durch 4y2 in der ersten Gleichung (c2 u003d 2b2) ergibt 4y2u003d 2b2.
Da b2 gerade ist, muss b gerade sein.
Dies widerspricht jedoch der Annahme, dass sie keine gemeinsamen Faktoren haben.
Hippasus wurde jedoch für seine Bemühungen nicht gelobt: Einer Legende zufolge machte er seine Entdeckung auf See und wurde anschließend von seinen Pythagoreerkollegen über Bord geworfen, „… weil er ein Element im Universum hervorgebracht hatte, das die … Lehre davon leugnete.“ Alle Phänomene im Universum können auf ganze Zahlen und deren Verhältnisse reduziert werden.“
Betrachten Sie beispielsweise ein Liniensegment: Dieses Segment kann in zwei Hälften geteilt werden, diese Hälfte in zwei Hälften, die Hälfte der Hälfte in zwei Hälften und so weiter.
Genau das wollte Zeno beweisen.
Nach Ansicht der Griechen bewies die Widerlegung der Gültigkeit einer Ansicht nicht unbedingt die Gültigkeit einer anderen, weshalb weitere Untersuchungen erforderlich waren.
Eine Größe „... war keine Zahl, sondern stand für Einheiten wie Linienabschnitte, Winkel, Flächen, Volumina und Zeit, die, wie wir sagen würden, kontinuierlich variieren konnten.
Da den Größen keine quantitativen Werte zugewiesen wurden, konnte Eudoxos sowohl kommensurable als auch inkommensurable Verhältnisse erklären, indem er ein Verhältnis anhand seiner Größe und Proportionen als Gleichheit zwischen zwei Verhältnissen definierte.
Diese Inkommensurabilität wird in Euklids Elementen, Buch X, Satz 9 behandelt.
Tatsächlich wurden in vielen Fällen algebraische Vorstellungen in geometrische Begriffe umformuliert.
Die Erkenntnis, dass einige Grundkonzepte der bestehenden Theorie im Widerspruch zur Realität standen, erforderte eine vollständige und gründliche Untersuchung der Axiome und Annahmen, die dieser Theorie zugrunde liegen.
Allerdings schreibt der Historiker Carl Benjamin Boyer, dass „solche Behauptungen nicht gut begründet sind und wahrscheinlich nicht wahr sind“.
Mathematiker wie Brahmagupta (im Jahr 628 n. Chr.) und Bhāskara I. (im Jahr 629 n. Chr.) leisteten Beiträge auf diesem Gebiet, ebenso wie andere Mathematiker, die folgten.
Im Jahr 1872 wurden die Theorien von Karl Weierstrass (von seinem Schüler Ernst Kossak), Eduard Heine (Crelles Tagebuch, 74), Georg Cantor (Annalen, 5) und Richard Dedekind veröffentlicht.
Weierstrass, Cantor und Heine basieren ihre Theorien auf unendlichen Reihen, während Dedekind seine auf der Idee eines Schnitts (Schnitt) im System aller rationalen Zahlen gründet, der sie in zwei Gruppen mit bestimmten charakteristischen Eigenschaften aufteilt.
Dirichlet trug ebenfalls zur allgemeinen Theorie bei, ebenso wie zahlreiche Mitwirkende an den Anwendungen des Themas.
Dies besagt, dass jede ganze Zahl eine eindeutige Faktorisierung in Primzahlen hat.
Um dies zu zeigen, nehmen wir an, wir dividieren ganze Zahlen n durch m (wobei m ungleich Null ist).
Wenn 0 nie auftritt, kann der Algorithmus höchstens m − 1 Schritte ausführen, ohne einen Rest mehr als einmal zu verwenden.
In der Mathematik werden die natürlichen Zahlen zum Zählen (wie in „Es liegen sechs Münzen auf dem Tisch“) und zum Ordnen (wie in „Dies ist die drittgrößte Stadt des Landes“) verwendet.
Durch diese Erweiterungsketten werden die natürlichen Zahlen kanonisch in die anderen Zahlensysteme eingebettet (identifiziert).
Der erste große Fortschritt in der Abstraktion war die Verwendung von Ziffern zur Darstellung von Zahlen.
